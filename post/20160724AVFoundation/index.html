<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Yofer Zhang" />



<meta name="description" content="新博客：完整版 - AVFoundation Programming Guide
分章节版：– 第1章：About AVFoundation - AVFoundation概述– 第2章：Using Assets - 使用Assets– 第3章：Playback - 播放– 第4章：Editing - 编辑– 第5章：Still and Video Media Capture - 静态视频媒体捕获">
<meta property="og:type" content="article">
<meta property="og:title" content="AVFoundation Programming Guide(官方文档翻译)完整版中英对照">
<meta property="og:url" content="http://yoferzhang.com/post/20160724AVFoundation/index.html">
<meta property="og:site_name" content="YoferZhang 的博客">
<meta property="og:description" content="新博客：完整版 - AVFoundation Programming Guide
分章节版：– 第1章：About AVFoundation - AVFoundation概述– 第2章：Using Assets - 使用Assets– 第3章：Playback - 播放– 第4章：Editing - 编辑– 第5章：Still and Video Media Capture - 静态视频媒体捕获">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f650lssb7bj20l80d5jsa.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f651hbl7ohj20l80dg0ts.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6f7bsp7lbj20oo04u74b.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/a9c4d5f6gw1f6f94r8pzvj20u00e0wep.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6f9awlylkj20sh0klgmb.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6fu4l7pu1j20wu0goaal.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6fu9qkxtjj20mo0giq3g.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6gafmhl5zj214v0lf0ti.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/a9c4d5f6gw1f6galdybkvj20zc0nnab4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6gm15jfy4j20ug0h9753.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6gmf843fwj20vi0n9jsb.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6gvmy61otj20ma0msdfz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6kx28dgzuj20lc0cojri.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6kxhx9jerj20n009f0st.jpg">
<meta property="og:updated_time" content="2017-02-09T05:33:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AVFoundation Programming Guide(官方文档翻译)完整版中英对照">
<meta name="twitter:description" content="新博客：完整版 - AVFoundation Programming Guide
分章节版：– 第1章：About AVFoundation - AVFoundation概述– 第2章：Using Assets - 使用Assets– 第3章：Playback - 播放– 第4章：Editing - 编辑– 第5章：Still and Video Media Capture - 静态视频媒体捕获">
<meta name="twitter:image" content="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f650lssb7bj20l80d5jsa.jpg">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="YoferZhang 的博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">





    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>AVFoundation Programming Guide(官方文档翻译)完整版中英对照 | YoferZhang 的博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: false,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: undefined
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/Image/author.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Yofer Zhang</a></h1>
        </hgroup>

        
        <p class="header-subtitle">数学出身，功底扎实，热爱编程，虽然编程起步晚，但是冲劲十足。</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:yoferzhang@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="https://github.com/yoferzhang" title="GitHub"></a>
                            
                                <a class="fa 知乎" href="https://www.zhihu.com/people/yoferzhang/" title="知乎"></a>
                            
                                <a class="fa 豆瓣" href="https://www.douban.com/people/zyq522376829/" title="豆瓣"></a>
                            
                                <a class="fa CSDN" href="http://blog.csdn.net/zyq522376829" title="CSDN"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AlphaGo/">AlphaGo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C语言/">C语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HFS/">HFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NBA/">NBA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NTFS/">NTFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/calloc/">calloc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jacman/">jacman</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/malloc/">malloc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oc/">oc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/relloc/">relloc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/健身/">健身</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/内存四区/">内存四区</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分类/">分类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大脑/">大脑</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习/">学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/心得/">心得</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/快速记忆/">快速记忆</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数字记忆/">数字记忆</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/番茄/">番茄</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/磁盘格式/">磁盘格式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/科比/">科比</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/笔记/">笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/纸牌/">纸牌</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/结构体，内存，对齐/">结构体，内存，对齐</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网站/">网站</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/联想/">联想</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/肌肉/">肌肉</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/腾讯/">腾讯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/艾宾浩斯/">艾宾浩斯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/英语/">英语</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语法/">语法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/读书/">读书</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔，测试/">随笔，测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/黑苹果/">黑苹果</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">Amor Fati</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Yofer Zhang</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/Image/author.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Yofer Zhang</a></h1>
            </hgroup>
            
            <p class="header-subtitle">数学出身，功底扎实，热爱编程，虽然编程起步晚，但是冲劲十足。</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:yoferzhang@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/yoferzhang" title="GitHub"></a>
                            
                                <a class="fa 知乎" target="_blank" href="https://www.zhihu.com/people/yoferzhang/" title="知乎"></a>
                            
                                <a class="fa 豆瓣" target="_blank" href="https://www.douban.com/people/zyq522376829/" title="豆瓣"></a>
                            
                                <a class="fa CSDN" target="_blank" href="http://blog.csdn.net/zyq522376829" title="CSDN"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-20160724AVFoundation" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/post/20160724AVFoundation/" class="article-date">
      <time datetime="2016-07-24T02:57:30.000Z" itemprop="datePublished">2016-07-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      AVFoundation Programming Guide(官方文档翻译)完整版中英对照
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/iOS/">iOS</a>
    </div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <blockquote>
<p>新博客：<br>完整版 - <a href="http://yoferzhang.com/post/20160724AVFoundation/">AVFoundation Programming Guide</a></p>
<p>分章节版：<br>– 第1章：<a href="http://yoferzhang.com/post/20160803AVFoundation01Introduction/">About AVFoundation - AVFoundation概述</a><br>– 第2章：<a href="http://yoferzhang.com/post/20160803AVFoundation02UsingAssets/">Using Assets - 使用Assets</a><br>– 第3章：<a href="http://yoferzhang.com/post/20160803AVFoundation03Playback/">Playback - 播放</a><br>– 第4章：<a href="http://yoferzhang.com/post/20160803AVFoundation04Editing/">Editing - 编辑</a><br>– 第5章：<a href="http://yoferzhang.com/post/20160803AVFoundation05StillAndVideoMediaCapture/">Still and Video Media Capture - 静态视频媒体捕获</a><br>– 第6章：<a href="http://yoferzhang.com/post/20160803AVFoundation06Export/">Export - 输出</a><br>– 第7章：<a href="http://yoferzhang.com/post/20160803AVFoundation07TimeAndMediaRepresentations/">Time and Media Representations 时间和媒体表现</a></p>
<p>CSDN博客：<br>完整版 - <a href="http://blog.csdn.net/zyq522376829/article/details/52144394" target="_blank" rel="external">AVFoundation Programming Guide</a></p>
<p>分章节版：<br>– 第1章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144317" target="_blank" rel="external">About AVFoundation - AVFoundation概述</a><br>– 第2章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144326" target="_blank" rel="external">Using Assets - 使用Assets</a><br>– 第3章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144333" target="_blank" rel="external">Playback - 播放</a><br>– 第4章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144342" target="_blank" rel="external">Editing - 编辑</a><br>– 第5章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144355" target="_blank" rel="external">Still and Video Media Capture - 静态视频媒体捕获</a><br>– 第6章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144366" target="_blank" rel="external">Export - 输出</a><br>– 第7章：<a href="http://blog.csdn.net/zyq522376829/article/details/52144372" target="_blank" rel="external">Time and Media Representations 时间和媒体表现</a></p>
<p>版权声明：本文为博主原创翻译，如需转载请注明出处。</p>
<p>苹果源文档地址 - <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW3" target="_blank" rel="external">点击这里</a></p>
</blockquote>
<h1 id="About-AVFoundation-AVFoundation概述"><a href="#About-AVFoundation-AVFoundation概述" class="headerlink" title="About AVFoundation - AVFoundation概述"></a>About AVFoundation - AVFoundation概述</h1><p>AVFoundation is one of several frameworks that you can use to play and create time-based audiovisual media. It provides an Objective-C interface you use to work on a detailed level with time-based audiovisual data. For example, you can use it to examine, create, edit, or reencode media files. You can also get input streams from devices and manipulate video during realtime capture and playback. Figure I-1 shows the architecture on iOS.</p>
<p><code>AVFoundation</code> 是可以用它来播放和创建基于时间的视听媒体的几个框架之一。它提供了基于时间的视听数据的详细级别上的Objective-C接口。例如，你可以用它来检查，创建，编辑或重新编码媒体文件。您也可以从设备得到输入流和在实时捕捉回放过程中操控视频。图I-1显示了iOS上的架构。</p>
<center><br>    <img src="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f650lssb7bj20l80d5jsa.jpg" alt="Figure I-1  AVFoundation stack on iOS"><br></center>

<a id="more"></a>
<p>Figure I-2 shows the corresponding media architecture on OS X.</p>
<p>图1-2显示了<code>OS X</code>上相关媒体的架构：</p>
<center><br>    <img src="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f651hbl7ohj20l80dg0ts.jpg" alt="Figure I-2  AVFoundation stack on OS X"><br></center>

<p>You should typically use the highest-level abstraction available that allows you to perform the tasks you want.</p>
<ul>
<li><p>If you simply want to play movies, use the AVKit framework.</p>
</li>
<li><p>On iOS, to record video when you need only minimal control over format, use the UIKit framework(<a href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIImagePickerController_Class/index.html#//apple_ref/occ/cl/UIImagePickerController" target="_blank" rel="external">UIImagePickerController</a>)</p>
</li>
</ul>
<p>Note, however, that some of the primitive data structures that you use in AV Foundation—including time-related data structures and opaque objects to carry and describe media data—are declared in the Core Media framework.</p>
<p>通常，您应该使用可用的最高级别的抽象接口，执行所需的任务。</p>
<ul>
<li><p>如果你只是想播放电影，使用 <code>AVKit</code> 框架。</p>
</li>
<li><p>在iOS上，当你在格式上只需要最少的控制，使用UIKit框架录制视频。(<a href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIImagePickerController_Class/index.html#//apple_ref/occ/cl/UIImagePickerController" target="_blank" rel="external">UIImagePickerController</a>).</p>
</li>
</ul>
<p>但是请注意，某些在<code>AV Foundation</code> 中使用的原始数据结构，包括时间相关的数据结构和不透明数据对象的传递和描述媒体数据是在<code>Core Media framework</code>声明的。</p>
<h2 id="At-a-Glance-摘要"><a href="#At-a-Glance-摘要" class="headerlink" title="At a Glance - 摘要"></a>At a Glance - 摘要</h2><p>There are two facets to the AVFoundation framework—APIs related to video and APIs related just to audio. The older audio-related classes provide easy ways to deal with audio. They are described in the <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/MultimediaPG/Introduction/Introduction.html#//apple_ref/doc/uid/TP40009767" target="_blank" rel="external">Multimedia Programming Guide</a>, not in this document.</p>
<ul>
<li><p>To play sound files, you can use <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioPlayerClassReference/index.html#//apple_ref/occ/cl/AVAudioPlayer" target="_blank" rel="external">AVAudioPlayer</a>.</p>
</li>
<li><p>To record audio, you can use <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioRecorder_ClassReference/index.html#//apple_ref/occ/cl/AVAudioRecorder" target="_blank" rel="external">AVAudioRecorder</a>.</p>
</li>
</ul>
<p>You can also configure the audio behavior of your application using <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioSession_ClassReference/index.html#//apple_ref/occ/cl/AVAudioSession" target="_blank" rel="external">AVAudioSession</a>; this is described in <a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875" target="_blank" rel="external">Audio Session Programming Guide</a>.</p>
<p>AVFoundation框架包含视频相关的APIs和音频相关的APIs。旧的音频相关类提供了简便的方法来处理音频。他们在<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/MultimediaPG/Introduction/Introduction.html#//apple_ref/doc/uid/TP40009767" target="_blank" rel="external">Multimedia Programming Guide</a>,中介绍，不在这个文档中。</p>
<ul>
<li><p>要播放声音文件，您可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioPlayerClassReference/index.html#//apple_ref/occ/cl/AVAudioPlayer" target="_blank" rel="external">AVAudioPlayer</a>。</p>
</li>
<li><p>要录制音频，您可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioRecorder_ClassReference/index.html#//apple_ref/occ/cl/AVAudioRecorder" target="_blank" rel="external">AVAudioRecorder</a>。</p>
</li>
</ul>
<p>您还可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioSession_ClassReference/index.html#//apple_ref/occ/cl/AVAudioSession" target="_blank" rel="external">AVAudioSession</a> 来配置应用程序的音频行为;这是在 <a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875" target="_blank" rel="external">Audio Session Programming Guide</a> 文档中介绍的。</p>
<h3 id="Representing-and-Using-Media-with-AVFoundation-用AVFoundation-表示和使用媒体"><a href="#Representing-and-Using-Media-with-AVFoundation-用AVFoundation-表示和使用媒体" class="headerlink" title="Representing and Using Media with AVFoundation - 用AVFoundation 表示和使用媒体"></a>Representing and Using Media with AVFoundation - 用AVFoundation 表示和使用媒体</h3><p>The primary class that the AV Foundation framework uses to represent media is <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/cl/AVAsset" target="_blank" rel="external">AVAsset</a>. The design of the framework is largely guided by this representation. Understanding its structure will help you to understand how the framework works. An AVAssetinstance is an aggregated representation of a collection of one or more pieces of media data (audio and video tracks). It provides information about the collection as a whole, such as its title, duration, natural presentation size, and so on. AVAsset is not tied to particular data format. AVAsset is the superclass of other classes used to create asset instances from media at a URL (see <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a>) and to create new compositions (see <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW1" target="_blank" rel="external">Editing</a>).</p>
<p><code>AV Foundation</code>框架用来表示媒体的主要类是 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/cl/AVAsset" target="_blank" rel="external">AVAsset</a>。框架的设计主要是由这种表示引导。了解它的结构将有助于您了解该框架是如何工作的。一个 <code>AVAsset</code> 实例的媒体数据的一个或更多个（音频和视频轨道）的集合的聚集表示。它规定将有关集合的信息作为一个整体，如它的名称，时间，自然呈现大小等的信息。 <code>AVAsset</code> 是不依赖于特定的数据格式。 <code>AVAsset</code>是常常从URL中的媒体创建资产实例的这种类父类（请参阅 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a>），并创造新的成分（见 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW1" target="_blank" rel="external">Editing</a>）。</p>
<p>Each of the individual pieces of media data in the asset is of a uniform type and called a track. In a typical simple case, one track represents the audio component, and another represents the video component; in a complex composition, however, there may be multiple overlapping tracks of audio and video. Assets may also have metadata.</p>
<p><code>Asset</code>中媒体数据的各个部分，每一个都是一个统一的类型，把这个类型称为“轨道”。在一个典型简单的情况下，一个轨道代表这个音频组件，另一个代表视频组件。然而复杂的组合中，有可能是多个重叠的音频和视频轨道。Assets也可能有元数据。</p>
<p>A vital concept in AV Foundation is that initializing an asset or a track does not necessarily mean that it is ready for use. It may require some time to calculate even the duration of an item (an MP3 file, for example, may not contain summary information). Rather than blocking the current thread while a value is being calculated, you ask for values and get an answer back asynchronously through a callback that you define using a block.</p>
<p>在 <code>AV Foundation</code> 中一个非常重要的概念是：初始化一个 <code>asset</code> 或者一个轨道并不一定意味着它已经准备好可以被使用。这可能需要一些时间来计算一个项目的持续时间（例如一个MP3文件，其中可能不包含摘要信息）。而不是当一个值被计算的时候阻塞当前线程，你访问这个值，并且通过调用你定义的一个 <code>block</code> 来得到异步返回。</p>
<p>Relevant Chapters: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a>, <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW1" target="_blank" rel="external">Time and Media Representations</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a>, <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW1" target="_blank" rel="external">Time and Media Representations</a></p>
<h3 id="Playback-播放"><a href="#Playback-播放" class="headerlink" title="Playback - 播放"></a>Playback - 播放</h3><p>AVFoundation allows you to manage the playback of asset in sophisticated ways. To support this, it separates the presentation state of an asset from the asset itself. This allows you to, for example, play two different segments of the same asset at the same time rendered at different resolutions. The presentation state for an asset is managed by a player item object; the presentation state for each track within an asset is managed by a player item track object. Using the player item and player item tracks you can, for example, set the size at which the visual portion of the item is presented by the player, set the audio mix parameters and video composition settings to be applied during playback, or disable components of the asset during playback.</p>
<p><code>AVFoundation</code>允许你用一种复杂的方式来管理<code>asset</code>的播放。为了支持这一点，它将一个<code>asset</code>的呈现状态从<code>asset</code>自身中分离出来。例如允许你在不同的分辨率下同时播放同一个<code>asset</code>中的两个不同的片段。一个<code>asset</code>的呈现状态是由<code>player item</code>对象管理的。<code>Asset</code>中的每个轨道的呈现状态是由<code>player item track</code>对象管理的。例如使用<code>player item</code>和<code>player item tracks</code>，你可以设置被播放器呈现的项目中可视的那一部分，设置音频的混合参数以及被应用于播放期间的视频组合设定，或者播放期间的禁用组件。</p>
<p>You play player items using a player object, and direct the output of a player to the Core Animation layer. You can use a player queue to schedule playback of a collection of player items in sequence.</p>
<p>你可以使用一个 <code>player</code> 对象来播放播放器项目，并且直接输出一个播放器给核心动画层。你可以使用一个 <code>player queue</code>（player对象的队列）去给队列中<code>player items</code>集合中的播放项目安排序列。</p>
<p>Relevant Chapter: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW1" target="_blank" rel="external">Playback</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW1" target="_blank" rel="external">Playback</a></p>
<h3 id="Reading-Writing-and-Reencoding-Assets-读取，写入和重新编码Assets"><a href="#Reading-Writing-and-Reencoding-Assets-读取，写入和重新编码Assets" class="headerlink" title="Reading, Writing, and Reencoding Assets - 读取，写入和重新编码Assets"></a>Reading, Writing, and Reencoding Assets - 读取，写入和重新编码Assets</h3><p>AVFoundation allows you to create new representations of an asset in several ways. You can simply reencode an existing asset, or—in iOS 4.1 and later—you can perform operations on the contents of an asset and save the result as a new asset.</p>
<p><code>AVFoundation</code> 允许你用几种方式创建新的 <code>asset</code> 的表现形式。你可以简单将已经存在的 <code>asset</code> 重新编码，或者在iOS4.1以及之后的版本中，你可以在一个 <code>asset</code> 的目录中执行一些操作并且将结果保存为一个新的 <code>asset</code> 。</p>
<p>You use an export session to reencode an existing asset into a format defined by one of a small number of commonly-used presets. If you need more control over the transformation, in iOS 4.1 and later you can use an asset reader and asset writer object in tandem to convert an asset from one representation to another. Using these objects you can, for example, choose which of the tracks you want to be represented in the output file, specify your own output format, or modify the asset during the conversion process.</p>
<p>你可以使用 <code>export session</code> 将一个现有的<code>asset</code>重新编码为一个小数字，这个小数字是常用的预先设定好的一些小数字中的一个。如果在转换中你需要更多的控制，在iOS4.1已经以后的版本中，你可以使用 <code>asset reader</code> 和 <code>asset writer</code> 对象串联的一个一个的转换。例如你可以使用这些对象选择在输出的文件中想要表示的轨道，指定你自己的输出格式，或者在转换过程中修改这个<code>asset</code>。</p>
<p>To produce a visual representation of the waveform, you use an asset reader to read the audio track of an asset.</p>
<p>为了产生波形的可视化表示，你可以使用<code>asset reader</code>去读取<code>asset</code>中的音频轨道。</p>
<p>Relevant Chapter: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a></p>
<h3 id="Thumbnails-缩略图"><a href="#Thumbnails-缩略图" class="headerlink" title="Thumbnails - 缩略图"></a>Thumbnails - 缩略图</h3><p>To create thumbnail images of video presentations, you initialize an instance of <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/cl/AVAssetImageGenerator" target="_blank" rel="external">AVAssetImageGenerator</a> using the asset from which you want to generate thumbnails. AVAssetImageGenerator uses the default enabled video tracks to generate images.</p>
<p>创建视频演示图像的缩略图，使用想要生成缩略图的<code>asset</code>初始化一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/cl/AVAssetImageGenerator" target="_blank" rel="external">AVAssetImageGenerator</a> 的实例。<code>AVAssetImageGenerator</code> 使用默认启用视频轨道来生成图像。</p>
<p>Relevant Chapter: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a></p>
<h3 id="Editing-编辑"><a href="#Editing-编辑" class="headerlink" title="Editing - 编辑"></a>Editing - 编辑</h3><p>AVFoundation uses compositions to create new assets from existing pieces of media (typically, one or more video and audio tracks). You use a mutable composition to add and remove tracks, and adjust their temporal orderings. You can also set the relative volumes and ramping of audio tracks; and set the opacity, and opacity ramps, of video tracks. A composition is an assemblage of pieces of media held in memory. When you export a composition using an export session, it’s collapsed to a file.</p>
<p><code>AVFoundation</code> 使用 <code>compositions</code> 去从现有的媒体片段（通常是一个或多个视频和音频轨道）创建新的 <code>assets</code> 。你可以使用一个可变成分去添加和删除轨道，并调整它们的时间排序。你也可以设置相对音量和增加音频轨道；并且设置不透明度，浑浊坡道，视频跟踪。一种组合物，是一种在内存中存储的介质的组合。当年你使用 <code>export session</code> 导出一个成份，它会坍塌到一个文件中。</p>
<p>You can also create an asset from media such as sample buffers or still images using an asset writer.</p>
<p>你也可以从媒体上创建一个<code>asset</code>，比如使用<code>asset writer</code>.的示例缓冲区或静态图像。</p>
<p>Relevant Chapter: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1" target="_blank" rel="external">Editing</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1" target="_blank" rel="external">Editing</a></p>
<h3 id="Still-and-Video-Media-Capture-静态和视频媒体捕获"><a href="#Still-and-Video-Media-Capture-静态和视频媒体捕获" class="headerlink" title="Still and Video Media Capture - 静态和视频媒体捕获"></a>Still and Video Media Capture - 静态和视频媒体捕获</h3><p>Recording input from cameras and microphones is managed by a capture session. A capture session coordinates the flow of data from input devices to outputs such as a movie file. You can configure multiple inputs and outputs for a single session, even when the session is running. You send messages to the session to start and stop data flow.</p>
<p>从相机和麦克风记录输入是由一个 <code>capture session</code> 管理的。一个 <code>capture session</code> 协调从输入设备到输出的数据流，比如一个电影文件。你可以为一个单一的 <code>session</code> 配置多个输入和输出，甚至 <code>session</code> 正在运行的时候也可以。你将消息发送到 <code>session</code> 去启动和停止数据流。</p>
<p>In addition, you can use an instance of a preview layer to show the user what a camera is recording.</p>
<p>此外，你可以使用 <code>preview layer</code> 的一个实例来向用户显示一个相机是正在录制的。</p>
<p>Relevant Chapter: <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW2" target="_blank" rel="external">Still and Video Media Capture</a></p>
<p>相关章节：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW2" target="_blank" rel="external">Still and Video Media Capture</a></p>
<h3 id="Concurrent-Programming-with-AVFoundation-AVFoundation并发编程"><a href="#Concurrent-Programming-with-AVFoundation-AVFoundation并发编程" class="headerlink" title="Concurrent Programming with AVFoundation - AVFoundation并发编程"></a>Concurrent Programming with AVFoundation - AVFoundation并发编程</h3><p>Callbacks from AVFoundation—invocations of blocks, key-value observers, and notification handlers—are not guaranteed to be made on any particular thread or queue. Instead, AVFoundation invokes these handlers on threads or queues on which it performs its internal tasks.</p>
<p>从 <code>AVFoundation</code> 回调，比如块的调用、键值观察者以及通知处理程序，都不能保证在任何特定的线程或队列进行。相反，<code>AVFoundation</code> 在线程或者执行其内部任务的队列上调用这些处理程序。</p>
<p>There are two general guidelines as far as notifications and threading:</p>
<ul>
<li>UI related notifications occur on the main thread.</li>
<li>Classes or methods that require you create and/or specify a queue will return notifications on that queue.</li>
</ul>
<p>Beyond those two guidelines (and there are exceptions, which are noted in the reference documentation) you should not assume that a notification will be returned on any specific thread.</p>
<p>下面是两个有关通知和线程的准则</p>
<ul>
<li>在主线程上发生的与用户界面相关的通知。</li>
<li>需要创建并且/或者 指定一个队列的类或者方法将返回该队列的通知。</li>
</ul>
<p>除了这两个准则（当然是有一些例外，在参考文档中会被指出），你不应该假设一个通知将在任何特定的线程返回。</p>
<p>If you’re writing a multithreaded application, you can use the NSThread method <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSThread_Class/index.html#//apple_ref/occ/instm/NSThread/isMainThread" target="_blank" rel="external">isMainThread</a> or <code>[[NSThread currentThread] isEqual:&lt;#A stored thread reference#&gt;]</code> to test whether the invocation thread is a thread you expect to perform your work on. You can redirect messages to appropriate threads using methods such as <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSObject_Class/index.html#//apple_ref/occ/instm/NSObject/performSelectorOnMainThread:withObject:waitUntilDone:" target="_blank" rel="external">performSelectorOnMainThread:withObject:waitUntilDone:</a> and <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSObject_Class/index.html#//apple_ref/occ/instm/NSObject/performSelector:onThread:withObject:waitUntilDone:modes:" target="_blank" rel="external">performSelector:onThread:withObject:waitUntilDone:modes:</a>. You could also use <a href="https://developer.apple.com/library/ios/documentation/Performance/Reference/GCD_libdispatch_Ref/index.html#//apple_ref/c/func/dispatch_async" target="_blank" rel="external">dispatch_async</a> to “bounce” to your blocks on an appropriate queue, either the main queue for UI tasks or a queue you have up for concurrent operations. For more about concurrent operations, see <a href="https://developer.apple.com/library/ios/documentation/General/Conceptual/ConcurrencyProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40008091" target="_blank" rel="external">Concurrency Programming Guide</a>; for more about blocks, see <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/Blocks/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40007502" target="_blank" rel="external">Blocks Programming Topics</a>. The <a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a> sample code is considered the primary example for all AVFoundation functionality and can be consulted for examples of thread and queue usage with AVFoundation.</p>
<p>如果你在写一个多线程的应用程序，你可以使用 <code>NSThread</code>  方法 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSThread_Class/index.html#//apple_ref/occ/instm/NSThread/isMainThread" target="_blank" rel="external">isMainThread</a> 或者 <code>[[NSThread currentThread] isEqual:&lt;#A stored thread reference#&gt;]</code> 去测试是否调用了你期望执行你任务的线程。你可以使用方法重定向 消息给适合的线程，比如 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSObject_Class/index.html#//apple_ref/occ/instm/NSObject/performSelectorOnMainThread:withObject:waitUntilDone:" target="_blank" rel="external">performSelectorOnMainThread:withObject:waitUntilDone:</a> 以及  <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSObject_Class/index.html#//apple_ref/occ/instm/NSObject/performSelector:onThread:withObject:waitUntilDone:modes:" target="_blank" rel="external">performSelector:onThread:withObject:waitUntilDone:modes:</a>.你也可以使用 <a href="https://developer.apple.com/library/ios/documentation/Performance/Reference/GCD_libdispatch_Ref/index.html#//apple_ref/c/func/dispatch_async" target="_blank" rel="external">dispatch_async</a>弹回到适当队列的 <code>blocks</code> 中，无论是在主界面的任务队列还是有了并发操作的队列。更多关于并行操作，请查看 <a href="https://developer.apple.com/library/ios/documentation/General/Conceptual/ConcurrencyProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40008091" target="_blank" rel="external">Concurrency Programming Guide</a>；更多关于块，请查看 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/Blocks/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40007502" target="_blank" rel="external">Blocks Programming Topics</a>.  <a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a> 示例代码是所有 <code>AVFoundation</code>  功能最主要的例子，可以对线程和队列使用 <code>AVFoundation</code> 实例参考。</p>
<h2 id="Prerequisites-预备知识"><a href="#Prerequisites-预备知识" class="headerlink" title="Prerequisites - 预备知识"></a>Prerequisites - 预备知识</h2><p>AVFoundation is an advanced Cocoa framework. To use it effectively, you must have:</p>
<ul>
<li>A solid understanding of fundamental Cocoa development tools and techniques</li>
<li>A basic grasp of blocks</li>
<li>A basic understanding of key-value coding and key-value observing</li>
<li>For playback, a basic understanding of Core Animation (see <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514" target="_blank" rel="external">Core Animation Programming Guide</a> or, for basic playback, the <a href="https://developer.apple.com/library/ios/documentation/AVKit/Reference/AVKitFramework/index.html#//apple_ref/doc/uid/TP40013178" target="_blank" rel="external">AVKit Framework Reference</a>.</li>
</ul>
<p><code>AVFoundation</code> 是一种先进的 <code>Cocoa</code> 框架，为了有效的使用，你必须掌握下面的知识：</p>
<ul>
<li>扎实的了解基本的 <code>Cocoa</code> 开发工具和框架</li>
<li>对块有基本的了解</li>
<li>了解基本的键值编码(<code>key-value coding</code>)和键值观察（<code>key-value observing</code>）</li>
<li>对于播放，对核心动画的基本理解 (see <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514" target="_blank" rel="external">Core Animation Programming Guide</a> )或者,对于基本播放, 请看 <a href="https://developer.apple.com/library/ios/documentation/AVKit/Reference/AVKitFramework/index.html#//apple_ref/doc/uid/TP40013178" target="_blank" rel="external">AVKit Framework Reference</a>.</li>
</ul>
<h2 id="See-Also-参考"><a href="#See-Also-参考" class="headerlink" title="See Also - 参考"></a>See Also - 参考</h2><p>There are several AVFoundation examples including two that are key to understanding and implementation Camera capture functionality:</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a> is the canonical sample code for implementing any program that uses the camera functionality. It is a complete sample, well documented, and covers the majority of the functionality showing the best practices.</li>
<li><a href="https://developer.apple.com/library/ios/samplecode/AVCamManual/Introduction/Intro.html#//apple_ref/doc/uid/TP40014578" target="_blank" rel="external">AVCamManual: Extending AVCam to Use Manual Capture API</a> is the companion application to AVCam. It implements Camera functionality using the manual camera controls. It is also a complete example, well documented, and should be considered the canonical example for creating camera applications that take advantage of manual controls.</li>
<li><a href="https://developer.apple.com/library/ios/samplecode/RosyWriter/Introduction/Intro.html#//apple_ref/doc/uid/DTS40011110" target="_blank" rel="external">RosyWriter</a> is an example that demonstrates real time frame processing and in particular how to apply filters to video content. This is a very common developer requirement and this example covers that functionality.</li>
<li>AVLocationPlayer: Using AVFoundation Metadata Reading APIs demonstrates using the metadata APIs.</li>
</ul>
<p>有几个 <code>AVFoundation</code> 的例子，包括两个理解和实现摄像头捕捉功能的关键点：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a> 是实现任何想使用摄像头功能的程序的典型示例代码。它是一个完整的样本，以及记录，并涵盖了大部分主要的功能。</li>
<li><a href="https://developer.apple.com/library/ios/samplecode/AVCamManual/Introduction/Intro.html#//apple_ref/doc/uid/TP40014578" target="_blank" rel="external">AVCamManual: Extending AVCam to Use Manual Capture API</a> 是AVCam相对应的应用程序。它使用手动相机控制实现相机功能。它也是一个完成的例子，以及记录，并且应该被视为利用手动控制创建相机应用程序的典型例子。</li>
<li><a href="https://developer.apple.com/library/ios/samplecode/RosyWriter/Introduction/Intro.html#//apple_ref/doc/uid/DTS40011110" target="_blank" rel="external">RosyWriter</a> 是一个演示实时帧处理的例子，特别是如果过滤器应用到视频内容。这是一个非常普遍的开发人员的需求，这个例子涵盖了这个功能。</li>
<li><code>AVLocationPlayer</code>: 使用 <code>AVFoundation Metadata Reading APIs</code> 演示使用 <code>the metadata APIs</code>.</li>
</ul>
<h1 id="Using-Assets-使用Assets"><a href="#Using-Assets-使用Assets" class="headerlink" title="Using Assets - 使用Assets"></a>Using Assets - 使用Assets</h1><p>Assets can come from a file or from media in the user’s iPod library or Photo library. When you create an asset object all the information that you might want to retrieve for that item is not immediately available. Once you have a movie asset, you can extract still images from it, transcode it to another format, or trim the contents.</p>
<p><code>Assets</code> 可以来自文件或者媒体用户的iPod库、图片库。当你创建一个 <code>asset</code> 对象时，所有你可能想要检索该项目的信息不是立即可用的。一旦你有了一个电影 <code>asset</code> ，你可以从里面提取静态图像，转换到另一个格式，或者对内容就行修剪。</p>
<h2 id="Creating-an-Asset-Object-创建一个Asset对象"><a href="#Creating-an-Asset-Object-创建一个Asset对象" class="headerlink" title="Creating an Asset Object - 创建一个Asset对象"></a>Creating an Asset Object - 创建一个Asset对象</h2><p>To create an asset to represent any resource that you can identify using a URL, you use <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/occ/cl/AVURLAsset" target="_blank" rel="external">AVURLAsset</a>. The simplest case is creating an asset from a file:</p>
<p>为了创建一个 <code>asset</code> ，去代表任何你能用一个 <code>URL</code> 识别的资源，你可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/occ/cl/AVURLAsset" target="_blank" rel="external">AVURLAsset</a> .最简单的情况是从一个文件创建一个 <code>asset</code> 。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSURL</span> *url = &lt;<span class="meta">#A URL that identifies an audiovisual asset such as a movie file#&gt;;</span></span><br><span class="line"><span class="built_in">AVURLAsset</span> *anAsset = [[<span class="built_in">AVURLAsset</span> alloc] initWithURL:url options:<span class="literal">nil</span>];</span><br></pre></td></tr></table></figure>
<h3 id="Options-for-Initializing-an-Asset-初始化一个Asset的选择"><a href="#Options-for-Initializing-an-Asset-初始化一个Asset的选择" class="headerlink" title="Options for Initializing an Asset - 初始化一个Asset的选择"></a>Options for Initializing an Asset - 初始化一个Asset的选择</h3><p>The AVURLAsset initialization methods take as their second argument an options dictionary. The only key used in the dictionary is <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/c/data/AVURLAssetPreferPreciseDurationAndTimingKey" target="_blank" rel="external">AVURLAssetPreferPreciseDurationAndTimingKey</a>. The corresponding value is a Boolean (contained in an NSValue object) that indicates whether the asset should be prepared to indicate a precise duration and provide precise random access by time.</p>
<p><code>AVURLAsset</code> 初始化方法作为它们的第二个参数选项字典。本字典中唯一被使用的 <code>key</code> 是 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/c/data/AVURLAssetPreferPreciseDurationAndTimingKey" target="_blank" rel="external">AVURLAssetPreferPreciseDurationAndTimingKey</a>. 相应的值是一个布尔值（包含在一个 <code>NSValue</code> 对象中），这个布尔值指出是否该 <code>asset</code> 应该准备标出一个精确的时间和提供一个以时间为种子的随机存取。</p>
<p>Getting the exact duration of an asset may require significant processing overhead. Using an approximate duration is typically a cheaper operation and sufficient for playback. Thus:</p>
<ul>
<li>If you only intend to play the asset, either pass nil instead of a dictionary, or pass a dictionary that contains the AVURLAssetPreferPreciseDurationAndTimingKey key and a corresponding value of NO (contained in an NSValue object).</li>
<li>If you want to add the asset to a composition (<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a>), you typically need precise random access. Pass a dictionary that contains theAVURLAssetPreferPreciseDurationAndTimingKey key and a corresponding value of YES (contained in an NSValue object—recall that NSNumberinherits from NSValue):</li>
</ul>
<p>获得一个asset的确切持续时间可能需要大量的处理开销。使用一个近似的持续时间通常是一个更便宜的操作并且对于播放已经足够了。因此：</p>
<ul>
<li>如果你只打算播放这个 <code>asset</code>， 要么传递一个 <code>nil</code> 代替 <code>dictionary</code> ，或者传递一个字典，这个字典包含 <code>AVURLAssetPreferPreciseDurationAndTimingKey</code> 的 <code>key</code>和相应 <code>NO</code>(包含在一个 <code>NSValue</code> 对象) 的值。</li>
<li>如果你想要把 <code>asset</code> 添加给一个 <code>composition</code> (<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a>), 通常你需要精确的随机存取。传递一个字典（这个字典包含 <code>AVURLAssetPreferPreciseDurationAndTimingKey</code> key） 和一个相应的 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/ObjCRuntimeRef/index.html#//apple_ref/doc/c_ref/YES" target="_blank" rel="external">YES</a> 的值（YES 包含在一个 <code>NSValue</code> 对象中，回忆一下继承自 <code>NSValue</code>的 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Classes/NSNumber_Class/index.html#//apple_ref/occ/cl/NSNumber" target="_blank" rel="external">NSNmuber</a>）</li>
</ul>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSURL</span> *url = &lt;<span class="meta">#A URL that identifies an audiovisual asset such as a movie file#&gt;;</span></span><br><span class="line"><span class="built_in">NSDictionary</span> *options = @&#123; <span class="built_in">AVURLAssetPreferPreciseDurationAndTimingKey</span> : @YES &#125;;</span><br><span class="line"><span class="built_in">AVURLAsset</span> *anAssetToUseInAComposition = [[<span class="built_in">AVURLAsset</span> alloc] initWithURL:url options:options];</span><br></pre></td></tr></table></figure>
<h3 id="Accessing-the-User’s-Assets-访问用户的Assets"><a href="#Accessing-the-User’s-Assets-访问用户的Assets" class="headerlink" title="Accessing the User’s Assets - 访问用户的Assets"></a>Accessing the User’s Assets - 访问用户的<code>Assets</code></h3><p>To access the assets managed by the iPod library or by the Photos application, you need to get a URL of the asset you want.</p>
<ul>
<li>To access the iPod Library, you create an <a href="https://developer.apple.com/library/ios/documentation/MediaPlayer/Reference/MPMediaQuery_ClassReference/index.html#//apple_ref/occ/cl/MPMediaQuery" target="_blank" rel="external">MPMediaQuery</a> instance to find the item you want, then get its URL using <a href="https://developer.apple.com/library/ios/documentation/MediaPlayer/Reference/MPMediaItem_ClassReference/index.html#//apple_ref/c/data/MPMediaItemPropertyAssetURL" target="_blank" rel="external">MPMediaItemPropertyAssetURL</a>.For more about the Media Library, see Multimedia Programming Guide.</li>
<li>To access the assets managed by the Photos application, you use <a href="https://developer.apple.com/library/ios/documentation/AssetsLibrary/Reference/ALAssetsLibrary_Class/index.html#//apple_ref/occ/cl/ALAssetsLibrary" target="_blank" rel="external">ALAssetsLibrary</a>.</li>
</ul>
<p>The following example shows how you can get an asset to represent the first video in the Saved Photos Album.</p>
<p>为了访问由 iPod 库或者照片应用程序管理的 <code>assets</code> ，你需要得到你想要 <code>asset</code> 的一个 <code>URL</code>。</p>
<ul>
<li>为了访问 iPod 库，创建一个 <a href="https://developer.apple.com/library/ios/documentation/MediaPlayer/Reference/MPMediaQuery_ClassReference/index.html#//apple_ref/occ/cl/MPMediaQuery" target="_blank" rel="external">MPMediaQuery</a> 实例去找到你想要的项目，然后使用<a href="https://developer.apple.com/library/ios/documentation/MediaPlayer/Reference/MPMediaItem_ClassReference/index.html#//apple_ref/c/data/MPMediaItemPropertyAssetURL" target="_blank" rel="external">MPMediaItemPropertyAssetURL</a>得到它的 <code>URL</code> ，</li>
<li>为了访问有照片应用程序管理的 <code>assets</code> ，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AssetsLibrary/Reference/ALAssetsLibrary_Class/index.html#//apple_ref/occ/cl/ALAssetsLibrary" target="_blank" rel="external">ALAssetsLibrary</a>。</li>
</ul>
<p>下面的例子展示了如何获得一个 <code>asset</code> 来保存照片相册中的第一个视频。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ALAssetsLibrary *library = [[ALAssetsLibrary alloc] init];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Enumerate just the photos and videos group by using ALAssetsGroupSavedPhotos.</span></span><br><span class="line">[library enumerateGroupsWithTypes:ALAssetsGroupSavedPhotos usingBlock:^(ALAssetsGroup *group, <span class="built_in">BOOL</span> *stop) &#123;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Within the group enumeration block, filter to enumerate just videos.</span></span><br><span class="line">[group setAssetsFilter:[ALAssetsFilter allVideos]];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// For this example, we're only interested in the first item.</span></span><br><span class="line">[group enumerateAssetsAtIndexes:[<span class="built_in">NSIndexSet</span> indexSetWithIndex:<span class="number">0</span>]</span><br><span class="line">                        options:<span class="number">0</span></span><br><span class="line">                     usingBlock:^(ALAsset *alAsset, <span class="built_in">NSUInteger</span> index, <span class="built_in">BOOL</span> *innerStop) &#123;</span><br><span class="line"> </span><br><span class="line">                         <span class="comment">// The end of the enumeration is signaled by asset == nil.</span></span><br><span class="line">                         <span class="keyword">if</span> (alAsset) &#123;</span><br><span class="line">                             ALAssetRepresentation *representation = [alAsset defaultRepresentation];</span><br><span class="line">                             <span class="built_in">NSURL</span> *url = [representation url];</span><br><span class="line">                             <span class="built_in">AVAsset</span> *avAsset = [<span class="built_in">AVURLAsset</span> URLAssetWithURL:url options:<span class="literal">nil</span>];</span><br><span class="line">                             <span class="comment">// Do something interesting with the AV asset.</span></span><br><span class="line">                         &#125;</span><br><span class="line">                     &#125;];</span><br><span class="line">                 &#125;</span><br><span class="line">                 failureBlock: ^(<span class="built_in">NSError</span> *error) &#123;</span><br><span class="line">                     <span class="comment">// Typically you should handle an error more gracefully than this.</span></span><br><span class="line">                     <span class="built_in">NSLog</span>(<span class="string">@"No groups"</span>);</span><br><span class="line">                 &#125;];</span><br></pre></td></tr></table></figure>
<h2 id="Preparing-an-Asset-for-Use-将-Asset-准备好使用"><a href="#Preparing-an-Asset-for-Use-将-Asset-准备好使用" class="headerlink" title="Preparing an Asset for Use - 将 Asset 准备好使用"></a>Preparing an Asset for Use - 将 <code>Asset</code> 准备好使用</h2><p>Initializing an asset (or track) does not necessarily mean that all the information that you might want to retrieve for that item is immediately available. It may require some time to calculate even the duration of an item (an MP3 file, for example, may not contain summary information). Rather than blocking the current thread while a value is being calculated, you should use the <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intf/AVAsynchronousKeyValueLoading" target="_blank" rel="external">AVAsynchronousKeyValueLoading</a>  protocol to ask for values and get an answer back later through a completion handler you define using a block. (AVAsset and AVAssetTrack conform to the AVAsynchronousKeyValueLoading protocol.)</p>
<p>初始化一个 <code>asset</code> （或者轨道）并不意味着你可能想要检索该项的所有信息是立即可用的。这可能需要一些时间来计算一个项目的持续时间（例如一个 MP3 文件可能不包含摘要信息）。当一个值被计算的时候不应该阻塞当前线程，你应该使用<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intf/AVAsynchronousKeyValueLoading" target="_blank" rel="external">AVAsynchronousKeyValueLoading</a> 协议去请求值，通过完成处理你定义使用的一个 <code>block</code> 后得到答复。(<code>AVAsset</code> and <code>AVAssetTrack</code> 遵循 <code>AVAsynchronousKeyValueLoading</code> 协议.)</p>
<p>You test whether a value is loaded for a property using <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intfm/AVAsynchronousKeyValueLoading/statusOfValueForKey:error:" target="_blank" rel="external">statusOfValueForKey:error:</a>. When an asset is first loaded, the value of most or all of its properties is <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/c/econst/AVKeyValueStatusUnknown" target="_blank" rel="external">AVKeyValueStatusUnknown</a>. To load a value for one or more properties, you invoke <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intfm/AVAsynchronousKeyValueLoading/loadValuesAsynchronouslyForKeys:completionHandler:" target="_blank" rel="external">loadValuesAsynchronouslyForKeys:completionHandler:</a>. In the completion handler, you take whatever action is appropriate depending on the property’s status. You should always be prepared for loading to not complete successfully, either because it failed for some reason such as a network-based URL being inaccessible, or because the load was canceled.</p>
<p>测试一个值是否是使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intfm/AVAsynchronousKeyValueLoading/statusOfValueForKey:error:" target="_blank" rel="external">statusOfValueForKey:error:</a> 加载为一个属性。当 <code>asset</code> 被首次加载时，大部分的或全部属性值是 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/c/econst/AVKeyValueStatusUnknown" target="_blank" rel="external">AVKeyValueStatusUnknown</a>。为一个或多个属性加载一个值，调用<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsynchronousKeyValueLoading_Protocol/index.html#//apple_ref/occ/intfm/AVAsynchronousKeyValueLoading/loadValuesAsynchronouslyForKeys:completionHandler:" target="_blank" rel="external">loadValuesAsynchronouslyForKeys:completionHandler:</a>。在完成处理程序中，你采取的行动是否恰当，取决于属性的状态。你应该总是准备加载不会完全成功，它可能有一些原因，比如基于网络的 <code>URL</code>是无法访问的，或者因为负载被取消。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSURL</span> *url = &lt;<span class="meta">#A URL that identifies an audiovisual asset such as a movie file#&gt;;</span></span><br><span class="line"><span class="built_in">AVURLAsset</span> *anAsset = [[<span class="built_in">AVURLAsset</span> alloc] initWithURL:url options:<span class="literal">nil</span>];</span><br><span class="line"><span class="built_in">NSArray</span> *keys = @[<span class="string">@"duration"</span>];</span><br><span class="line"> </span><br><span class="line">[asset loadValuesAsynchronouslyForKeys:keys completionHandler:^() &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSError</span> *error = <span class="literal">nil</span>;</span><br><span class="line">    <span class="built_in">AVKeyValueStatus</span> tracksStatus = [asset statusOfValueForKey:<span class="string">@"duration"</span> error:&amp;error];</span><br><span class="line">    <span class="keyword">switch</span> (tracksStatus) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="built_in">AVKeyValueStatusLoaded</span>:</span><br><span class="line">            [<span class="keyword">self</span> updateUserInterfaceForDuration];</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="built_in">AVKeyValueStatusFailed</span>:</span><br><span class="line">            [<span class="keyword">self</span> reportError:error forAsset:asset];</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="built_in">AVKeyValueStatusCancelled</span>:</span><br><span class="line">            <span class="comment">// Do whatever is appropriate for cancelation.</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<p>If you want to prepare an asset for playback, you should load its tracks property. For more about playing assets, see Playback.</p>
<p>如果你想准备一个 <code>asset</code> 去播放，你应该加载它的轨道属性。更多有关播放 <code>assets</code>，请看 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW1" target="_blank" rel="external">Playback</a></p>
<h2 id="Getting-Still-Images-From-a-Video-从视频中获取静态图像"><a href="#Getting-Still-Images-From-a-Video-从视频中获取静态图像" class="headerlink" title="Getting Still Images From a Video - 从视频中获取静态图像"></a>Getting Still Images From a Video - 从视频中获取静态图像</h2><p>To get still images such as thumbnails from an asset for playback, you use an AVAssetImageGenerator object. You initialize an image generator with your asset. Initialization may succeed, though, even if the asset possesses no visual tracks at the time of initialization, so if necessary you should test whether the asset has any tracks with the visual characteristic using tracksWithMediaCharacteristic:.</p>
<p>为了从一个准备播放的 <code>asset</code> 中得到静态图像，比如缩略图，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/cl/AVAssetImageGenerator" target="_blank" rel="external">AVAssetImageGenerator</a> 对象。用你的 <code>asset</code> 初始化一个图像发生器。不过即使 <code>asset</code> 进程在初始化的时候没有视觉跟踪，也可以成功，所以如果有必要，你应该测试一下， <code>asset</code> 是否有轨道有使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/instm/AVAsset/tracksWithMediaCharacteristic:" target="_blank" rel="external">tracksWithMediaCharacteristic</a> 的视觉特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AVAsset anAsset = &lt;#Get an asset#&gt;;</span><br><span class="line">if ([[anAsset tracksWithMediaType:AVMediaTypeVideo] count] &gt; 0) &#123;</span><br><span class="line">    AVAssetImageGenerator *imageGenerator =</span><br><span class="line">        [AVAssetImageGenerator assetImageGeneratorWithAsset:anAsset];</span><br><span class="line">    // Implementation continues...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You can configure several aspects of the image generator, for example, you can specify the maximum dimensions for the images it generates and the aperture mode using maximumSize and apertureMode respectively.You can then generate a single image at a given time, or a series of images. You must ensure that you keep a strong reference to the image generator until it has generated all the images.</p>
<p>你可以配置几个图像发生器的部分，例如，可以指定生成的图像采用最大值，并且光圈的模式分别使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/instm/AVAssetImageGenerator/maximumSize" target="_blank" rel="external">maximumSize</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/instm/AVAssetImageGenerator/apertureMode" target="_blank" rel="external">apertureMode</a> 。然后可以在给定的时间生成一个单独的图像，或者一系列图像。你必须确定，在生成所有图像之前，必须对图像生成器保持一个强引用。</p>
<h3 id="Generating-a-Single-Image-生成一个单独的图像"><a href="#Generating-a-Single-Image-生成一个单独的图像" class="headerlink" title="Generating a Single Image - 生成一个单独的图像"></a>Generating a Single Image - 生成一个单独的图像</h3><p>You use copyCGImageAtTime:actualTime:error: to generate a single image at a specific time. AVFoundation may not be able to produce an image at exactly the time you request, so you can pass as the second argument a pointer to a CMTime that upon return contains the time at which the image was actually generated.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/instm/AVAssetImageGenerator/copyCGImageAtTime:actualTime:error:" target="_blank" rel="external">copyCGImageAtTime:actualTime:error:</a> 方法在指定时间生成一个图像。AVFoundation 在你要求的确切时间可能无法产生一个图像，所以你可以将一个指向 CMTime 的指针当做第二个参数穿过去，这个指针返回的时候包含图像被实际生成的时间。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *myAsset = &lt;<span class="meta">#An asset#&gt;];</span></span><br><span class="line"><span class="built_in">AVAssetImageGenerator</span> *imageGenerator = [[<span class="built_in">AVAssetImageGenerator</span> alloc] initWithAsset:myAsset];</span><br><span class="line"> </span><br><span class="line">Float64 durationSeconds = CMTimeGetSeconds([myAsset duration]);</span><br><span class="line">CMTime midpoint = CMTimeMakeWithSeconds(durationSeconds/<span class="number">2.0</span>, <span class="number">600</span>);</span><br><span class="line"><span class="built_in">NSError</span> *error;</span><br><span class="line">CMTime actualTime;</span><br><span class="line"> </span><br><span class="line"><span class="built_in">CGImageRef</span> halfWayImage = [imageGenerator <span class="keyword">copy</span><span class="built_in">CGImageAtTime</span>:midpoint actualTime:&amp;actualTime error:&amp;error];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (halfWayImage != <span class="literal">NULL</span>) &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSString</span> *actualTimeString = (<span class="built_in">NSString</span> *)CMTimeCopyDescription(<span class="literal">NULL</span>, actualTime);</span><br><span class="line">    <span class="built_in">NSString</span> *requestedTimeString = (<span class="built_in">NSString</span> *)CMTimeCopyDescription(<span class="literal">NULL</span>, midpoint);</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"Got halfWayImage: Asked for %@, got %@"</span>, requestedTimeString, actualTimeString);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Do something interesting with the image.</span></span><br><span class="line">    <span class="built_in">CGImageRelease</span>(halfWayImage);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Generating-a-Sequence-of-Images-生成一系列图像"><a href="#Generating-a-Sequence-of-Images-生成一系列图像" class="headerlink" title="Generating a Sequence of Images - 生成一系列图像"></a>Generating a Sequence of Images - 生成一系列图像</h2><p>To generate a series of images, you send the image generator a generateCGImagesAsynchronouslyForTimes:completionHandler: message. The first argument is an array of NSValue objects, each containing a CMTime structure, specifying the asset times for which you want images to be generated. The second argument is a block that serves as a callback invoked for each image that is generated. The block arguments provide a result constant that tells you whether the image was created successfully or if the operation was canceled, and, as appropriate:</p>
<ul>
<li>The image</li>
<li>The time for which you requested the image and the actual time for which the image was generated</li>
<li>An error object that describes the reason generation failed</li>
</ul>
<p>In your implementation of the block, check the result constant to determine whether the image was created. In addition, ensure that you keep a strong reference to the image generator until it has finished creating the images.</p>
<p>生成一系列图像，可以给图像生成器发送 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/instm/AVAssetImageGenerator/generateCGImagesAsynchronouslyForTimes:completionHandler:" target="_blank" rel="external">generateCGImagesAsynchronouslyForTimes:completionHandler:</a> 消息。第一个参数是一个 <code>NSValue</code> 对象的数组，每个都包含一个 <code>CMTime</code> 结构体，指定了图像想要被生成的 <code>asset</code> 时间。<code>block</code> 参数提供了一个结果，这个结果包含了告诉你是否图像被成功生成，或者操作某些情况下被取消。结果：</p>
<ul>
<li>图像</li>
<li>你要求的图像和图像生成的实际时间</li>
<li>一个 <code>error</code> 对象，描述了生成失败的原因</li>
</ul>
<p>在 <code>block</code> 的实现中，检查结果常数，来确定图像是否被创建。此外，在完成创建图像之前，确保保持一个强引用给图像生成器。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *myAsset = &lt;<span class="meta">#An asset#&gt;];</span></span><br><span class="line"><span class="comment">// Assume: @property (strong) AVAssetImageGenerator *imageGenerator;</span></span><br><span class="line"><span class="keyword">self</span>.imageGenerator = [<span class="built_in">AVAssetImageGenerator</span> assetImageGeneratorWithAsset:myAsset];</span><br><span class="line"> </span><br><span class="line">Float64 durationSeconds = CMTimeGetSeconds([myAsset duration]);</span><br><span class="line">CMTime firstThird = CMTimeMakeWithSeconds(durationSeconds/<span class="number">3.0</span>, <span class="number">600</span>);</span><br><span class="line">CMTime secondThird = CMTimeMakeWithSeconds(durationSeconds*<span class="number">2.0</span>/<span class="number">3.0</span>, <span class="number">600</span>);</span><br><span class="line">CMTime end = CMTimeMakeWithSeconds(durationSeconds, <span class="number">600</span>);</span><br><span class="line"><span class="built_in">NSArray</span> *times = @[<span class="built_in">NSValue</span> valueWithCMTime:kCMTimeZero],</span><br><span class="line">                  [<span class="built_in">NSValue</span> valueWithCMTime:firstThird], [<span class="built_in">NSValue</span> valueWithCMTime:secondThird],</span><br><span class="line">                  [<span class="built_in">NSValue</span> valueWithCMTime:end]];</span><br><span class="line"> </span><br><span class="line">[imageGenerator generate<span class="built_in">CGImagesAsynchronouslyForTimes</span>:times</span><br><span class="line">                completionHandler:^(CMTime requestedTime, <span class="built_in">CGImageRef</span> image, CMTime actualTime,</span><br><span class="line">                                    <span class="built_in">AVAssetImageGeneratorResult</span> result, <span class="built_in">NSError</span> *error) &#123;</span><br><span class="line"> </span><br><span class="line">                <span class="built_in">NSString</span> *requestedTimeString = (<span class="built_in">NSString</span> *)</span><br><span class="line">                    <span class="built_in">CFBridgingRelease</span>(CMTimeCopyDescription(<span class="literal">NULL</span>, requestedTime));</span><br><span class="line">                <span class="built_in">NSString</span> *actualTimeString = (<span class="built_in">NSString</span> *)</span><br><span class="line">                    <span class="built_in">CFBridgingRelease</span>(CMTimeCopyDescription(<span class="literal">NULL</span>, actualTime));</span><br><span class="line">                <span class="built_in">NSLog</span>(<span class="string">@"Requested: %@; actual %@"</span>, requestedTimeString, actualTimeString);</span><br><span class="line"> </span><br><span class="line">                <span class="keyword">if</span> (result == <span class="built_in">AVAssetImageGeneratorSucceeded</span>) &#123;</span><br><span class="line">                    <span class="comment">// Do something interesting with the image.</span></span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="keyword">if</span> (result == <span class="built_in">AVAssetImageGeneratorFailed</span>) &#123;</span><br><span class="line">                    <span class="built_in">NSLog</span>(<span class="string">@"Failed with error: %@"</span>, [error localizedDescription]);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (result == <span class="built_in">AVAssetImageGeneratorCancelled</span>) &#123;</span><br><span class="line">                    <span class="built_in">NSLog</span>(<span class="string">@"Canceled"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">  &#125;];</span><br></pre></td></tr></table></figure>
<p>You can cancel the generation of the image sequence by sending the image generator a cancelAllCGImageGeneration message.</p>
<p>你发送给图像生成器一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetImageGenerator_Class/index.html#//apple_ref/occ/instm/AVAssetImageGenerator/cancelAllCGImageGeneration" target="_blank" rel="external">cancelAllCGImageGeneration</a> 消息，可以取消队列中的图像生成。</p>
<h2 id="Trimming-and-Transcoding-a-Movie-微调和转化为一个电影"><a href="#Trimming-and-Transcoding-a-Movie-微调和转化为一个电影" class="headerlink" title="Trimming and Transcoding a Movie - 微调和转化为一个电影"></a>Trimming and Transcoding a Movie - 微调和转化为一个电影</h2><p>You can transcode a movie from one format to another, and trim a movie, using an AVAssetExportSession object. The workflow is shown in Figure 1-1. An export session is a controller object that manages asynchronous export of an asset. You initialize the session using the asset you want to export and the name of a export preset that indicates the export options you want to apply (see allExportPresets). You then configure the export session to specify the output URL and file type, and optionally other settings such as the metadata and whether the output should be optimized for network use.</p>
<blockquote>
<p><code>asset</code>一律使用“资产”代码，切换还要加``略麻烦</p>
</blockquote>
<p>你可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/cl/AVAssetExportSession" target="_blank" rel="external">AVAssetExportSession</a> 对象，将一个电影的编码进行转换，并且对电影进行微调。工作流程如图1-1所示。一个 <code>export session</code> 是一个控制器对象，管理一个资产的异步导出。使用想要导出的资产初始化一个 <code>session</code> 和输出设定的名称，这个输出设定表明你想申请的导出选项（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/clm/AVAssetExportSession/allExportPresets" target="_blank" rel="external">allExportPresets</a>）。然后配置导出会话去指定输出的 URL 和文件类型，以及其他可选的设定，比如元数据，是否将输出优化用于网络使用。</p>
<center><br><img src="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6f7bsp7lbj20oo04u74b.jpg" alt="Figure 1-1  The export session workflow"><br></center>

<p>You can check whether you can export a given asset using a given preset using exportPresetsCompatibleWithAsset: as illustrated in this example:</p>
<p>你可以检查你能否用给定的预设导出一个给定的资产，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/clm/AVAssetExportSession/exportPresetsCompatibleWithAsset:" target="_blank" rel="external">exportPresetsCompatibleWithAsset:</a> 作为示例。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *anAsset = &lt;<span class="meta">#Get an asset#&gt;;</span></span><br><span class="line"><span class="built_in">NSArray</span> *compatiblePresets = [<span class="built_in">AVAssetExportSession</span> exportPresetsCompatibleWithAsset:anAsset];</span><br><span class="line"><span class="keyword">if</span> ([compatiblePresets containsObject:<span class="built_in">AVAssetExportPresetLowQuality</span>]) &#123;</span><br><span class="line">    <span class="built_in">AVAssetExportSession</span> *exportSession = [[<span class="built_in">AVAssetExportSession</span> alloc]</span><br><span class="line">        initWithAsset:anAsset presetName:<span class="built_in">AVAssetExportPresetLowQuality</span>];</span><br><span class="line">    <span class="comment">// Implementation continues.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You complete the configuration of the session by providing the output URL (The URL must be a file URL.) AVAssetExportSession can infer the output file type from the URL’s path extension; typically, however, you set it directly using outputFileType. You can also specify additional properties such as the time range, a limit for the output file length, whether the exported file should be optimized for network use, and a video composition. The following example illustrates how to use the timeRange property to trim the movie:</p>
<p>完成会话的配置，是由输出的 <code>URL</code> （URL 必须是文件的 URL）控制的。AVAssetExportSession可以从 <code>URL</code> 的路径延伸推断输出文件的类型。然而通常情况下，直接使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instm/AVAssetExportSession/setOutputFileType:" target="_blank" rel="external">outputFileType</a> 设定。还可以指定附加属性，如时间范围、输出文件长度的限制、导出的文件是否应该为了网络使用而优化、还有一个视频的构成。下面的示例展示了如果使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instm/AVAssetExportSession/setTimeRange:" target="_blank" rel="external">timeRange</a> 属性修剪电影。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">exportSession.outputURL = &lt;<span class="meta">#A file URL#&gt;;</span></span><br><span class="line">exportSession.outputFileType = <span class="built_in">AVFileTypeQuickTimeMovie</span>;</span><br><span class="line"> </span><br><span class="line">CMTime start = CMTimeMakeWithSeconds(<span class="number">1.0</span>, <span class="number">600</span>);</span><br><span class="line">CMTime duration = CMTimeMakeWithSeconds(<span class="number">3.0</span>, <span class="number">600</span>);</span><br><span class="line">CMTimeRange range = CMTimeRangeMake(start, duration);</span><br><span class="line">exportSession.timeRange = range;</span><br></pre></td></tr></table></figure>
<p>To create the new file, you invoke exportAsynchronouslyWithCompletionHandler:. The completion handler block is called when the export operation finishes; in your implementation of the handler, you should check the session’s status value to determine whether the export was successful, failed, or was canceled:</p>
<p>调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instm/AVAssetExportSession/exportAsynchronouslyWithCompletionHandler:" target="_blank" rel="external">exportAsynchronouslyWithCompletionHandler:</a> 创建新的文件。当导出操作完成的时候完成处理的 <code>block</code> 被调用，你应该检查会话的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instp/AVAssetExportSession/status" target="_blank" rel="external">status</a> 值，去判断导出是否成功、失败或者被取消。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[exportSession exportAsynchronouslyWithCompletionHandler:^&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">switch</span> ([exportSession status]) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="built_in">AVAssetExportSessionStatusFailed</span>:</span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"Export failed: %@"</span>, [[exportSession error] localizedDescription]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="built_in">AVAssetExportSessionStatusCancelled</span>:</span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"Export canceled"</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<p>You can cancel the export by sending the session a cancelExport message.</p>
<p>The export will fail if you try to overwrite an existing file, or write a file outside of the application’s sandbox. It may also fail if:</p>
<ul>
<li>There is an incoming phone call</li>
<li>Your application is in the background and another application starts playback</li>
</ul>
<p>In these situations, you should typically inform the user that the export failed, then allow the user to restart the export.</p>
<p>你可以通过给会话发送一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instm/AVAssetExportSession/cancelExport" target="_blank" rel="external">cancelExport</a> 消息来取消导出。</p>
<p>如果你尝试覆盖一个现有的文件或者在应用程序的沙盒外部写一个文件，都将会是导出失败。如果发生下面情况也可能失败：</p>
<ul>
<li>有一个来电</li>
<li>你的应用程序在后台并且另一个程序开始播放</li>
</ul>
<p>在这种情况下，你通常应该通知用户导出失败，然后允许用户重新启动导出。</p>
<h1 id="Playback-播放-1"><a href="#Playback-播放-1" class="headerlink" title="Playback - 播放"></a>Playback - 播放</h1><p>To control the playback of assets, you use an AVPlayer object. During playback, you can use an AVPlayerItem instance to manage the presentation state of an asset as a whole, and an AVPlayerItemTrack object to manage the presentation state of an individual track. To display video, you use an AVPlayerLayer object.</p>
<p>使用 <code>AVPlayer</code> 对象控制资产的播放。在播放期间，可以使用一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/cl/AVPlayerItem" target="_blank" rel="external">AVPlayerItem</a> 实例去管理资产作为一个整体的显示状态，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItemTrack_Class/index.html#//apple_ref/occ/cl/AVPlayerItemTrack" target="_blank" rel="external">AVPlayerItemTrack</a>  对象来管理一个单独轨道的显示状态。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/cl/AVPlayerLayer" target="_blank" rel="external">AVPlayerLayer</a> 显示视频。</p>
<h2 id="Playing-Assets-播放资产"><a href="#Playing-Assets-播放资产" class="headerlink" title="Playing Assets - 播放资产"></a>Playing Assets - 播放资产</h2><p>A player is a controller object that you use to manage playback of an asset, for example starting and stopping playback, and seeking to a particular time. You use an instance of AVPlayer to play a single asset. You can use an AVQueuePlayer object to play a number of items in sequence (AVQueuePlayer is a subclass of AVPlayer). On OS X you have the option of the using the AVKit framework’s AVPlayerView class to play the content back within a view.</p>
<p>播放器是一个控制器对象，使用这个控制器对象去管理一个资产的播放，例如开始和停止播放，并且追踪一个特定的时间。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/cl/AVPlayer" target="_blank" rel="external">AVPlayer</a> 的实例去播放单个资产。可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/cl/AVQueuePlayer" target="_blank" rel="external">AVQueuePlayer</a> 对象去播放在一些在队列的项目（<code>AVQueuePlayer</code> 是 <code>AVPlayer</code> 的子类）。在 <code>OS X</code> 系统中，可以选择使用 <code>AVKit</code> 框架的 <code>AVPlayerView</code> 类去播放一个视图的内容。</p>
<p>A player provides you with information about the state of the playback so, if you need to, you can synchronize your user interface with the player’s state. You typically direct the output of a player to a specialized Core Animation layer (an instance of AVPlayerLayer or AVSynchronizedLayer). To learn more about layers, see Core Animation Programming Guide.</p>
<p>播放器提供了关于播放状态的信息，因此如果需要，可以将用户界面与播放器的状态同步。通常将播放器的输出指向专门的动画核心层（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/cl/AVPlayerLayer" target="_blank" rel="external">AVPlayerLayer</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVSynchronizedLayer_Class/index.html#//apple_ref/occ/cl/AVSynchronizedLayer" target="_blank" rel="external">AVSynchronizedLayer</a> 的一个实例）。想要了解更多关于 <code>layers</code>，请看 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514" target="_blank" rel="external">Core Animation Programming Guide</a>。</p>
<blockquote>
<p>Multiple player layers: You can create many AVPlayerLayer objects from a single AVPlayer instance, but only the most recently created such layer will display any video content onscreen.</p>
<p>多个播放器层：可以从一个单独的 <code>AVPlayer</code> 实例创建许多 <code>AVPlayerLayer</code> 对象，但是只有最近被创建的那一层将会屏幕上显示视频的内容。</p>
</blockquote>
<p>Although ultimately you want to play an asset, you don’t provide assets directly to an AVPlayer object. Instead, you provide an instance of AVPlayerItem. A player item manages the presentation state of an asset with which it is associated. A player item contains player item tracks—instances of AVPlayerItemTrack—that correspond to the tracks in the asset. The relationship between the various objects is shown in Figure 2-1.</p>
<p>虽然最终想要播放一个资产，但又没有直接给提供资产一个 <code>AVPlayer</code> 对象。相反，提供一个 <code>AVPlayerItem</code> 的实例。一个 <code>player item</code> 管理与它相关的资产的显示状态。一个<code>player item</code>包含了播放器项目轨道 –  <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItemTrack_Class/index.html#//apple_ref/occ/cl/AVPlayerItemTrack" target="_blank" rel="external">AVPlayerItemTrack—that</a> 的实例，对应资产内的轨道。各个对象之间的关系如图2-1所示。</p>
<center><br><img src="http://ww2.sinaimg.cn/large/a9c4d5f6gw1f6f94r8pzvj20u00e0wep.jpg" alt="Figure 2-1  Playing an asset"><br></center>

<p>This abstraction means that you can play a given asset using different players simultaneously, but rendered in different ways by each player. Figure 2-2 shows one possibility, with two different players playing the same asset, with different settings. Using the item tracks, you can, for example, disable a particular track during playback (for example, you might not want to play the sound component).</p>
<p>这个摘要意味着可以同时使用不同的播放器播放一个给定的资产，但每个播放器都以不同的方式呈现。图2-2显示了一种可能性，同一个资产有两个不同的播放器，并且有不同的设定。可以使用不同的项目轨道，在播放期间禁用一个特定的轨道（例如，你可能不想播放这个声音组件）。</p>
<center><br><img src="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6f9awlylkj20sh0klgmb.jpg" alt="Figure 2-2  Playing the same asset in different ways"><br></center>

<p>You can initialize a player item with an existing asset, or you can initialize a player item directly from a URL so that you can play a resource at a particular location (AVPlayerItem will then create and configure an asset for the resource). As with AVAsset, though, simply initializing a player item doesn’t necessarily mean it’s ready for immediate playback. You can observe (using key-value observing) an item’s status property to determine if and when it’s ready to play.</p>
<p>可以用现有的资产初始化一个播放器项目，或者可以直接从一个 <code>URL</code> 初始化播放器项目，为了可以在一个特定位置播放一个资源（<code>AVPlayerItem</code> 将为资源创建和配置资产）。即使带着 <code>AVAsset</code> 简单地初始化一个播放器项目并不一定意味着它已经准备可以立即播放了。可以观察（使用 <a href="">key-value observing</a>]）一个项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instm/AVPlayerItem/status" target="_blank" rel="external">status</a> 属性，以确定是否可以播放并且当已经准备好去播放。</p>
<h2 id="Handling-Different-Types-of-Asset-处理不同类型的资产"><a href="#Handling-Different-Types-of-Asset-处理不同类型的资产" class="headerlink" title="Handling Different Types of Asset - 处理不同类型的资产"></a>Handling Different Types of Asset - 处理不同类型的资产</h2><p>The way you configure an asset for playback may depend on the sort of asset you want to play. Broadly speaking, there are two main types: file-based assets, to which you have random access (such as from a local file, the camera roll, or the Media Library), and stream-based assets (HTTP Live Streaming format).</p>
<p>配置一个准备播放的资产的方法可能取决于你想播放的资产的顺序。概括地说，主要由两种类型：基于文件的资产，可以随机访问（例如从一个本地文件，相机胶卷，或者媒体库），和基于流的资产（HTTP直播流媒体格式）。</p>
<p>To load and play a file-based asset. There are several steps to playing a file-based asset:</p>
<ul>
<li>Create an asset using AVURLAsset.</li>
<li>Create an instance of AVPlayerItem using the asset.</li>
<li>Associate the item with an instance of AVPlayer.</li>
<li>Wait until the item’s status property indicates that it’s ready to play (typically you use key-value observing to receive a notification when the status changes).</li>
</ul>
<p>This approach is illustrated in Putting It All Together: Playing a Video File Using AVPlayerLayer.</p>
<p>To create and prepare an HTTP live stream for playback. Initialize an instance of AVPlayerItem using the URL. (You cannot directly create an AVAsset instance to represent the media in an HTTP Live Stream.)</p>
<p>加载和播放一个基于文件的资产，播放基于文件的资产有几个步骤：</p>
<ul>
<li>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/occ/cl/AVURLAsset" target="_blank" rel="external">AVURLAsset</a> 创建一个资产</li>
<li>使用资产创建一个 <code>AVPlayerItem</code> 的实例</li>
<li>将 <code>AVPlayer</code> 的实例与项目联结</li>
<li>等待，直到项目的 <code>status</code> 属性表明已经准备好播放了（通常当状态改变时，使用 <a href="https://developer.apple.com/library/ios/documentation/General/Devpedia-CocoaApp-MOSX/KVO.html#//apple_ref/doc/uid/TP40009448-CH16" target="_blank" rel="external">key-value observing</a> 接受通知）</li>
</ul>
<p>该方法的说明都在：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW2" target="_blank" rel="external">Putting It All Together: Playing a Video File Using AVPlayerLayer</a> </p>
<p>创建和编写能够播放的HTTP直播流媒体。使用 <code>URL</code> 初始化一个 <code>AVPlayerItem</code> 的实例。（你不能直接创建一个 <code>AVAsset</code> 的实例去代表媒体在HTTP直播流中）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NSURL *url = [NSURL URLWithString:@"&lt;#Live stream URL#&gt;];</span><br><span class="line">// You may find a test stream at &lt;http://devimages.apple.com/iphone/samples/bipbop/bipbopall.m3u8&gt;.</span><br><span class="line">self.playerItem = [AVPlayerItem playerItemWithURL:url];</span><br><span class="line">[playerItem addObserver:self forKeyPath:@"status" options:0 context:&amp;ItemStatusContext];</span><br><span class="line">self.player = [AVPlayer playerWithPlayerItem:playerItem];</span><br></pre></td></tr></table></figure>
<p>When you associate the player item with a player, it starts to become ready to play. When it is ready to play, the player item creates the AVAsset and AVAssetTrack instances, which you can use to inspect the contents of the live stream.</p>
<p>To get the duration of a streaming item, you can observe the duration property on the player item. When the item becomes ready to play, this property updates to the correct value for the stream.</p>
<p>当你把播放项目和播放器联结起来时，它开始准备播放。当它准备播放时，播放项目创建 <code>AVAsset</code> 和 <code>AVAssetTrack</code> 实例，可以用它来检查直播流的内容。</p>
<p>获取一个流项目的持续时间，可以观察播放项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/duration" target="_blank" rel="external">duration</a> 属性。当项目准备就绪时，这个属性更新为流的正确值。</p>
<blockquote>
<p>Note: Using the duration property on the player item requires iOS 4.3 or later. An approach that is compatible with all versions of iOS involves observing the status property of the player item. When the status becomes AVPlayerItemStatusReadyToPlay, the duration can be fetched with the following line of code:</p>
<p>注意：在播放项目里使用 <code>duration</code> 属性要求 <code>iOS4.3</code> ，或者更高的版本。一种方法是所有版本的iOS兼容包括播放项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instm/AVPlayerItem/status" target="_blank" rel="external">status</a> 属性。当 <code>status</code> 变成 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/c/econst/AVPlayerItemStatusReadyToPlay" target="_blank" rel="external">AVPlayerItemStatusReadyToPlay</a>，持续时间可以被下面的代码获取到：</p>
</blockquote>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[[[[playerItem tracks] objectAtIndex:<span class="number">0</span>] assetTrack] asset] duration];</span><br></pre></td></tr></table></figure>
<p>If you simply want to play a live stream, you can take a shortcut and create a player directly using the URL use the following code:</p>
<p>如果你只是想播放一个直播流，你可以采取一种快捷方式，并使用 <code>URL</code> 直接创建一个播放器，代码如下：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">self</span>.player = [<span class="built_in">AVPlayer</span> playerWithURL:&lt;<span class="meta">#Live stream URL#&gt;];</span></span><br><span class="line">[player addObserver:<span class="keyword">self</span> forKeyPath:<span class="string">@"status"</span> options:<span class="number">0</span> context:&amp;PlayerStatusContext];</span><br></pre></td></tr></table></figure>
<p>As with assets and items, initializing the player does not mean it’s ready for playback. You should observe the player’s status property, which changes to AVPlayerStatusReadyToPlay when it is ready to play. You can also observe the currentItem property to access the player item created for the stream.</p>
<p>作为资产和项目，初始化播放器并不意味着它已经准备就绪可以播放。你应该观察播放器的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instp/AVPlayer/status" target="_blank" rel="external">status</a> 属性，当准备就绪的时候改变 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/c/econst/AVPlayerStatusReadyToPlay" target="_blank" rel="external">AVPlayerStatusReadyToPlay</a> 。也可以观察 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/currentItem" target="_blank" rel="external">currentItem</a> 属性去访问被流所创建播放项目。</p>
<p>If you don’t know what kind of URL you have, follow these steps:</p>
<ul>
<li>Try to initialize an AVURLAsset using the URL, then load its tracks key.<br>If the tracks load successfully, then you create a player item for the asset.</li>
<li>If 1 fails, create an AVPlayerItem directly from the URL.<br>Observe the player’s status property to determine whether it becomes playable.</li>
</ul>
<p>If either route succeeds, you end up with a player item that you can then associate with a player.</p>
<p>如果你不知道现有的 <code>URL</code> 是什么类型的，按照下面步骤：</p>
<ul>
<li>尝试用 <code>URL</code> 初始化一个 <code>AVURLAsset</code> ，然后将其加载为轨道的 <code>key</code>。</li>
<li>如果上一步失败，直接从 <code>URL</code> 创建一个 <code>AVPlayerItem</code> 。观察这个播放器的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instp/AVPlayer/status" target="_blank" rel="external">status</a> 属性来决定它是否是可播放的。</li>
</ul>
<p>如果两个都可以成功，你最终用可以联结给一个播放器的播放项目。</p>
<h2 id="Playing-an-Item-播放一个项目"><a href="#Playing-an-Item-播放一个项目" class="headerlink" title="Playing an Item - 播放一个项目"></a>Playing an Item - 播放一个项目</h2><p>To start playback, you send a play message to the player.</p>
<p>发送一个播放消息给播放器，开始播放：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">IBAction</span>)play:sender &#123;</span><br><span class="line">    [player play];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In addition to simply playing, you can manage various aspects of the playback, such as the rate and the location of the playhead. You can also monitor the play state of the player; this is useful if you want to, for example, synchronize the user interface to the presentation state of the asset—see Monitoring Playback.</p>
<p>除了简单的播放，可以管理播放的各个方面，如速度和播放头的位置。也可以监视播放器的播放状态；这是很有用的，例如如果你想将用户界面同步到资产的呈现状态 – 详情看：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW8" target="_blank" rel="external">Monitoring Playback</a>.</p>
<h3 id="Changing-the-Playback-Rate-改变播放的速率"><a href="#Changing-the-Playback-Rate-改变播放的速率" class="headerlink" title="Changing the Playback Rate - 改变播放的速率"></a>Changing the Playback Rate - 改变播放的速率</h3><p>You change the rate of playback by setting the player’s rate property.</p>
<p>通过发送播放器的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instp/AVPlayer/rate" target="_blank" rel="external">rate</a> 属性来改变播放速率。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">aPlayer.rate = <span class="number">0.5</span>;</span><br><span class="line">aPlayer.rate = <span class="number">2.0</span>;</span><br></pre></td></tr></table></figure>
<p>A value of 1.0 means “play at the natural rate of the current item”. Setting the rate to 0.0 is the same as pausing playback—you can also use pause.</p>
<p>值如果是 <code>1.0</code> 意味着“当前项目按正常速率播放”。将速率设置为 <code>0.0</code> 就和暂停播放一样了 – 也可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/pause" target="_blank" rel="external">pause</a></p>
<p>Items that support reverse playback can use the rate property with a negative number to set the reverse playback rate. You determine the type of reverse play that is supported by using the playerItem properties canPlayReverse (supports a rate value of -1.0), canPlaySlowReverse (supports rates between 0.0 and 1.0) and canPlayFastReverse (supports rate values less than -1.0).</p>
<p>支持逆向播放的项目可以使用带有负数 <code>rate</code> 属性，负数可以设置反向播放速率。确定反向播放的类型，通过使用 <code>playerItem</code> 属性 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/canPlayReverse" target="_blank" rel="external">canPlayReverse</a> （支持一个速率值 <code>-1.0</code>），<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/canPlaySlowReverse" target="_blank" rel="external">canPlaySlowReverse</a> （速率支持<code>0.0</code> 到 <code>1.0</code>）和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/canPlayFastReverse" target="_blank" rel="external">canPlayFastReverse</a> （速率值可以小于 <code>-1.0</code>）。</p>
<h3 id="Seeking—Repositioning-the-Playhead-寻找-重新定位播放头"><a href="#Seeking—Repositioning-the-Playhead-寻找-重新定位播放头" class="headerlink" title="Seeking—Repositioning the Playhead - 寻找-重新定位播放头"></a>Seeking—Repositioning the Playhead - 寻找-重新定位播放头</h3><p>To move the playhead to a particular time, you generally use seekToTime: as follows:</p>
<p>通常使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/seekToTime:" target="_blank" rel="external">seekToTime:</a> 把播放头移动到一个指定的时间，示例：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CMTime fiveSecondsIn = CMTimeMake(<span class="number">5</span>, <span class="number">1</span>);</span><br><span class="line">[player seekToTime:fiveSecondsIn];</span><br></pre></td></tr></table></figure>
<p>The seekToTime: method, however, is tuned for performance rather than precision. If you need to move the playhead precisely, instead you use seekToTime:toleranceBefore:toleranceAfter: as in the following code fragment:</p>
<p>然而 <code>seekToTime:</code> 方法是为了性能的调试，而不是精度。如果你需要精确的移动播放头，你需要使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/seekToTime:toleranceBefore:toleranceAfter:" target="_blank" rel="external">seekToTime:toleranceBefore:toleranceAfter:</a> 代替，示例代码：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CMTime fiveSecondsIn = CMTimeMake(<span class="number">5</span>, <span class="number">1</span>);</span><br><span class="line">[player seekToTime:fiveSecondsIn toleranceBefore:kCMTimeZero toleranceAfter:kCMTimeZero];</span><br></pre></td></tr></table></figure>
<p>Using a tolerance of zero may require the framework to decode a large amount of data. You should use zero only if you are, for example, writing a sophisticated media editing application that requires precise control.</p>
<p>After playback, the player’s head is set to the end of the item and further invocations of play have no effect. To position the playhead back at the beginning of the item, you can register to receive an AVPlayerItemDidPlayToEndTimeNotification notification from the item. In the notification’s callback method, you invoke seekToTime: with the argument kCMTimeZero.</p>
<p>使用一个零的限制可能需要框架来解码大量的数据。例如应该只是用零编写一个复杂的需要精确控制的媒体编辑应用。</p>
<p>播放之后，播放器的头被设置在项目的结尾处，接着进行播放的调用没有任何影响。将播放头放置在项目的开始位置，可以注册从项目接收一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/c/data/AVPlayerItemDidPlayToEndTimeNotification" target="_blank" rel="external">AVPlayerItemDidPlayToEndTimeNotification</a> 消息。在消息的回调方法中，调用带着参数 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/c_ref/kCMTimeZero" target="_blank" rel="external">kCMTimeZero</a> 的 <code>seekToTime:</code> 方法。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register with the notification center after creating the player item.</span></span><br><span class="line">    [[<span class="built_in">NSNotificationCenter</span> defaultCenter]</span><br><span class="line">        addObserver:<span class="keyword">self</span></span><br><span class="line">        selector:<span class="keyword">@selector</span>(playerItemDidReachEnd:)</span><br><span class="line">        name:<span class="built_in">AVPlayerItemDidPlayToEndTimeNotification</span></span><br><span class="line">        object:&lt;<span class="meta">#The player item#&gt;];</span></span><br><span class="line"> </span><br><span class="line">- (<span class="keyword">void</span>)playerItemDidReachEnd:(<span class="built_in">NSNotification</span> *)notification &#123;</span><br><span class="line">    [player seekToTime:kCMTimeZero];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Playing-Multiple-Items-播放多个项目"><a href="#Playing-Multiple-Items-播放多个项目" class="headerlink" title="Playing Multiple Items - 播放多个项目"></a>Playing Multiple Items - 播放多个项目</h2><p>You can use an AVQueuePlayer object to play a number of items in sequence. The AVQueuePlayer class is a subclass of AVPlayer. You initialize a queue player with an array of player items.</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/cl/AVQueuePlayer" target="_blank" rel="external">AVQueuePlayer</a> 对象去播放队列中的一些项目。<code>AVQueuePlayer</code> 类是 <code>AVPlayer</code> 的子类。初始化一个带着播放项目数组的队列播放器：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSArray</span> *items = &lt;<span class="meta">#An array of player items#&gt;;</span></span><br><span class="line"><span class="built_in">AVQueuePlayer</span> *queuePlayer = [[<span class="built_in">AVQueuePlayer</span> alloc] initWithItems:items];</span><br></pre></td></tr></table></figure>
<p>You can then play the queue using play, just as you would an AVPlayer object. The queue player plays each item in turn. If you want to skip to the next item, you send the queue player an advanceToNextItem message.</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/play" target="_blank" rel="external">play</a> 播放队列，就像你是一个 <code>AVPlayer</code> 对象。队列播放器依次播放每个项目。如果想要跳过这一项，给队列播放器发送一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/instm/AVQueuePlayer/advanceToNextItem" target="_blank" rel="external">advanceToNextItem</a> 信息。</p>
<p>You can modify the queue using insertItem:afterItem:, removeItem:, and removeAllItems. When adding a new item, you should typically check whether it can be inserted into the queue, using canInsertItem:afterItem:. You pass nil as the second argument to test whether the new item can be appended to the queue.</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/instm/AVQueuePlayer/insertItem:afterItem:" target="_blank" rel="external">insertItem:afterItem:</a> ，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/instm/AVQueuePlayer/removeItem:" target="_blank" rel="external">removeItem:</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/instm/AVQueuePlayer/removeAllItems" target="_blank" rel="external">removeAllItems</a> 这三个方法修改队列。当添加一个新项目，通常应该检查它是否可以被插入到队列中，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVQueuePlayer_Class/index.html#//apple_ref/occ/instm/AVQueuePlayer/canInsertItem:afterItem:" target="_blank" rel="external">canInsertItem:afterItem:</a>。传 <code>nil</code> 作为第二个参数去测试是否将新项目添加到队列中。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVPlayerItem</span> *anItem = &lt;<span class="meta">#Get a player item#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> ([queuePlayer canInsertItem:anItem afterItem:<span class="literal">nil</span>]) &#123;</span><br><span class="line">    [queuePlayer insertItem:anItem afterItem:<span class="literal">nil</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Monitoring-Playback-监视播放"><a href="#Monitoring-Playback-监视播放" class="headerlink" title="Monitoring Playback - 监视播放"></a>Monitoring Playback - 监视播放</h2><p>You can monitor a number of aspects of both the presentation state of a player and the player item being played. This is particularly useful for state changes that are not under your direct control. For example:</p>
<ul>
<li>If the user uses multitasking to switch to a different application, a player’s rate property will drop to 0.0.</li>
<li>If you are playing remote media, a player item’s loadedTimeRanges and seekableTimeRanges properties will change as more data becomes available.</li>
</ul>
<p>These properties tell you what portions of the player item’s timeline are available.</p>
<ul>
<li>A player’s currentItem property changes as a player item is created for an HTTP live stream.</li>
<li>A player item’s tracks property may change while playing an HTTP live stream.</li>
</ul>
<p>This may happen if the stream offers different encodings for the content; the tracks change if the player switches to a different encoding.</p>
<ul>
<li>A player or player item’s status property may change if playback fails for some reason.</li>
</ul>
<p>You can use key-value observing to monitor changes to values of these properties.</p>
<p>可以监视播放器的演示状态和正在播放的播放项目的很多方面的情况。状态的改变并不是在你的直接控制下，监视是非常有用的。例如：</p>
<ul>
<li>如果用户使用多任务处理切换到另一个应用程序，播放器的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instp/AVPlayer/rate" target="_blank" rel="external">rate</a> 属性将下降到 <code>0.0</code>。</li>
<li>如果正在播放远程媒体，播放项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/loadedTimeRanges" target="_blank" rel="external">loadedTimeRanges</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instm/AVPlayerItem/seekableTimeRanges" target="_blank" rel="external">seekableTimeRanges</a>  属性将会改变使得更多的数据成为可用的。</li>
</ul>
<p>这些属性告诉你，播放项目时间轴的那一部分是可用的。</p>
<ul>
<li>播放器的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/currentItem" target="_blank" rel="external">currentItem</a> 属性变化，随着播放项目被 <code>HTTP</code> 直播流创建。</li>
<li>当播放 <code>HTTP</code> 直播流时，播放项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instm/AVPlayerItem/tracks" target="_blank" rel="external">tracks</a> 属性可能会改变。</li>
</ul>
<p>如果流的内容提供了不同的编码上述情况就可能发生；如果用户切换到不同的编码轨道就改变了。</p>
<ul>
<li>如果因为一些原因播放失败，播放器或者播放项目的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/status" target="_blank" rel="external">status</a> 属性可能会改变。</li>
</ul>
<p>可以使用 <code>key-value observing</code> 去监视这些属性值的改变。</p>
<blockquote>
<p>Important: You should register for KVO change notifications and unregister from KVO change notifications on the main thread. This avoids the possibility of receiving a partial notification if a change is being made on another thread. AV Foundation invokes observeValueForKeyPath:ofObject:change:context: on the main thread, even if the change operation is made on another thread.</p>
<p>重要的是：你应该对 <code>KVO</code> 改变通知登记，从主线程中 <code>KVO</code> 改变通知而注销。如果在另一个线程上正在更改，这避免了只接受到部分通知的可能性。<code>AV Foundation</code> 在主线程中调用 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/Foundation/Protocols/NSKeyValueObserving_Protocol/index.html#//apple_ref/occ/instm/NSObject/observeValueForKeyPath:ofObject:change:context:" target="_blank" rel="external">observeValueForKeyPath:ofObject:change:context:</a> ，即使改变操作是在另一个线程中。</p>
</blockquote>
<h3 id="Responding-to-a-Change-in-Status-响应状态的变化"><a href="#Responding-to-a-Change-in-Status-响应状态的变化" class="headerlink" title="Responding to a Change in Status - 响应状态的变化"></a>Responding to a Change in Status - 响应状态的变化</h3><p>When a player or player item’s status changes, it emits a key-value observing change notification. If an object is unable to play for some reason (for example, if the media services are reset), the status changes to AVPlayerStatusFailed or AVPlayerItemStatusFailed as appropriate. In this situation, the value of the object’s error property is changed to an error object that describes why the object is no longer be able to play.</p>
<p>当一个播放器或者播放项目的 <code>status</code> 改变，它会发出一个 <code>key-value observing</code> 改变通知。如果一个对象由于一些原因不能播放（例如，如果媒体服务器复位），<code>status</code> 适当的改变为 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/c/econst/AVPlayerStatusFailed" target="_blank" rel="external">AVPlayerStatusFailed</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/c/econst/AVPlayerItemStatusFailed" target="_blank" rel="external">AVPlayerItemStatusFailed</a>。在这种情况下，对象的 <code>error</code> 属性的值被更改为一个错误对象，该对象描述了为什么对象不能播放了。</p>
<p>AV Foundation does not specify what thread that the notification is sent on. If you want to update the user interface, you must make sure that any relevant code is invoked on the main thread. This example uses dispatch_async to execute code on the main thread.</p>
<p><code>AV Foundation</code> 没有指定通知发送的是什么线程。如果要更新用户界面，必须确保相关的代码都是在主线程被调用的。这个例子使用了 <a href="https://developer.apple.com/library/ios/documentation/Performance/Reference/GCD_libdispatch_Ref/index.html#//apple_ref/c/func/dispatch_async" target="_blank" rel="external">dispatch_async</a> 去执行在主线程中的代码。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)observeValueForKeyPath:(<span class="built_in">NSString</span> *)keyPath ofObject:(<span class="keyword">id</span>)object</span><br><span class="line">                        change:(<span class="built_in">NSDictionary</span> *)change context:(<span class="keyword">void</span> *)context &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (context == &lt;<span class="meta">#Player status context#&gt;) &#123;</span></span><br><span class="line">        <span class="built_in">AVPlayer</span> *thePlayer = (<span class="built_in">AVPlayer</span> *)object;</span><br><span class="line">        <span class="keyword">if</span> ([thePlayer status] == <span class="built_in">AVPlayerStatusFailed</span>) &#123;</span><br><span class="line">            <span class="built_in">NSError</span> *error = [&lt;<span class="meta">#The AVPlayer object#&gt; error];</span></span><br><span class="line">            <span class="comment">// Respond to error: for example, display an alert sheet.</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Deal with other status change if appropriate.</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Deal with other change notifications if appropriate.</span></span><br><span class="line">    [<span class="keyword">super</span> observeValueForKeyPath:keyPath ofObject:object</span><br><span class="line">           change:change context:context];</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Tracking-Readiness-for-Visual-Display-为视觉展示做追踪准备"><a href="#Tracking-Readiness-for-Visual-Display-为视觉展示做追踪准备" class="headerlink" title="Tracking Readiness for Visual Display - 为视觉展示做追踪准备"></a>Tracking Readiness for Visual Display - 为视觉展示做追踪准备</h3><p>You can observe an AVPlayerLayer object’s readyForDisplay property to be notified when the layer has user-visible content. In particular, you might insert the player layer into the layer tree only when there is something for the user to look at and then perform a transition from.</p>
<p>可以观察一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/cl/AVPlayerLayer" target="_blank" rel="external">AVPlayerLayer</a> 对象的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/instm/AVPlayerLayer/isReadyForDisplay" target="_blank" rel="external">readyForDisplay</a> 属性，当层有了用户可见的内容时属性可以被通知。特别是，可能将播放器层插入到层树，只有当有东西给用户看的时候，在从里面执行一个转变。</p>
<h3 id="Tracking-Time-追踪时间"><a href="#Tracking-Time-追踪时间" class="headerlink" title="Tracking Time - 追踪时间"></a>Tracking Time - 追踪时间</h3><p>To track changes in the position of the playhead in an AVPlayer object, you can use addPeriodicTimeObserverForInterval:queue:usingBlock: or addBoundaryTimeObserverForTimes:queue:usingBlock:. You might do this to, for example, update your user interface with information about time elapsed or time remaining, or perform some other user interface synchronization.</p>
<ul>
<li>With addPeriodicTimeObserverForInterval:queue:usingBlock:, the block you provide is invoked at the interval you specify, if time jumps, and when playback starts or stops.</li>
<li>With addBoundaryTimeObserverForTimes:queue:usingBlock:, you pass an array of CMTime structures contained in NSValue objects. The block you provide is invoked whenever any of those times is traversed.</li>
</ul>
<p>追踪一个 <code>AVPlayer</code> 对象中播放头位置的变化，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/addPeriodicTimeObserverForInterval:queue:usingBlock:" target="_blank" rel="external">addPeriodicTimeObserverForInterval:queue:usingBlock:</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/addBoundaryTimeObserverForTimes:queue:usingBlock:" target="_blank" rel="external">addBoundaryTimeObserverForTimes:queue:usingBlock:</a> 。可以这样做，例如更新用户界面与时间消耗或者剩余时间的有关信息，或者执行一些其他用户界面的同步。</p>
<ul>
<li>有关 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/addPeriodicTimeObserverForInterval:queue:usingBlock:" target="_blank" rel="external">addBoundaryTimeObserverForTimes:queue:usingBlock:</a> ，你提供的块在你指定的时间间隔内被调用，如果时间有跳跃，那就在播放开始或者结束的时候。</li>
<li>有关 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/addBoundaryTimeObserverForTimes:queue:usingBlock:" target="_blank" rel="external">addBoundaryTimeObserverForTimes:queue:usingBlock:</a>，传递一个 <code>CMTime</code> 结构体的数组，包含在 <code>NSValue</code> 对象中。任何这些时间被遍历的时候你提供的块都会被调用。</li>
</ul>
<p>Both of the methods return an opaque object that serves as an observer. You must keep a strong reference to the returned object as long as you want the time observation block to be invoked by the player. You must also balance each invocation of these methods with a corresponding call to removeTimeObserver:.</p>
<p>With both of these methods, AV Foundation does not guarantee to invoke your block for every interval or boundary passed. AV Foundation does not invoke a block if execution of a previously invoked block has not completed. You must make sure, therefore, that the work you perform in the block does not overly tax the system.</p>
<p>这两种方法都返回一个作为观察者的不透明对象。只要你希望播放器调用时间观察的块，就必须对返回的对象保持一个强引用。你也必须平衡每次调用这些方法，与相应的调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/instm/AVPlayer/removeTimeObserver:" target="_blank" rel="external">removeTimeObserver:</a>.</p>
<p>有了这两种方法， <code>AV Foundation</code> 不保证每个间隔或者通过边界时都调用你的块。如果以前调用的块执行没有完成，<code>AV Foundation</code>不会调用块。因此必须确保你在该块中执行的工作不会对系统过载。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assume a property: @property (strong) id playerObserver;</span></span><br><span class="line"> </span><br><span class="line">Float64 durationSeconds = CMTimeGetSeconds([&lt;<span class="meta">#An asset#&gt; duration]);</span></span><br><span class="line">CMTime firstThird = CMTimeMakeWithSeconds(durationSeconds/<span class="number">3.0</span>, <span class="number">1</span>);</span><br><span class="line">CMTime secondThird = CMTimeMakeWithSeconds(durationSeconds*<span class="number">2.0</span>/<span class="number">3.0</span>, <span class="number">1</span>);</span><br><span class="line"><span class="built_in">NSArray</span> *times = @[[<span class="built_in">NSValue</span> valueWithCMTime:firstThird], [<span class="built_in">NSValue</span> valueWithCMTime:secondThird]];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">self</span>.playerObserver = [&lt;<span class="meta">#A player#&gt; addBoundaryTimeObserverForTimes:times queue:NULL usingBlock:^&#123;</span></span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSString</span> *timeDescription = (<span class="built_in">NSString</span> *)</span><br><span class="line">        <span class="built_in">CFBridgingRelease</span>(CMTimeCopyDescription(<span class="literal">NULL</span>, [<span class="keyword">self</span>.player currentTime]));</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"Passed a boundary at %@"</span>, timeDescription);</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<h3 id="Reaching-the-End-of-an-Item-到达一个项目的结束"><a href="#Reaching-the-End-of-an-Item-到达一个项目的结束" class="headerlink" title="Reaching the End of an Item - 到达一个项目的结束"></a>Reaching the End of an Item - 到达一个项目的结束</h3><p>You can register to receive an AVPlayerItemDidPlayToEndTimeNotification notification when a player item has completed playback.</p>
<p>当一个播放项目已经完成播放的时候，可以注册接收一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/c/data/AVPlayerItemDidPlayToEndTimeNotification" target="_blank" rel="external">AVPlayerItemDidPlayToEndTimeNotification</a> 通知。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="built_in">NSNotificationCenter</span> defaultCenter] addObserver:&lt;<span class="meta">#The observer, typically self#&gt;</span></span><br><span class="line">                                         selector:<span class="keyword">@selector</span>(&lt;<span class="meta">#The selector name#&gt;)</span></span><br><span class="line">                                             name:<span class="built_in">AVPlayerItemDidPlayToEndTimeNotification</span></span><br><span class="line">                                           object:&lt;<span class="meta">#A player item#&gt;];</span></span><br></pre></td></tr></table></figure>
<h2 id="Putting-It-All-Together-Playing-a-Video-File-Using-AVPlayerLayer-总而言之，使用-AVPlayerLayer-播放视频文件"><a href="#Putting-It-All-Together-Playing-a-Video-File-Using-AVPlayerLayer-总而言之，使用-AVPlayerLayer-播放视频文件" class="headerlink" title="Putting It All Together: Playing a Video File Using AVPlayerLayer - 总而言之，使用 AVPlayerLayer 播放视频文件"></a>Putting It All Together: Playing a Video File Using AVPlayerLayer - 总而言之，使用 <code>AVPlayerLayer</code> 播放视频文件</h2><p>This brief code example illustrates how you can use an AVPlayer object to play a video file. It shows how to:</p>
<ul>
<li>Configure a view to use an AVPlayerLayer layer</li>
<li>Create an AVPlayer object</li>
<li>Create an AVPlayerItem object for a file-based asset and use key-value observing to observe its status</li>
<li>Respond to the item becoming ready to play by enabling a button</li>
<li>Play the item and then restore the player’s head to the beginning</li>
</ul>
<p>这个简短的代码示例演示如何使用一个 <code>AVPlayer</code> 对象播放一个视频文件。它显示了如何：</p>
<ul>
<li>使用 <code>AVPlayerLayer</code> 层配置视图</li>
<li>创建一个 <code>AVPlayer</code> 对象</li>
<li>创建一个基于文件资产的 <code>AVPlayerItem</code> 对象和使用 <code>key-value observing</code> 去观察它的状态</li>
<li>通过启用按钮来响应项目准备就绪播放</li>
<li>播放项目，然后将播放器的头重置到开始位置</li>
</ul>
<blockquote>
<p>Note: To focus on the most relevant code, this example omits several aspects of a complete application, such as memory management and unregistering as an observer (for key-value observing or for the notification center). To use AV Foundation, you are expected to have enough experience with Cocoa to be able to infer the missing pieces.</p>
<p>注意：关注最相关的代码，这个例子中省略了一个完整应用程序的几个方面，比如内存管理和注销观察者（<code>key-value observing</code> 或者 <code>notification center</code>）。为了使用 <code>AV Foundation</code> ，你应该有足够的 <code>Cocoa</code> 经验，有能力去推断出丢失的碎片。</p>
</blockquote>
<p>For a conceptual introduction to playback, skip to Playing Assets.</p>
<p>对于播放的概念性的介绍，跳去看 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW4" target="_blank" rel="external">Playing Assets</a>。</p>
<h3 id="The-Player-View-播放器视图"><a href="#The-Player-View-播放器视图" class="headerlink" title="The Player View - 播放器视图"></a>The Player View - 播放器视图</h3><p>To play the visual component of an asset, you need a view containing an AVPlayerLayer layer to which the output of an AVPlayer object can be directed. You can create a simple subclass of UIView to accommodate this:</p>
<p>播放一个资产的可视化部分，需要一个包含了 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/cl/AVPlayerLayer" target="_blank" rel="external">AVPlayerLayer</a> 层的视图，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerLayer_Class/index.html#//apple_ref/occ/cl/AVPlayerLayer" target="_blank" rel="external">AVPlayerLayer</a> 层可以直接输出 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/index.html#//apple_ref/occ/cl/AVPlayer" target="_blank" rel="external">AVPlayer</a> 对象。可以创建一个 <a href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIView_Class/index.html#//apple_ref/occ/cl/UIView" target="_blank" rel="external">UIView</a> 的简单子类来容纳：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">&lt;UIKit/UIKit.h&gt;</span></span></span><br><span class="line"><span class="meta">#import <span class="meta-string">&lt;AVFoundation/AVFoundation.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">@interface</span> <span class="title">PlayerView</span> : <span class="title">UIView</span></span></span><br><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>) <span class="built_in">AVPlayer</span> *player;</span><br><span class="line"><span class="keyword">@end</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">@implementation</span> <span class="title">PlayerView</span></span></span><br><span class="line">+ (Class)layerClass &#123;</span><br><span class="line">    <span class="keyword">return</span> [<span class="built_in">AVPlayerLayer</span> class];</span><br><span class="line">&#125;</span><br><span class="line">- (<span class="built_in">AVPlayer</span>*)player &#123;</span><br><span class="line">    <span class="keyword">return</span> [(<span class="built_in">AVPlayerLayer</span> *)[<span class="keyword">self</span> layer] player];</span><br><span class="line">&#125;</span><br><span class="line">- (<span class="keyword">void</span>)setPlayer:(<span class="built_in">AVPlayer</span> *)player &#123;</span><br><span class="line">    [(<span class="built_in">AVPlayerLayer</span> *)[<span class="keyword">self</span> layer] setPlayer:player];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">@end</span></span><br></pre></td></tr></table></figure>
<h3 id="A-Simple-View-Controller-一个简单的-View-Controller"><a href="#A-Simple-View-Controller-一个简单的-View-Controller" class="headerlink" title="A Simple View Controller - 一个简单的 View Controller"></a>A Simple View Controller - 一个简单的 <code>View Controller</code></h3><p>Assume you have a simple view controller, declared as follows:</p>
<p>假设你有一个简单的 <code>view controller</code>，声明如下：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">@class</span> <span class="title">PlayerView</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">@interface</span> <span class="title">PlayerViewController</span> : <span class="title">UIViewController</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>) <span class="built_in">AVPlayer</span> *player;</span><br><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>) <span class="built_in">AVPlayerItem</span> *playerItem;</span><br><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">weak</span>) <span class="keyword">IBOutlet</span> PlayerView *playerView;</span><br><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">weak</span>) <span class="keyword">IBOutlet</span> <span class="built_in">UIButton</span> *playButton;</span><br><span class="line">- (<span class="keyword">IBAction</span>)loadAssetFromFile:sender;</span><br><span class="line">- (<span class="keyword">IBAction</span>)play:sender;</span><br><span class="line">- (<span class="keyword">void</span>)syncUI;</span><br><span class="line"><span class="keyword">@end</span></span><br></pre></td></tr></table></figure>
<p>The syncUI method synchronizes the button’s state with the player’s state:</p>
<p><code>syncUI</code> 方法同步按钮状态和播放器的状态：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)syncUI &#123;</span><br><span class="line">    <span class="keyword">if</span> ((<span class="keyword">self</span>.player.currentItem != <span class="literal">nil</span>) &amp;&amp;</span><br><span class="line">        ([<span class="keyword">self</span>.player.currentItem status] == <span class="built_in">AVPlayerItemStatusReadyToPlay</span>)) &#123;</span><br><span class="line">        <span class="keyword">self</span>.playButton.enabled = <span class="literal">YES</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">self</span>.playButton.enabled = <span class="literal">NO</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You can invoke syncUI in the view controller’s viewDidLoad method to ensure a consistent user interface when the view is first displayed.</p>
<p>当视图第一次显示的时候，可以在视图控制器的 <a href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIViewController_Class/index.html#//apple_ref/occ/instm/UIViewController/viewDidLoad" target="_blank" rel="external">viewDidLoad</a> 方法中调用 <code>invoke</code> 去确保用户界面的一致性。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)viewDidLoad &#123;</span><br><span class="line">    [<span class="keyword">super</span> viewDidLoad];</span><br><span class="line">    [<span class="keyword">self</span> syncUI];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The other properties and methods are described in the remaining sections.</p>
<p>在其余章节描述其他属性和方法。</p>
<h3 id="Creating-the-Asset-创建一个资产"><a href="#Creating-the-Asset-创建一个资产" class="headerlink" title="Creating the Asset - 创建一个资产"></a>Creating the Asset - 创建一个资产</h3><p>You create an asset from a URL using AVURLAsset. (The following example assumes your project contains a suitable video resource.)</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/occ/cl/AVURLAsset" target="_blank" rel="external">AVURLAsset</a> 从一个 <code>URL</code> 创建一个资产。（下面的例子假设你的工程包含了一个合适的视频资源）</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">IBAction</span>)loadAssetFromFile:sender &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSURL</span> *fileURL = [[<span class="built_in">NSBundle</span> mainBundle]</span><br><span class="line">        URLForResource:&lt;<span class="meta">#@<span class="meta-string">"VideoFileName"</span>#&gt; withExtension:<span class="meta-string">&lt;#@"extension"#&gt;</span>];</span></span><br><span class="line"> </span><br><span class="line">    <span class="built_in">AVURLAsset</span> *asset = [<span class="built_in">AVURLAsset</span> URLAssetWithURL:fileURL options:<span class="literal">nil</span>];</span><br><span class="line">    <span class="built_in">NSString</span> *tracksKey = <span class="string">@"tracks"</span>;</span><br><span class="line"> </span><br><span class="line">    [asset loadValuesAsynchronouslyForKeys:@[tracksKey] completionHandler:</span><br><span class="line">     ^&#123;</span><br><span class="line">         <span class="comment">// The completion block goes here.</span></span><br><span class="line">     &#125;];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In the completion block, you create an instance of AVPlayerItem for the asset and set it as the player for the player view. As with creating the asset, simply creating the player item does not mean it’s ready to use. To determine when it’s ready to play, you can observe the item’s status property. You should configure this observing before associating the player item instance with the player itself.</p>
<p>You trigger the player item’s preparation to play when you associate it with the player.</p>
<p>在完成块中，为资产创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/cl/AVPlayerItem" target="_blank" rel="external">AVPlayerItem</a> 的实例，并设置它为播放页面的播放器。与创建资产一样，简单地创建播放器项目并不意味着它已经准备好使用。为了确定它已经准备好了，可以观察项目的 <code>status</code> 属性。你应该在该播放器项目实例与播放器本身关联之前，配置这个 <code>observing</code> 。</p>
<p>当你将它与播放器连接时，就是触发播放项目的播放准备。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Define this constant for the key-value observation context.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="built_in">NSString</span> *ItemStatusContext;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Completion handler block.</span></span><br><span class="line">         <span class="built_in">dispatch_async</span>(dispatch_get_main_queue(),</span><br><span class="line">            ^&#123;</span><br><span class="line">                <span class="built_in">NSError</span> *error;</span><br><span class="line">                <span class="built_in">AVKeyValueStatus</span> status = [asset statusOfValueForKey:tracksKey error:&amp;error];</span><br><span class="line"> </span><br><span class="line">                <span class="keyword">if</span> (status == <span class="built_in">AVKeyValueStatusLoaded</span>) &#123;</span><br><span class="line">                    <span class="keyword">self</span>.playerItem = [<span class="built_in">AVPlayerItem</span> playerItemWithAsset:asset];</span><br><span class="line">                     <span class="comment">// ensure that this is done before the playerItem is associated with the player</span></span><br><span class="line">                    [<span class="keyword">self</span>.playerItem addObserver:<span class="keyword">self</span> forKeyPath:<span class="string">@"status"</span></span><br><span class="line">                                options:<span class="built_in">NSKeyValueObservingOptionInitial</span> context:&amp;ItemStatusContext];</span><br><span class="line">                    [[<span class="built_in">NSNotificationCenter</span> defaultCenter] addObserver:<span class="keyword">self</span></span><br><span class="line">                                                              selector:<span class="keyword">@selector</span>(playerItemDidReachEnd:)</span><br><span class="line">                                                                  name:<span class="built_in">AVPlayerItemDidPlayToEndTimeNotification</span></span><br><span class="line">                                                                object:<span class="keyword">self</span>.playerItem];</span><br><span class="line">                    <span class="keyword">self</span>.player = [<span class="built_in">AVPlayer</span> playerWithPlayerItem:<span class="keyword">self</span>.playerItem];</span><br><span class="line">                    [<span class="keyword">self</span>.playerView setPlayer:<span class="keyword">self</span>.player];</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// You should deal with the error appropriately.</span></span><br><span class="line">                    <span class="built_in">NSLog</span>(<span class="string">@"The asset's tracks were not loaded:\n%@"</span>, [error localizedDescription]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br></pre></td></tr></table></figure>
<h3 id="Responding-to-the-Player-Item’s-Status-Change-相应播放项目的状态改变"><a href="#Responding-to-the-Player-Item’s-Status-Change-相应播放项目的状态改变" class="headerlink" title="Responding to the Player Item’s Status Change - 相应播放项目的状态改变"></a>Responding to the Player Item’s Status Change - 相应播放项目的状态改变</h3><p>When the player item’s status changes, the view controller receives a key-value observing change notification. AV Foundation does not specify what thread that the notification is sent on. If you want to update the user interface, you must make sure that any relevant code is invoked on the main thread. This example uses dispatch_async to queue a message on the main thread to synchronize the user interface.</p>
<p>当播放项目的状态改变时，视图控制器接收一个 <code>key-value observing</code> 改变通知。<code>AV Foundation</code> 没有指定通知发送的是什么线程。如果你想更新用户界面，必须确保任何相关的代码都要在主线程中调用。这个例子使用 <a href="https://developer.apple.com/library/ios/documentation/Performance/Reference/GCD_libdispatch_Ref/index.html#//apple_ref/c/func/dispatch_async" target="_blank" rel="external">dispatch_async</a> 让主线程同步用户界面的消息进入队列。</p>
<h3 id="Playing-the-Item-播放项目"><a href="#Playing-the-Item-播放项目" class="headerlink" title="Playing the Item - 播放项目"></a>Playing the Item - 播放项目</h3><p>Playing the item involves sending a play message to the player.</p>
<p>播放项目涉及到想播放器发送一个播放消息。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">IBAction</span>)play:sender &#123;</span><br><span class="line">    [player play];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The item is played only once. After playback, the player’s head is set to the end of the item, and further invocations of the play method will have no effect. To position the playhead back at the beginning of the item, you can register to receive an AVPlayerItemDidPlayToEndTimeNotification from the item. In the notification’s callback method, invoke seekToTime: with the argument kCMTimeZero.</p>
<p>该项目只播放一次。播放之后，播放器的头被设置在项目的结束位置，播放方法进一步调用将没有效果。将播放头放在项目的开始，可以注册从项目去接收 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/c/data/AVPlayerItemDidPlayToEndTimeNotification" target="_blank" rel="external">AVPlayerItemDidPlayToEndTimeNotification</a>。在通知的回调方法，调用带着参数 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/data/kCMTimeZero" target="_blank" rel="external">kCMTimeZero</a> 的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instm/AVPlayerItem/seekToTime:" target="_blank" rel="external">seekToTime:</a> 方法。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register with the notification center after creating the player item.</span></span><br><span class="line">    [[<span class="built_in">NSNotificationCenter</span> defaultCenter]</span><br><span class="line">        addObserver:<span class="keyword">self</span></span><br><span class="line">        selector:<span class="keyword">@selector</span>(playerItemDidReachEnd:)</span><br><span class="line">        name:<span class="built_in">AVPlayerItemDidPlayToEndTimeNotification</span></span><br><span class="line">        object:[<span class="keyword">self</span>.player currentItem]];</span><br><span class="line"> </span><br><span class="line">- (<span class="keyword">void</span>)playerItemDidReachEnd:(<span class="built_in">NSNotification</span> *)notification &#123;</span><br><span class="line">    [<span class="keyword">self</span>.player seekToTime:kCMTimeZero];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Editing-编辑-1"><a href="#Editing-编辑-1" class="headerlink" title="Editing - 编辑"></a>Editing - 编辑</h1><p>The AVFoundation framework provides a feature-rich set of classes to facilitate the editing of audio visual assets. At the heart of AVFoundation’s editing API are compositions. A composition is simply a collection of tracks from one or more different media assets. The AVMutableComposition class provides an interface for inserting and removing tracks, as well as managing their temporal orderings. Figure 3-1 shows how a new composition is pieced together from a combination of existing assets to form a new asset. If all you want to do is merge multiple assets together sequentially into a single file, that is as much detail as you need. If you want to perform any custom audio or video processing on the tracks in your composition, you need to incorporate an audio mix or a video composition, respectively.</p>
<p><code>AVFoundation</code> 框架提供了一个功能丰富的类集合去帮助音视频资产的编辑。 <code>AVFoundation</code><br>编辑 <code>API</code> 的核心是一些组合。一种组合物是简单的一个或者多个不同媒体资产的轨道的集合。<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a> 类提供一个可以插入和移除轨道的接口，以及管理它们的时间序列。图3-1显示了一个新的组合是怎样从一些现有的资产拼凑起来，形成新的资产。如果你想做的是将多个资产合并为一个单一的文件，这里有尽可能多的你需要掌握的细节。如果你想在你的作品中的轨道上执行任何自定义音频或视频处理，你需要分别将一个音频组合或者视频组成。</p>
<center><br><img src="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6fu4l7pu1j20wu0goaal.jpg" alt="Figure 3-1  AVMutableComposition assembles assets together"><br></center>

<p>Using the AVMutableAudioMix class, you can perform custom audio processing on the audio tracks in your composition, as shown in Figure 3-2. Currently, you can specify a maximum volume or set a volume ramp for an audio track.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableAudioMix_Class/index.html#//apple_ref/occ/cl/AVMutableAudioMix" target="_blank" rel="external">AVMutableAudioMix</a> 类，可以在你作品的音频轨道中执行自定义处理，如图3-2所示。目前，你可以指定一个最大音量或设置一个音频轨道的音量斜坡</p>
<center><br><img src="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6fu9qkxtjj20mo0giq3g.jpg" alt="Figure 3-2  AVMutableAudioMix performs audio mixing"><br></center>

<p>You can use the AVMutableVideoComposition class to work directly with the video tracks in your composition for the purposes of editing, shown in Figure 3-3. With a single video composition, you can specify the desired render size and scale, as well as the frame duration, for the output video. Through a video composition’s instructions (represented by the AVMutableVideoCompositionInstruction class), you can modify the background color of your video and apply layer instructions. These layer instructions (represented by the AVMutableVideoCompositionLayerInstruction class) can be used to apply transforms, transform ramps, opacity and opacity ramps to the video tracks within your composition. The video composition class also gives you the ability to introduce effects from the Core Animation framework into your video using the animationTool property.</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/cl/AVMutableVideoComposition" target="_blank" rel="external">AVMutableVideoComposition</a> 类直接在视频中跟踪你想编辑的部分，如图3-3所示。一个单一的视频组件，可以为输出视频指定所需的渲染大小和规模，以及帧的持续时间。通过视频组件的指令（以 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionInstruction_Class/index.html#//apple_ref/occ/cl/AVMutableVideoCompositionInstruction" target="_blank" rel="external">AVMutableVideoCompositionInstruction</a> 类为代表），你可以修改视频的背景颜色和应用层的指令。这些层的指令（以 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionLayerInstruction_Class/index.html#//apple_ref/occ/cl/AVMutableVideoCompositionLayerInstruction" target="_blank" rel="external">AVMutableVideoCompositionLayerInstruction</a> 类为代表）可以可应用于应用变换，变换坡道，不透明度以及不透明度的坡道到你的组件中的视频轨道。视频组件类也能让你做一些事，从核心动画框架到使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instp/AVMutableVideoComposition/animationTool" target="_blank" rel="external">animationTool</a> 属性的视频。</p>
<center><br><img src="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6gafmhl5zj214v0lf0ti.jpg" alt="Figure 3-3  AVMutableVideoComposition"><br></center>

<p>To combine your composition with an audio mix and a video composition, you use an AVAssetExportSession object, as shown in Figure 3-4. You initialize the export session with your composition and then simply assign your audio mix and video composition to the audioMix and videoComposition properties respectively.</p>
<p>将音频和视频的成分组合，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/cl/AVAssetExportSession" target="_blank" rel="external">AVAssetExportSession</a> 对象，如图3-4所所示。初始化导出会话，然后简单的分别将音频部分和视频组件分配给 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instp/AVAssetExportSession/audioMix" target="_blank" rel="external">audioMix</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instm/AVAssetExportSession/videoComposition" target="_blank" rel="external">videoComposition</a> 属性。</p>
<center><br>    <img src="http://ww2.sinaimg.cn/large/a9c4d5f6gw1f6galdybkvj20zc0nnab4.jpg" alt="Figure 3-4  Use AVAssetExportSession to combine media elements into an output file"><br></center>

<h2 id="Creating-a-Composition-创建组件"><a href="#Creating-a-Composition-创建组件" class="headerlink" title="Creating a Composition - 创建组件"></a>Creating a Composition - 创建组件</h2><p>To create your own composition, you use the AVMutableComposition class. To add media data to your composition, you must add one or more composition tracks, represented by the AVMutableCompositionTrack class. The simplest case is creating a mutable composition with one video track and one audio track:</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a> 类创建自己的组件。在你的组件中添加媒体数据，必须添加一个或者多个组件轨道，以 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableCompositionTrack_Class/index.html#//apple_ref/occ/cl/AVMutableCompositionTrack" target="_blank" rel="external">AVMutableCompositionTrack</a> 类为代表。最简单的例子创建一个有一个音频轨道和一个视频轨道的可变组件。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableComposition</span> *mutableComposition = [<span class="built_in">AVMutableComposition</span> composition];</span><br><span class="line"><span class="comment">// Create the video composition track.</span></span><br><span class="line"><span class="built_in">AVMutableCompositionTrack</span> *mutableCompositionVideoTrack = [mutableComposition addMutableTrackWithMediaType:<span class="built_in">AVMediaTypeVideo</span> preferredTrackID:kC<span class="built_in">MPersistentTrackID_Invalid</span>];</span><br><span class="line"><span class="comment">// Create the audio composition track.</span></span><br><span class="line"><span class="built_in">AVMutableCompositionTrack</span> *mutableCompositionAudioTrack = [mutableComposition addMutableTrackWithMediaType:<span class="built_in">AVMediaTypeAudio</span> preferredTrackID:kC<span class="built_in">MPersistentTrackID_Invalid</span>];</span><br></pre></td></tr></table></figure>
<h3 id="Options-for-Initializing-a-Composition-Track-初始化组件轨道的选项"><a href="#Options-for-Initializing-a-Composition-Track-初始化组件轨道的选项" class="headerlink" title="Options for Initializing a Composition Track - 初始化组件轨道的选项"></a>Options for Initializing a Composition Track - 初始化组件轨道的选项</h3><p>When adding new tracks to a composition, you must provide both a media type and a track ID. Although audio and video are the most commonly used media types, you can specify other media types as well, such as AVMediaTypeSubtitle or AVMediaTypeText.</p>
<p>Every track associated with some audiovisual data has a unique identifier referred to as a track ID. If you specify kCMPersistentTrackID_Invalid as the preferred track ID, a unique identifier is automatically generated for you and associated with the track.</p>
<p>当给轨道添加一个新的轨道时，必须提供媒体类型和轨道 <code>ID</code> 。虽然音频和视频是最常用的媒体类型，你可以指定其他媒体类型，比如 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVMediaTypeSubtitle" target="_blank" rel="external">AVMediaTypeSubtitle</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVMediaTypeText" target="_blank" rel="external">AVMediaTypeText</a> 。</p>
<p>每个和视听数据相关联的轨道都有一个唯一的标示符，叫做 <code>track ID</code>。如果你指定了 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CoreMedia_Constants/index.html#//apple_ref/c/econst/kCMPersistentTrackID_Invalid" target="_blank" rel="external">kCMPersistentTrackID_Invalid</a> 作为首先的 <code>track ID</code>，将会为你生成一个唯一的标示符并且与轨道相关联。</p>
<h2 id="Adding-Audiovisual-Data-to-a-Composition-将视听数据添加到一个组件中"><a href="#Adding-Audiovisual-Data-to-a-Composition-将视听数据添加到一个组件中" class="headerlink" title="Adding Audiovisual Data to a Composition - 将视听数据添加到一个组件中"></a>Adding Audiovisual Data to a Composition - 将视听数据添加到一个组件中</h2><p>Once you have a composition with one or more tracks, you can begin adding your media data to the appropriate tracks. To add media data to a composition track, you need access to the AVAsset object where the media data is located. You can use the mutable composition track interface to place multiple tracks with the same underlying media type together on the same track. The following example illustrates how to add two different video asset tracks in sequence to the same composition track:</p>
<p>一旦有带着一个或多个轨道的组件，就可以把你的媒体数据添加到适当的轨道中。为了将媒体数据添加到组件轨道，需要访问媒体数据所在位置的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/cl/AVAsset" target="_blank" rel="external">AVAsset</a> 对象。可以使用可变组件轨道接口将有相同基础的媒体类型的多个轨道放置到一个轨道上。下面的示例演示了如何将一个队列中两个不同的音频资产轨道添加到同一个组件轨道中。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// You can retrieve AVAssets from a number of places, like the camera roll for example.</span></span><br><span class="line"><span class="built_in">AVAsset</span> *videoAsset = &lt;<span class="meta">#AVAsset with at least one video track#&gt;;</span></span><br><span class="line"><span class="built_in">AVAsset</span> *anotherVideoAsset = &lt;<span class="meta">#another AVAsset with at least one video track#&gt;;</span></span><br><span class="line"><span class="comment">// Get the first video track from each asset.</span></span><br><span class="line"><span class="built_in">AVAssetTrack</span> *videoAssetTrack = [[videoAsset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line"><span class="built_in">AVAssetTrack</span> *anotherVideoAssetTrack = [[anotherVideoAsset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line"><span class="comment">// Add them both to the composition.</span></span><br><span class="line">[mutableCompositionVideoTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero,videoAssetTrack.timeRange.duration) ofTrack:videoAssetTrack atTime:kCMTimeZero error:<span class="literal">nil</span>];</span><br><span class="line">[mutableCompositionVideoTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero,anotherVideoAssetTrack.timeRange.duration) ofTrack:anotherVideoAssetTrack atTime:videoAssetTrack.timeRange.duration error:<span class="literal">nil</span>];</span><br></pre></td></tr></table></figure>
<h3 id="Retrieving-Compatible-Composition-Tracks-检索兼容的组件轨道"><a href="#Retrieving-Compatible-Composition-Tracks-检索兼容的组件轨道" class="headerlink" title="Retrieving Compatible Composition Tracks - 检索兼容的组件轨道"></a>Retrieving Compatible Composition Tracks - 检索兼容的组件轨道</h3><p>Where possible, you should have only one composition track for each media type. This unification of compatible asset tracks leads to a minimal amount of resource usage. When presenting media data serially, you should place any media data of the same type on the same composition track. You can query a mutable composition to find out if there are any composition tracks compatible with your desired asset track:</p>
<p>在可能的情况下，每个媒体类型应该只有一个组件轨道。这种统一兼容的资产轨道可以达到最小的资源使用量。当串行显示媒体数据时，应该将相同类型的媒体数据放置在相同的组件轨道上。你可以查询一个可变组件，找出是否有组件轨道与你想要的资产轨道兼容。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableCompositionTrack</span> *compatibleCompositionTrack = [mutableComposition mutableTrackCompatibleWithTrack:&lt;<span class="meta">#the AVAssetTrack you want to insert#&gt;];</span></span><br><span class="line"><span class="keyword">if</span> (compatibleCompositionTrack) &#123;</span><br><span class="line">    <span class="comment">// Implementation continues.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Placing multiple video segments on the same composition track can potentially lead to dropping frames at the transitions between video segments, especially on embedded devices. Choosing the number of composition tracks for your video segments depends entirely on the design of your app and its intended platform.</p>
<p>注意：在相同的组件轨道放置多个视频片段，可能会导致在视频片段之间的转换会掉帧，尤其是在嵌入式设备下。你的视频片段的组件轨道数量取决于你的应用程序预期和它的平台设计。</p>
</blockquote>
<h2 id="Generating-a-Volume-Ramp-生成一个音量坡度"><a href="#Generating-a-Volume-Ramp-生成一个音量坡度" class="headerlink" title="Generating a Volume Ramp - 生成一个音量坡度"></a>Generating a Volume Ramp - 生成一个音量坡度</h2><p>A single AVMutableAudioMix object can perform custom audio processing on all of the audio tracks in your composition individually. You create an audio mix using the audioMix class method, and you use instances of the AVMutableAudioMixInputParameters class to associate the audio mix with specific tracks within your composition. An audio mix can be used to vary the volume of an audio track. The following example displays how to set a volume ramp on a specific audio track to slowly fade the audio out over the duration of the composition:</p>
<p>一个单独的 <code>AVMutableAudioMix</code> 对象可以分别执行自定义音频，处理组件中的所有轨道。可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableAudioMix_Class/index.html#//apple_ref/occ/clm/AVMutableAudioMix/audioMix" target="_blank" rel="external">audioMix</a> 类方法创建一个音频混合，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableAudioMixInputParameters_Class/index.html#//apple_ref/occ/cl/AVMutableAudioMixInputParameters" target="_blank" rel="external">AVMutableAudioMixInputParameters</a> 类的实例将混合音频与组件中指定的轨道联结起来。一个混合音频可以用来改变音频轨道的音量。下面的例子展示了，如何在一个指定的音频轨道设置一个音量坡度，使得在组件的持续时间让音频缓慢淡出：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableAudioMix</span> *mutableAudioMix = [<span class="built_in">AVMutableAudioMix</span> audioMix];</span><br><span class="line"><span class="comment">// Create the audio mix input parameters object.</span></span><br><span class="line"><span class="built_in">AVMutableAudioMixInputParameters</span> *mixParameters = [<span class="built_in">AVMutableAudioMixInputParameters</span> audioMixInputParametersWithTrack:mutableCompositionAudioTrack];</span><br><span class="line"><span class="comment">// Set the volume ramp to slowly fade the audio out over the duration of the composition.</span></span><br><span class="line">[mixParameters setVolumeRampFromStartVolume:<span class="number">1.</span>f toEndVolume:<span class="number">0.</span>f timeRange:CMTimeRangeMake(kCMTimeZero, mutableComposition.duration)];</span><br><span class="line"><span class="comment">// Attach the input parameters to the audio mix.</span></span><br><span class="line">mutableAudioMix.inputParameters = @[mixParameters];</span><br></pre></td></tr></table></figure>
<h2 id="Performing-Custom-Video-Processing-执行自定义配置"><a href="#Performing-Custom-Video-Processing-执行自定义配置" class="headerlink" title="Performing Custom Video Processing - 执行自定义配置"></a>Performing Custom Video Processing - 执行自定义配置</h2><p>As with an audio mix, you only need one AVMutableVideoComposition object to perform all of your custom video processing on your composition’s video tracks. Using a video composition, you can directly set the appropriate render size, scale, and frame rate for your composition’s video tracks. For a detailed example of setting appropriate values for these properties, see Setting the Render Size and Frame Duration.</p>
<p>作为一个混合音频，只需要一个 <code>AVMutableVideoComposition</code> 对象就可以执行组件音频轨道中的所有自定义音频配置。使用一个音频组件，可以直接为组件音频轨道设置适当的渲染大小，规模以及帧速率。有一个设置这些属性值的详细的示例，请看 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW18" target="_blank" rel="external">Setting the Render Size and Frame Duration</a></p>
<h3 id="Changing-the-Composition’s-Background-Color-改变组件的背景颜色"><a href="#Changing-the-Composition’s-Background-Color-改变组件的背景颜色" class="headerlink" title="Changing the Composition’s Background Color - 改变组件的背景颜色"></a>Changing the Composition’s Background Color - 改变组件的背景颜色</h3><p>All video compositions must also have an array of AVVideoCompositionInstruction objects containing at least one video composition instruction. You use the AVMutableVideoCompositionInstruction class to create your own video composition instructions. Using video composition instructions, you can modify the composition’s background color, specify whether post processing is needed or apply layer instructions.</p>
<p>The following example illustrates how to create a video composition instruction that changes the background color to red for the entire composition.</p>
<p>所有的视频组件必须有一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVVideoCompositionInstruction_Class/index.html#//apple_ref/occ/cl/AVVideoCompositionInstruction" target="_blank" rel="external">AVVideoCompositionInstruction</a> 对象的数组，每个对象至少包含一个视频组件指令。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionInstruction_Class/index.html#//apple_ref/occ/cl/AVMutableVideoCompositionInstruction" target="_blank" rel="external">AVMutableVideoCompositionInstruction</a> 类去创建自己的视频组件指令。使用视频组件指令，可以修改组件的背景颜色，指定是否需要处理推迟处理或者应用到层指令。</p>
<p>下面的例子展示了如果创建一个视频组件指令，将整个组件的背景颜色改为红色。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableVideoCompositionInstruction</span> *mutableVideoCompositionInstruction = [<span class="built_in">AVMutableVideoCompositionInstruction</span> videoCompositionInstruction];</span><br><span class="line">mutableVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, mutableComposition.duration);</span><br><span class="line">mutableVideoCompositionInstruction.backgroundColor = [[<span class="built_in">UIColor</span> redColor] <span class="built_in">CGColor</span>];</span><br></pre></td></tr></table></figure>
<h3 id="Applying-Opacity-Ramps-应用不透明的坡道"><a href="#Applying-Opacity-Ramps-应用不透明的坡道" class="headerlink" title="Applying Opacity Ramps - 应用不透明的坡道"></a>Applying Opacity Ramps - 应用不透明的坡道</h3><p>Video composition instructions can also be used to apply video composition layer instructions. An AVMutableVideoCompositionLayerInstruction object can apply transforms, transform ramps, opacity and opacity ramps to a certain video track within a composition. The order of the layer instructions in a video composition instruction’s layerInstructions array determines how video frames from source tracks should be layered and composed for the duration of that composition instruction. The following code fragment shows how to set an opacity ramp to slowly fade out the first video in a composition before transitioning to the second video:</p>
<p>视频组件指令可以用于视频组件层指令。一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionLayerInstruction_Class/index.html#//apple_ref/occ/cl/AVMutableVideoCompositionLayerInstruction" target="_blank" rel="external">AVMutableVideoCompositionLayerInstruction</a> 对象可以应用转换，转换坡道，不透明度和坡道的不透明度到某个组件内的视频轨道。视频组件指令的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionInstruction_Class/index.html#//apple_ref/occ/instp/AVMutableVideoCompositionInstruction/layerInstructions" target="_blank" rel="external">layerInstructions</a> 数组中 层指令的顺序决定了组件指令期间，资源轨道中的视频框架应该如何被应用和组合。下面的代码展示了如何设置一个不透明的坡度使得第二个视频之前，让第一个视频慢慢淡出：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *firstVideoAssetTrack = &lt;<span class="meta">#AVAssetTrack representing the first video segment played in the composition#&gt;;</span></span><br><span class="line"><span class="built_in">AVAsset</span> *secondVideoAssetTrack = &lt;<span class="meta">#AVAssetTrack representing the second video segment played in the composition#&gt;;</span></span><br><span class="line"><span class="comment">// Create the first video composition instruction.</span></span><br><span class="line"><span class="built_in">AVMutableVideoCompositionInstruction</span> *firstVideoCompositionInstruction = [<span class="built_in">AVMutableVideoCompositionInstruction</span> videoCompositionInstruction];</span><br><span class="line"><span class="comment">// Set its time range to span the duration of the first video track.</span></span><br><span class="line">firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);</span><br><span class="line"><span class="comment">// Create the layer instruction and associate it with the composition video track.</span></span><br><span class="line"><span class="built_in">AVMutableVideoCompositionLayerInstruction</span> *firstVideoLayerInstruction = [<span class="built_in">AVMutableVideoCompositionLayerInstruction</span> videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];</span><br><span class="line"><span class="comment">// Create the opacity ramp to fade out the first video track over its entire duration.</span></span><br><span class="line">[firstVideoLayerInstruction setOpacityRampFromStartOpacity:<span class="number">1.</span>f toEndOpacity:<span class="number">0.</span>f timeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration)];</span><br><span class="line"><span class="comment">// Create the second video composition instruction so that the second video track isn't transparent.</span></span><br><span class="line"><span class="built_in">AVMutableVideoCompositionInstruction</span> *secondVideoCompositionInstruction = [<span class="built_in">AVMutableVideoCompositionInstruction</span> videoCompositionInstruction];</span><br><span class="line"><span class="comment">// Set its time range to span the duration of the second video track.</span></span><br><span class="line">secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));</span><br><span class="line"><span class="comment">// Create the second layer instruction and associate it with the composition video track.</span></span><br><span class="line"><span class="built_in">AVMutableVideoCompositionLayerInstruction</span> *secondVideoLayerInstruction = [<span class="built_in">AVMutableVideoCompositionLayerInstruction</span> videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];</span><br><span class="line"><span class="comment">// Attach the first layer instruction to the first video composition instruction.</span></span><br><span class="line">firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];</span><br><span class="line"><span class="comment">// Attach the second layer instruction to the second video composition instruction.</span></span><br><span class="line">secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];</span><br><span class="line"><span class="comment">// Attach both of the video composition instructions to the video composition.</span></span><br><span class="line"><span class="built_in">AVMutableVideoComposition</span> *mutableVideoComposition = [<span class="built_in">AVMutableVideoComposition</span> videoComposition];</span><br><span class="line">mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];</span><br></pre></td></tr></table></figure>
<h3 id="Incorporating-Core-Animation-Effects-结合核心动画效果"><a href="#Incorporating-Core-Animation-Effects-结合核心动画效果" class="headerlink" title="Incorporating Core Animation Effects - 结合核心动画效果"></a>Incorporating Core Animation Effects - 结合核心动画效果</h3><p>A video composition can add the power of Core Animation to your composition through the animationTool property. Through this animation tool, you can accomplish tasks such as watermarking video and adding titles or animating overlays. Core Animation can be used in two different ways with video compositions: You can add a Core Animation layer as its own individual composition track, or you can render Core Animation effects (using a Core Animation layer) into the video frames in your composition directly. The following code displays the latter option by adding a watermark to the center of the video:</p>
<p>一个视频组件可以通过 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instp/AVMutableVideoComposition/animationTool" target="_blank" rel="external">animationTool</a> 属性将核心动画的力量添加到你的组件中。通过这个动画制作工具，可以完成一些任务，例如视频水印，添加片头或者动画覆盖。核心动画可以有两种不同的方式被用于视频组件：可以添加一个核心动画层到自己的个人组件轨道，或者可以渲染核心动画效果（使用一个核心动画层）直接进入组件的视频框架。下面的代码展示了在视频中央添加一个水印显示出来的效果。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CALayer</span> *watermarkLayer = &lt;<span class="meta">#CALayer representing your desired watermark image#&gt;;</span></span><br><span class="line"><span class="built_in">CALayer</span> *parentLayer = [<span class="built_in">CALayer</span> layer];</span><br><span class="line"><span class="built_in">CALayer</span> *videoLayer = [<span class="built_in">CALayer</span> layer];</span><br><span class="line">parentLayer.frame = <span class="built_in">CGRectMake</span>(<span class="number">0</span>, <span class="number">0</span>, mutableVideoComposition.renderSize.width, mutableVideoComposition.renderSize.height);</span><br><span class="line">videoLayer.frame = <span class="built_in">CGRectMake</span>(<span class="number">0</span>, <span class="number">0</span>, mutableVideoComposition.renderSize.width, mutableVideoComposition.renderSize.height);</span><br><span class="line">[parentLayer addSublayer:videoLayer];</span><br><span class="line">watermarkLayer.position = <span class="built_in">CGPointMake</span>(mutableVideoComposition.renderSize.width/<span class="number">2</span>, mutableVideoComposition.renderSize.height/<span class="number">4</span>);</span><br><span class="line">[parentLayer addSublayer:watermarkLayer];</span><br><span class="line">mutableVideoComposition.animationTool = [<span class="built_in">AVVideoCompositionCoreAnimationTool</span> videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer:videoLayer inLayer:parentLayer];</span><br></pre></td></tr></table></figure>
<h2 id="Putting-It-All-Together-Combining-Multiple-Assets-and-Saving-the-Result-to-the-Camera-Roll"><a href="#Putting-It-All-Together-Combining-Multiple-Assets-and-Saving-the-Result-to-the-Camera-Roll" class="headerlink" title="Putting It All Together: Combining Multiple Assets and Saving the Result to the Camera Roll -"></a>Putting It All Together: Combining Multiple Assets and Saving the Result to the Camera Roll -</h2><p>This brief code example illustrates how you can combine two video asset tracks and an audio asset track to create a single video file. It shows how to:</p>
<ul>
<li>Create an AVMutableComposition object and add multiple AVMutableCompositionTrack objects</li>
<li>Add time ranges of AVAssetTrack objects to compatible composition tracks</li>
<li>Check the preferredTransform property of a video asset track to determine the video’s orientation</li>
<li>Use AVMutableVideoCompositionLayerInstruction objects to apply transforms to the video tracks within - a composition</li>
<li>Set appropriate values for the renderSize and frameDuration properties of a video composition</li>
<li>Use a composition in conjunction with a video composition when exporting to a video file</li>
<li>Save a video file to the Camera Roll</li>
</ul>
<p>这个简短的代码示例说明了如何将两个视频资产轨道和一个音频资产轨道结合起来，创建一个单独的视频文件。有下面几个方面：</p>
<ul>
<li>创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a> 对象并且添加多个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableCompositionTrack_Class/index.html#//apple_ref/occ/cl/AVMutableCompositionTrack" target="_blank" rel="external">AVMutableCompositionTrack</a> 对象</li>
<li>添加 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/cl/AVAssetTrack" target="_blank" rel="external">AVAssetTrack</a> 对象的时间范围，兼容组件轨道</li>
<li>检查视频资产轨道的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/instm/AVAssetTrack/preferredTransform" target="_blank" rel="external">preferredTransform</a> 的属性，决定视频的方向</li>
<li>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionLayerInstruction_Class/index.html#//apple_ref/occ/cl/AVMutableVideoCompositionLayerInstruction" target="_blank" rel="external">AVMutableVideoCompositionLayerInstruction</a> 对象给组件内的视频轨道应用转换。</li>
<li>给视频组件的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instm/AVMutableVideoComposition/renderSize" target="_blank" rel="external">renderSize</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instp/AVMutableVideoComposition/frameDuration" target="_blank" rel="external">frameDuration</a> 属性设置适当的值。</li>
<li>当导出视频文件时，使用一个视频组件组合物中的组件</li>
<li>保存视频文件到相机胶卷</li>
</ul>
<blockquote>
<p>Note: To focus on the most relevant code, this example omits several aspects of a complete app, such as memory management and error handling. To use AVFoundation, you are expected to have enough experience with Cocoa to infer the missing pieces.</p>
<p>注意：关注最相关的代码，这个例子省略了一个完整应用程序的几个方面，如内存处理和错误处理。利用 <code>AVFoundation</code> ，希望你有足够的使用 <code>Cocoa</code> 的经验去判断丢失的碎片</p>
</blockquote>
<h3 id="Creating-the-Composition-创建组件"><a href="#Creating-the-Composition-创建组件" class="headerlink" title="Creating the Composition - 创建组件"></a>Creating the Composition - 创建组件</h3><p>To piece together tracks from separate assets, you use an AVMutableComposition object. Create the composition and add one audio and one video track.</p>
<p>使用 <code>AVMutableComposition</code> 对象将分离的资产拼凑成轨道。创建组件并且添加一个音频轨道和一个视频轨道。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableComposition</span> *mutableComposition = [<span class="built_in">AVMutableComposition</span> composition];</span><br><span class="line"><span class="built_in">AVMutableCompositionTrack</span> *videoCompositionTrack = [mutableComposition addMutableTrackWithMediaType:<span class="built_in">AVMediaTypeVideo</span> preferredTrackID:kC<span class="built_in">MPersistentTrackID_Invalid</span>];</span><br><span class="line"><span class="built_in">AVMutableCompositionTrack</span> *audioCompositionTrack = [mutableComposition addMutableTrackWithMediaType:<span class="built_in">AVMediaTypeAudio</span> preferredTrackID:kC<span class="built_in">MPersistentTrackID_Invalid</span>];</span><br></pre></td></tr></table></figure>
<h3 id="Adding-the-Assets-添加资产"><a href="#Adding-the-Assets-添加资产" class="headerlink" title="Adding the Assets - 添加资产"></a>Adding the Assets - 添加资产</h3><p>An empty composition does you no good. Add the two video asset tracks and the audio asset track to the composition.</p>
<p>一个空的资产并不是好。往组件中添加两个视频资产轨道和音频资产轨道。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAssetTrack</span> *firstVideoAssetTrack = [[firstVideoAsset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line"><span class="built_in">AVAssetTrack</span> *secondVideoAssetTrack = [[secondVideoAsset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line">[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration) ofTrack:firstVideoAssetTrack atTime:kCMTimeZero error:<span class="literal">nil</span>];</span><br><span class="line">[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, secondVideoAssetTrack.timeRange.duration) ofTrack:secondVideoAssetTrack atTime:firstVideoAssetTrack.timeRange.duration error:<span class="literal">nil</span>];</span><br><span class="line">[audioCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration)) ofTrack:[[audioAsset tracksWithMediaType:<span class="built_in">AVMediaTypeAudio</span>] objectAtIndex:<span class="number">0</span>] atTime:kCMTimeZero error:<span class="literal">nil</span>];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: This assumes that you have two assets that contain at least one video track each and a third asset that contains at least one audio track. The videos can be retrieved from the Camera Roll, and the audio track can be retrieved from the music library or the videos themselves.</p>
<p>注意：这里假定你有两个资产，每个资产中都至少包含一个视频轨道，第三个资产至少包含一个音频轨道。视频可以从相机胶卷中检索到，音频轨道可以从音乐库或者视频本身检索到。</p>
</blockquote>
<h3 id="Checking-the-Video-Orientations-检查视频的方向"><a href="#Checking-the-Video-Orientations-检查视频的方向" class="headerlink" title="Checking the Video Orientations - 检查视频的方向"></a>Checking the Video Orientations - 检查视频的方向</h3><p>Once you add your video and audio tracks to the composition, you need to ensure that the orientations of both video tracks are correct. By default, all video tracks are assumed to be in landscape mode. If your video track was taken in portrait mode, the video will not be oriented properly when it is exported. Likewise, if you try to combine a video shot in portrait mode with a video shot in landscape mode, the export session will fail to complete.</p>
<p>一旦给组件添加了音频和视频轨道，你需要确保两个视频轨道的方向都是正确的。默认情况下，所有的视频轨道都被假定为横向模式。如果你的视频轨道是在纵向模式下拍摄的，当它被导出的时候方向将出现错误。同样，如果你尝试将横向模式下拍摄的视频与纵向的视频结合在一起，导出会话将无法完成。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">BOOL</span> isFirstVideoPortrait = <span class="literal">NO</span>;</span><br><span class="line"><span class="built_in">CGAffineTransform</span> firstTransform = firstVideoAssetTrack.preferredTransform;</span><br><span class="line"><span class="comment">// Check the first video track's preferred transform to determine if it was recorded in portrait mode.</span></span><br><span class="line"><span class="keyword">if</span> (firstTransform.a == <span class="number">0</span> &amp;&amp; firstTransform.d == <span class="number">0</span> &amp;&amp; (firstTransform.b == <span class="number">1.0</span> || firstTransform.b == <span class="number">-1.0</span>) &amp;&amp; (firstTransform.c == <span class="number">1.0</span> || firstTransform.c == <span class="number">-1.0</span>)) &#123;</span><br><span class="line">    isFirstVideoPortrait = <span class="literal">YES</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">BOOL</span> isSecondVideoPortrait = <span class="literal">NO</span>;</span><br><span class="line"><span class="built_in">CGAffineTransform</span> secondTransform = secondVideoAssetTrack.preferredTransform;</span><br><span class="line"><span class="comment">// Check the second video track's preferred transform to determine if it was recorded in portrait mode.</span></span><br><span class="line"><span class="keyword">if</span> (secondTransform.a == <span class="number">0</span> &amp;&amp; secondTransform.d == <span class="number">0</span> &amp;&amp; (secondTransform.b == <span class="number">1.0</span> || secondTransform.b == <span class="number">-1.0</span>) &amp;&amp; (secondTransform.c == <span class="number">1.0</span> || secondTransform.c == <span class="number">-1.0</span>)) &#123;</span><br><span class="line">    isSecondVideoPortrait = <span class="literal">YES</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ((isFirstVideoAssetPortrait &amp;&amp; !isSecondVideoAssetPortrait) || (!isFirstVideoAssetPortrait &amp;&amp; isSecondVideoAssetPortrait)) &#123;</span><br><span class="line">    <span class="built_in">UIAlertView</span> *incompatibleVideoOrientationAlert = [[<span class="built_in">UIAlertView</span> alloc] initWithTitle:<span class="string">@"Error!"</span> message:<span class="string">@"Cannot combine a video shot in portrait mode with a video shot in landscape mode."</span> delegate:<span class="keyword">self</span> cancelButtonTitle:<span class="string">@"Dismiss"</span> otherButtonTitles:<span class="literal">nil</span>];</span><br><span class="line">    [incompatibleVideoOrientationAlert show];</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Applying-the-Video-Composition-Layer-Instructions-视频组件层指令的应用"><a href="#Applying-the-Video-Composition-Layer-Instructions-视频组件层指令的应用" class="headerlink" title="Applying the Video Composition Layer Instructions - 视频组件层指令的应用"></a>Applying the Video Composition Layer Instructions - 视频组件层指令的应用</h3><p>Once you know the video segments have compatible orientations, you can apply the necessary layer instructions to each one and add these layer instructions to the video composition.</p>
<p>如果你知道视频片段对方向有兼容性，可以将必要的层指令应用到每个视频片段，并将这些层指令添加到视频组合中。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVMutableVideoCompositionInstruction</span> *firstVideoCompositionInstruction = [<span class="built_in">AVMutableVideoCompositionInstruction</span> videoCompositionInstruction];</span><br><span class="line"><span class="comment">// Set the time range of the first instruction to span the duration of the first video track.</span></span><br><span class="line">firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);</span><br><span class="line"><span class="built_in">AVMutableVideoCompositionInstruction</span> * secondVideoCompositionInstruction = [<span class="built_in">AVMutableVideoCompositionInstruction</span> videoCompositionInstruction];</span><br><span class="line"><span class="comment">// Set the time range of the second instruction to span the duration of the second video track.</span></span><br><span class="line">secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));</span><br><span class="line"><span class="built_in">AVMutableVideoCompositionLayerInstruction</span> *firstVideoLayerInstruction = [<span class="built_in">AVMutableVideoCompositionLayerInstruction</span> videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];</span><br><span class="line"><span class="comment">// Set the transform of the first layer instruction to the preferred transform of the first video track.</span></span><br><span class="line">[firstVideoLayerInstruction setTransform:firstTransform atTime:kCMTimeZero];</span><br><span class="line"><span class="built_in">AVMutableVideoCompositionLayerInstruction</span> *secondVideoLayerInstruction = [<span class="built_in">AVMutableVideoCompositionLayerInstruction</span> videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];</span><br><span class="line"><span class="comment">// Set the transform of the second layer instruction to the preferred transform of the second video track.</span></span><br><span class="line">[secondVideoLayerInstruction setTransform:secondTransform atTime:firstVideoAssetTrack.timeRange.duration];</span><br><span class="line">firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];</span><br><span class="line">secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];</span><br><span class="line"><span class="built_in">AVMutableVideoComposition</span> *mutableVideoComposition = [<span class="built_in">AVMutableVideoComposition</span> videoComposition];</span><br><span class="line">mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];</span><br></pre></td></tr></table></figure>
<p>All AVAssetTrack objects have a preferredTransform property that contains the orientation information for that asset track. This transform is applied whenever the asset track is displayed onscreen. In the previous code, the layer instruction’s transform is set to the asset track’s transform so that the video in the new composition displays properly once you adjust its render size.</p>
<p>所有的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/cl/AVAssetTrack" target="_blank" rel="external">AVAssetTrack</a> 对象都有一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/instm/AVAssetTrack/preferredTransform" target="_blank" rel="external">preferredTransform</a> 属性，包含了资产轨道的方向信息。当资产轨道被展示到屏幕上时就进行这些转换。在之前的代码中，层指令信息的转换被设置为资产轨道的转换，使得一旦你调整了它的渲染大小，视频在新的组件中都能正确的显示。</p>
<h3 id="Setting-the-Render-Size-and-Frame-Duration-设置渲染大小和帧周期"><a href="#Setting-the-Render-Size-and-Frame-Duration-设置渲染大小和帧周期" class="headerlink" title="Setting the Render Size and Frame Duration - 设置渲染大小和帧周期"></a>Setting the Render Size and Frame Duration - 设置渲染大小和帧周期</h3><p>To complete the video orientation fix, you must adjust the renderSize property accordingly. You should also pick a suitable value for the frameDuration property, such as 1/30th of a second (or 30 frames per second). By default, the renderScale property is set to 1.0, which is appropriate for this composition.</p>
<p>为了完成视频方向的固定，必须调整相应的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instp/AVMutableVideoComposition/renderSize" target="_blank" rel="external">renderSize</a> 属性。也应该给 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instm/AVMutableVideoComposition/frameDuration" target="_blank" rel="external">frameDuration</a> 属性设置一个合适的值，比如 1/30th of a second (或者每秒30帧)。默认情况下，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoComposition_Class/index.html#//apple_ref/occ/instm/AVMutableVideoComposition/renderScale" target="_blank" rel="external">renderScale</a> 属性设置 <code>1.0</code>，对于组件是比较合适的。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CGSize</span> naturalSizeFirst, naturalSizeSecond;</span><br><span class="line"><span class="comment">// If the first video asset was shot in portrait mode, then so was the second one if we made it here.</span></span><br><span class="line"><span class="keyword">if</span> (isFirstVideoAssetPortrait) &#123;</span><br><span class="line"><span class="comment">// Invert the width and height for the video tracks to ensure that they display properly.</span></span><br><span class="line">    naturalSizeFirst = <span class="built_in">CGSizeMake</span>(firstVideoAssetTrack.naturalSize.height, firstVideoAssetTrack.naturalSize.width);</span><br><span class="line">    naturalSizeSecond = <span class="built_in">CGSizeMake</span>(secondVideoAssetTrack.naturalSize.height, secondVideoAssetTrack.naturalSize.width);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// If the videos weren't shot in portrait mode, we can just use their natural sizes.</span></span><br><span class="line">    naturalSizeFirst = firstVideoAssetTrack.naturalSize;</span><br><span class="line">    naturalSizeSecond = secondVideoAssetTrack.naturalSize;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">float</span> renderWidth, renderHeight;</span><br><span class="line"><span class="comment">// Set the renderWidth and renderHeight to the max of the two videos widths and heights.</span></span><br><span class="line"><span class="keyword">if</span> (naturalSizeFirst.width &gt; naturalSizeSecond.width) &#123;</span><br><span class="line">    renderWidth = naturalSizeFirst.width;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    renderWidth = naturalSizeSecond.width;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (naturalSizeFirst.height &gt; naturalSizeSecond.height) &#123;</span><br><span class="line">    renderHeight = naturalSizeFirst.height;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    renderHeight = naturalSizeSecond.height;</span><br><span class="line">&#125;</span><br><span class="line">mutableVideoComposition.renderSize = <span class="built_in">CGSizeMake</span>(renderWidth, renderHeight);</span><br><span class="line"><span class="comment">// Set the frame duration to an appropriate value (i.e. 30 frames per second for video).</span></span><br><span class="line">mutableVideoComposition.frameDuration = CMTimeMake(<span class="number">1</span>,<span class="number">30</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Exporting-the-Composition-and-Saving-it-to-the-Camera-Roll-导出组件并存到相机胶卷"><a href="#Exporting-the-Composition-and-Saving-it-to-the-Camera-Roll-导出组件并存到相机胶卷" class="headerlink" title="Exporting the Composition and Saving it to the Camera Roll - 导出组件并存到相机胶卷"></a>Exporting the Composition and Saving it to the Camera Roll - 导出组件并存到相机胶卷</h3><p>The final step in this process involves exporting the entire composition into a single video file and saving that video to the camera roll. You use an AVAssetExportSession object to create the new video file and you pass to it your desired URL for the output file. You can then use the ALAssetsLibrary class to save the resulting video file to the Camera Roll.</p>
<p>这个过程的最后一步，是将整个组件导出到一个单独的视频文件，并且将视频存到相机胶卷中。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/cl/AVAssetExportSession" target="_blank" rel="external">AVAssetExportSession</a> 对象去创建新的视频文件，并且给输出文件传递一个期望的 <code>URL</code> 。然后可以使用 <a href="https://developer.apple.com/library/ios/documentation/AssetsLibrary/Reference/ALAssetsLibrary_Class/index.html#//apple_ref/occ/cl/ALAssetsLibrary" target="_blank" rel="external">ALAssetsLibrary</a> 类去将视频文件结果保存到相机胶卷。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a static date formatter so we only have to initialize it once.</span></span><br><span class="line"><span class="keyword">static</span> <span class="built_in">NSDateFormatter</span> *kDateFormatter;</span><br><span class="line"><span class="keyword">if</span> (!kDateFormatter) &#123;</span><br><span class="line">    kDateFormatter = [[<span class="built_in">NSDateFormatter</span> alloc] init];</span><br><span class="line">    kDateFormatter.dateStyle = <span class="built_in">NSDateFormatterMediumStyle</span>;</span><br><span class="line">    kDateFormatter.timeStyle = <span class="built_in">NSDateFormatterShortStyle</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Create the export session with the composition and set the preset to the highest quality.</span></span><br><span class="line"><span class="built_in">AVAssetExportSession</span> *exporter = [[<span class="built_in">AVAssetExportSession</span> alloc] initWithAsset:mutableComposition presetName:<span class="built_in">AVAssetExportPresetHighestQuality</span>];</span><br><span class="line"><span class="comment">// Set the desired output URL for the file created by the export process.</span></span><br><span class="line">exporter.outputURL = [[[[<span class="built_in">NSFileManager</span> defaultManager] URLForDirectory:<span class="built_in">NSDocumentDirectory</span> inDomain:<span class="built_in">NSUserDomainMask</span> appropriateForURL:<span class="literal">nil</span> create:@YES error:<span class="literal">nil</span>] URLByAppendingPathComponent:[kDateFormatter stringFromDate:[<span class="built_in">NSDate</span> date]]] URLByAppendingPathExtension:<span class="built_in">CFBridgingRelease</span>(UTTypeCopyPreferredTagWithClass((<span class="built_in">CFStringRef</span>)<span class="built_in">AVFileTypeQuickTimeMovie</span>, kUTTagClassFilenameExtension))];</span><br><span class="line"><span class="comment">// Set the output file type to be a QuickTime movie.</span></span><br><span class="line">exporter.outputFileType = <span class="built_in">AVFileTypeQuickTimeMovie</span>;</span><br><span class="line">exporter.shouldOptimizeForNetworkUse = <span class="literal">YES</span>;</span><br><span class="line">exporter.videoComposition = mutableVideoComposition;</span><br><span class="line"><span class="comment">// Asynchronously export the composition to a video file and save this file to the camera roll once export completes.</span></span><br><span class="line">[exporter exportAsynchronouslyWithCompletionHandler:^&#123;</span><br><span class="line">    <span class="built_in">dispatch_async</span>(dispatch_get_main_queue(), ^&#123;</span><br><span class="line">        <span class="keyword">if</span> (exporter.status == <span class="built_in">AVAssetExportSessionStatusCompleted</span>) &#123;</span><br><span class="line">            ALAssetsLibrary *assetsLibrary = [[ALAssetsLibrary alloc] init];</span><br><span class="line">            <span class="keyword">if</span> ([assetsLibrary videoAtPathIsCompatibleWithSavedPhotosAlbum:exporter.outputURL]) &#123;</span><br><span class="line">                [assetsLibrary writeVideoAtPathToSavedPhotosAlbum:exporter.outputURL completionBlock:<span class="literal">NULL</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<h1 id="Still-and-Video-Media-Capture-静态视频媒体捕获。"><a href="#Still-and-Video-Media-Capture-静态视频媒体捕获。" class="headerlink" title="Still and Video Media Capture - 静态视频媒体捕获。"></a>Still and Video Media Capture - 静态视频媒体捕获。</h1><p>To manage the capture from a device such as a camera or microphone, you assemble objects to represent inputs and outputs, and use an instance of AVCaptureSession to coordinate the data flow between them. Minimally you need:</p>
<ul>
<li>An instance of AVCaptureDevice to represent the input device, such as a camera or microphone</li>
<li>An instance of a concrete subclass of AVCaptureInput to configure the ports from the input device</li>
<li>An instance of a concrete subclass of AVCaptureOutput to manage the output to a movie file or still image</li>
<li>An instance of AVCaptureSession to coordinate the data flow from the input to the output</li>
</ul>
<p>从一个设备，例如照相机或者麦克风管理捕获，组合对象来表示输入和输出，并使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/cl/AVCaptureSession" target="_blank" rel="external">AVCaptureSession</a> 的实例来协调它们之间的数据流。你需要最低限度的了解：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/cl/AVCaptureDevice" target="_blank" rel="external">AVCaptureDevice</a> 的实例表示输入设备，比如照相机或麦克风</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInput_Class/index.html#//apple_ref/occ/cl/AVCaptureInput" target="_blank" rel="external">AVCaptureInput</a> 的具体子类的实例从输入设备配置端口</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureOutput" target="_blank" rel="external">AVCaptureOutput</a> 的具体子类的实例来管理输出一个电影文件或者静态图像</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/cl/AVCaptureSession" target="_blank" rel="external">AVCaptureSession</a> 的实例从输入到输出协调数据流</li>
</ul>
<p>To show the user a preview of what the camera is recording, you can use an instance of AVCaptureVideoPreviewLayer (a subclass of CALayer).</p>
<p>You can configure multiple inputs and outputs, coordinated by a single session, as shown in Figure 4-1</p>
<p>为了向用户展示照相机之前记录的预览，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoPreviewLayer_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoPreviewLayer" target="_blank" rel="external">AVCaptureVideoPreviewLayer</a> 的实例（<a href="https://developer.apple.com/library/ios/documentation/GraphicsImaging/Reference/CALayer_class/index.html#//apple_ref/occ/cl/CALayer" target="_blank" rel="external">CALayer</a> 的一个子类）</p>
<p>可以配置多个输入和输出，由一个单独的会话协调。如图4-1所示：</p>
<center><br>    <img src="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6gm15jfy4j20ug0h9753.jpg" alt="Figure 4-1  A single session can configure multiple inputs and outputs"><br></center>

<p>For many applications, this is as much detail as you need. For some operations, however, (if you want to monitor the power levels in an audio channel, for example) you need to consider how the various ports of an input device are represented and how those ports are connected to the output.</p>
<p>对于大多数程序，这有尽可能多的你需要知道的细节。然而对于某些操作（例如如果你想监视音频信道中的功率水平），需要考虑输入设备的各种端口如何表示，以及这些端口是如何连接到输出的。</p>
<p>A connection between a capture input and a capture output in a capture session is represented by an AVCaptureConnection object. Capture inputs (instances of AVCaptureInput) have one or more input ports (instances of AVCaptureInputPort). Capture outputs (instances of AVCaptureOutput) can accept data from one or more sources (for example, an AVCaptureMovieFileOutput object accepts both video and audio data).</p>
<p>捕获输入和捕获输出的会话之间的连接表现为 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/cl/AVCaptureConnection" target="_blank" rel="external">AVCaptureConnection</a> 对象。捕获输入（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInput_Class/index.html#//apple_ref/occ/cl/AVCaptureInput" target="_blank" rel="external">AVCaptureInput</a>的实例）有一个或多个输入端口（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInputPort_Class/index.html#//apple_ref/occ/cl/AVCaptureInputPort" target="_blank" rel="external">AVCaptureInputPort</a>的实例）。捕获输出（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureOutput" target="_blank" rel="external">AVCaptureOutput</a>的实例）可以从一个或多个资源接受数据（例如，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput" target="_blank" rel="external">AVCaptureMovieFileOutput</a> 对象接受音频和视频数据）。</p>
<p>When you add an input or an output to a session, the session forms connections between all the compatible capture inputs’ ports and capture outputs, as shown in Figure 4-2. A connection between a capture input and a capture output is represented by an AVCaptureConnection object.</p>
<p>当给会话添加一个输入或者一个输出时，会话构成了所有可兼容的捕获输入端口和捕获输出端口的连接，如图4-2所示。捕获输入与捕获输出之间的连接是由 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/cl/AVCaptureConnection" target="_blank" rel="external">AVCaptureConnection</a> 对象表示。</p>
<center><br>    <img src="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6gmf843fwj20vi0n9jsb.jpg" alt="Figure 4-2  AVCaptureConnection represents a connection between an input and output"><br></center>

<p>You can use a capture connection to enable or disable the flow of data from a given input or to a given output. You can also use a connection to monitor the average and peak power levels in an audio channel.</p>
<p>可以使用捕获连接来启用或者禁用给定输入或给定输出的数据流。也可以使用连接来监视音频信道中的平均和峰值功率水平。</p>
<blockquote>
<p>Note: Media capture does not support simultaneous capture of both the front-facing and back-facing cameras on iOS devices.</p>
<p>注意：媒体捕获不支持iOS设备上的前置摄像头和后置摄像头的同时捕捉。</p>
</blockquote>
<h2 id="Use-a-Capture-Session-to-Coordinate-Data-Flow-使用捕捉会话来协调数据流"><a href="#Use-a-Capture-Session-to-Coordinate-Data-Flow-使用捕捉会话来协调数据流" class="headerlink" title="Use a Capture Session to Coordinate Data Flow - 使用捕捉会话来协调数据流"></a>Use a Capture Session to Coordinate Data Flow - 使用捕捉会话来协调数据流</h2><p>An AVCaptureSession object is the central coordinating object you use to manage data capture. You use an instance to coordinate the flow of data from AV input devices to outputs. You add the capture devices and outputs you want to the session, then start data flow by sending the session a startRunning message, and stop the data flow by sending a stopRunning message.</p>
<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/cl/AVCaptureSession" target="_blank" rel="external">AVCaptureSession</a> 对象是你用来管理数据捕获的中央协调对象。使用一个实例来协调从 <code>AV</code> 输入设备到输出的数据流。添加捕获设备并且输出你想要的会话，然后发送一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/startRunning" target="_blank" rel="external">startRunning</a> 消息启动数据流，发送 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/stopRunning" target="_blank" rel="external">stopRunning</a> 消息来停止数据流。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *session = [[<span class="built_in">AVCaptureSession</span> alloc] init];</span><br><span class="line"><span class="comment">// Add inputs and outputs.</span></span><br><span class="line">[session startRunning];</span><br></pre></td></tr></table></figure>
<h3 id="Configuring-a-Session-配置会话"><a href="#Configuring-a-Session-配置会话" class="headerlink" title="Configuring a Session - 配置会话"></a>Configuring a Session - 配置会话</h3><p>You use a preset on the session to specify the image quality and resolution you want. A preset is a constant that identifies one of a number of possible configurations; in some cases the actual configuration is device-specific:</p>
<p>使用会话上的 <code>preset</code> 来指定图像的质量和分辨率。预设是一个常数，确定了一部分可能的配置中的一个；在某些情况下，设计的配置是设备特有的：</p>
<p>| Symbol | Resolution | Comments |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPresetHigh" target="_blank" rel="external">AVCaptureSessionPresetHigh</a> | High | Highest recording quality.This varies per device.|<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPresetMedium" target="_blank" rel="external">AVCaptureSessionPresetMedium</a> | Medium | Suitable for Wi-Fi sharing.The actual values may change.|<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPresetLow" target="_blank" rel="external">AVCaptureSessionPresetLow</a> | Low | Suitable for 3G sharing.The actual values may change. |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPreset640x480" target="_blank" rel="external">AVCaptureSessionPreset640x480</a> | 640x480 | VGA |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPreset1280x720" target="_blank" rel="external">AVCaptureSessionPreset1280x720</a> | 1280x720 | 720p HD. |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionPresetPhoto" target="_blank" rel="external">AVCaptureSessionPresetPhoto</a> | Photo | Full photo resolution.This is not supported for video output. |</p>
<p>If you want to set a media frame size-specific configuration, you should check whether it is supported before setting it, as follows:</p>
<p>如果要设置媒体帧特定大小的配置，应该在设置之前检查是否支持被设定，如下所示：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ([session canSetSessionPreset:<span class="built_in">AVCaptureSessionPreset1280x720</span>]) &#123;</span><br><span class="line">    session.sessionPreset = <span class="built_in">AVCaptureSessionPreset1280x720</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Handle the failure.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>If you need to adjust session parameters at a more granular level than is possible with a preset, or you’d like to make changes to a running session, you surround your changes with the beginConfiguration and commitConfiguration methods. The beginConfiguration and commitConfiguration methods ensure that devices changes occur as a group, minimizing visibility or inconsistency of state. After calling beginConfiguration, you can add or remove outputs, alter the sessionPreset property, or configure individual capture input or output properties. No changes are actually made until you invoke commitConfiguration, at which time they are applied together.</p>
<p>如果需要比预设情况，更加精细的水平调整会话参数，或者想给一个正在运行的会话做些改变，用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/beginConfiguration" target="_blank" rel="external">beginConfiguration</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/commitConfiguration" target="_blank" rel="external">commitConfiguration</a> 方法。<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/beginConfiguration" target="_blank" rel="external">beginConfiguration</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/commitConfiguration" target="_blank" rel="external">commitConfiguration</a> 方法确保设备作为一个群体在变化，降低状态的清晰度或者不协调性。调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/beginConfiguration" target="_blank" rel="external">beginConfiguration</a> 之后，可以添加或者移除输出，改变 <code>sessionPreset</code> 属性，或者单独配置捕获输入或输出属性。在你调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/commitConfiguration" target="_blank" rel="external">commitConfiguration</a> 之前实际上是没有变化的，调用的时候它们才被应用到一起。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[session beginConfiguration];</span><br><span class="line"><span class="comment">// Remove an existing capture device.</span></span><br><span class="line"><span class="comment">// Add a new capture device.</span></span><br><span class="line"><span class="comment">// Reset the preset.</span></span><br><span class="line">[session commitConfiguration];</span><br></pre></td></tr></table></figure>
<h3 id="Monitoring-Capture-Session-State-监视捕获会话状态"><a href="#Monitoring-Capture-Session-State-监视捕获会话状态" class="headerlink" title="Monitoring Capture Session State - 监视捕获会话状态"></a>Monitoring Capture Session State - 监视捕获会话状态</h3><p>A capture session posts notifications that you can observe to be notified, for example, when it starts or stops running, or when it is interrupted. You can register to receive an AVCaptureSessionRuntimeErrorNotification if a runtime error occurs. You can also interrogate the session’s running property to find out if it is running, and its interrupted property to find out if it is interrupted. Additionally, both the running and interrupted properties are key-value observing compliant and the notifications are posted on the main thread.</p>
<p>捕获会话发出你能观察并被通知到的 <code>notifications</code>，例如，当它开始或者停止运行，或者当它被中断。你可以注册，如果发生了运行阶段的错误，可以接收 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/data/AVCaptureSessionRuntimeErrorNotification" target="_blank" rel="external">AVCaptureSessionRuntimeErrorNotification</a> 。也可以询问会话的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/isRunning" target="_blank" rel="external">running</a> 属性去发现它正在运行的状态，并且它的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instp/AVCaptureSession/interrupted" target="_blank" rel="external">interrupted</a> 属性可以找到它是否被中断了。此外， <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/isRunning" target="_blank" rel="external">running</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instp/AVCaptureSession/interrupted" target="_blank" rel="external">interrupted</a> 属性是遵从<code>key-value observing</code> ，并且在通知都是在主线程上发布的。</p>
<h2 id="An-AVCaptureDevice-Object-Represents-an-Input-Device-一个-AVCaptureDevice-对象代表一个输入设备"><a href="#An-AVCaptureDevice-Object-Represents-an-Input-Device-一个-AVCaptureDevice-对象代表一个输入设备" class="headerlink" title="An AVCaptureDevice Object Represents an Input Device - 一个 AVCaptureDevice 对象代表一个输入设备"></a>An AVCaptureDevice Object Represents an Input Device - 一个 <code>AVCaptureDevice</code> 对象代表一个输入设备</h2><p>An AVCaptureDevice object abstracts a physical capture device that provides input data (such as audio or video) to an AVCaptureSession object. There is one object for each input device, for example, two video inputs—one for the front-facing the camera, one for the back-facing camera—and one audio input for the microphone.</p>
<p>一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/cl/AVCaptureDevice" target="_blank" rel="external">AVCaptureDevice</a> 对象抽象出物理捕获设备，提供了输入数据(比如音频或者视频)给 <code>AVCaptureSession</code> 对象。例如每个输入设备都有一个对象，两个视频输入，一个用于前置摄像头，一个用于后置摄像头，一个用于麦克风的音频输入。</p>
<p>You can find out which capture devices are currently available using the AVCaptureDevice class methods devices and devicesWithMediaType:. And, if necessary, you can find out what features an iPhone, iPad, or iPod offers (see Device Capture Settings). The list of available devices may change, though. Current input devices may become unavailable (if they’re used by another application), and new input devices may become available, (if they’re relinquished by another application). You should register to receive AVCaptureDeviceWasConnectedNotification and AVCaptureDeviceWasDisconnectedNotification notifications to be alerted when the list of available devices changes.</p>
<p>使用 <code>AVCaptureDevice</code> 类方法 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/clm/AVCaptureDevice/devices" target="_blank" rel="external">devices</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/clm/AVCaptureDevice/devicesWithMediaType:" target="_blank" rel="external">devicesWithMediaType:</a> 可以找出哪一个捕获设备当前是可用的。而且如果有必要，可以找出 <code>iPhone</code>，<code>iPad</code> 或者 <code>iPod</code> 提供了什么功能（详情看：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW18" target="_blank" rel="external">Device Capture Settings</a>）。虽然可用设备的列表可能会改变。当前输入设备可能会变得不可用（如果他们被另一个应用程序使用），新的输入设备可能成为可用的，（如果他们被另一个应用程序让出）。应该注册，当可用设备列表改变时接收 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/data/AVCaptureDeviceWasConnectedNotification" target="_blank" rel="external">AVCaptureDeviceWasConnectedNotification</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/data/AVCaptureDeviceWasDisconnectedNotification" target="_blank" rel="external">AVCaptureDeviceWasDisconnectedNotification</a> 通知。</p>
<p>You add an input device to a capture session using a capture input (see Use Capture Inputs to Add a Capture Device to a Session).</p>
<p>使用捕获输入将输入设备添加到捕获会话中（详情请看：<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW6" target="_blank" rel="external">Use Capture Inputs to Add a Capture Device to a Session</a>）</p>
<h3 id="Device-Characteristics-设备特点"><a href="#Device-Characteristics-设备特点" class="headerlink" title="Device Characteristics - 设备特点"></a>Device Characteristics - 设备特点</h3><p>You can ask a device about its different characteristics. You can also test whether it provides a particular media type or supports a given capture session preset using hasMediaType: and supportsAVCaptureSessionPreset: respectively. To provide information to the user, you can find out the position of the capture device (whether it is on the front or the back of the unit being tested), and its localized name. This may be useful if you want to present a list of capture devices to allow the user to choose one.</p>
<p>你可以问一个有关设备的不同特征。你也可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/hasMediaType:" target="_blank" rel="external">hasMediaType:</a> 测试它是否提供了一个特定的媒体类型，或者使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/supportsAVCaptureSessionPreset:" target="_blank" rel="external">supportsAVCaptureSessionPreset:</a> 支持一个给定捕捉会话的预设状态。为了给用户提供信息，可以找到捕捉设备的位置（无论它是在正被测试单元的前面还是后面），以及本地化名称。这是很有用的，如果你想提出一个捕获设备的列表，让用户选择一个。</p>
<p>Figure 4-3 shows the positions of the back-facing (AVCaptureDevicePositionBack) and front-facing (AVCaptureDevicePositionFront) cameras.</p>
<p>图4-3显示了后置摄像头（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureDevicePositionBack" target="_blank" rel="external">AVCaptureDevicePositionBack</a>）和前置摄像头（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureDevicePositionFront" target="_blank" rel="external">AVCaptureDevicePositionFront</a>）的位置。</p>
<blockquote>
<p>Note: Media capture does not support simultaneous capture of both the front-facing and back-facing cameras on iOS devices.</p>
<p>注意：媒体捕获在iOS设备上不支持前置摄像头和后置摄像头同时捕捉。</p>
</blockquote>
<center><br>    <img src="http://ww4.sinaimg.cn/large/a9c4d5f6gw1f6gvmy61otj20ma0msdfz.jpg" alt="Figure 4-3  iOS device front and back facing camera positions"><br></center>

<p>The following code example iterates over all the available devices and logs their name—and for video devices, their position—on the unit.</p>
<p>下面的代码示例遍历了所有可用的设备并且记录了它们的名字，视频设备，在装置上的位置。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSArray</span> *devices = [<span class="built_in">AVCaptureDevice</span> devices];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">AVCaptureDevice</span> *device <span class="keyword">in</span> devices) &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"Device name: %@"</span>, [device localizedName]);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> ([device hasMediaType:<span class="built_in">AVMediaTypeVideo</span>]) &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> ([device position] == <span class="built_in">AVCaptureDevicePositionBack</span>) &#123;</span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"Device position : back"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">NSLog</span>(<span class="string">@"Device position : front"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In addition, you can find out the device’s model ID and its unique ID.</p>
<p>此外，你可以找到该设备的 <code>model ID</code> 和它的 <code>unique ID</code>。</p>
<h3 id="Device-Capture-Settings"><a href="#Device-Capture-Settings" class="headerlink" title="Device Capture Settings"></a>Device Capture Settings</h3><p>Different devices have different capabilities; for example, some may support different focus or flash modes; some may support focus on a point of interest.</p>
<p>例如不同设备有不同的功能，一些可能支持不同的聚焦或者闪光模式；一些可能会支持聚焦在一个兴趣点。</p>
<p>The following code fragment shows how you can find video input devices that have a torch mode and support a given capture session preset:</p>
<p>下面的代码片段展示了如何找到有一个 <code>torch</code> 模式的视频输入设备，并且支持一个捕捉会话预设。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSArray</span> *devices = [<span class="built_in">AVCaptureDevice</span> devicesWithMediaType:<span class="built_in">AVMediaTypeVideo</span>];</span><br><span class="line"><span class="built_in">NSMutableArray</span> *torchDevices = [[<span class="built_in">NSMutableArray</span> alloc] init];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">AVCaptureDevice</span> *device <span class="keyword">in</span> devices) &#123;</span><br><span class="line">    [<span class="keyword">if</span> ([device hasTorch] &amp;&amp;</span><br><span class="line">         [device supports<span class="built_in">AVCaptureSessionPreset</span>:<span class="built_in">AVCaptureSessionPreset640x480</span>]) &#123;</span><br><span class="line">        [torchDevices addObject:device];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>If you find multiple devices that meet your criteria, you might let the user choose which one they want to use. To display a description of a device to the user, you can use its localizedName property.</p>
<p>如果找到多个设备满足标准，你可能会让用户选择一个他们想使用的。给用户显示一个设备的描述，可以使用它的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/localizedName" target="_blank" rel="external">localizedName</a> 属性。</p>
<p>You use the various different features in similar ways. There are constants to specify a particular mode, and you can ask a device whether it supports a particular mode. In several cases, you can observe a property to be notified when a feature is changing. In all cases, you should lock the device before changing the mode of a particular feature, as described in Configuring a Device.</p>
<p>用类似的方法使用各种不同的功能。有常量来指定一个特定的模式，也可以问设备是否支持特定的模式。在一些情况下，当功能改变的时候可以观察到要通知的属性。在所有情况下，你应该改变特定功能的模式之前锁定设备，如在设备配置中描述。</p>
<blockquote>
<p>Note: Focus point of interest and exposure point of interest are mutually exclusive, as are focus mode and exposure mode.</p>
<p>注意：兴趣的焦点和兴趣的曝光点是相互排斥的，因为是聚焦模式和曝光模式。</p>
</blockquote>
<h4 id="Focus-Modes-聚焦模式"><a href="#Focus-Modes-聚焦模式" class="headerlink" title="Focus Modes - 聚焦模式"></a>Focus Modes - 聚焦模式</h4><p>There are three focus modes:</p>
<ul>
<li>AVCaptureFocusModeLocked: The focal position is fixed.<br>This is useful when you want to allow the user to compose a scene then lock the focus.</li>
<li>AVCaptureFocusModeAutoFocus: The camera does a single scan focus then reverts to locked.<br>This is suitable for a situation where you want to select a particular item on which to focus and then maintain focus on that item even if it is not the center of the scene.</li>
<li>AVCaptureFocusModeContinuousAutoFocus: The camera continuously autofocuses as needed.</li>
</ul>
<p>有3个聚焦模式：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFocusModeLocked" target="_blank" rel="external">AVCaptureFocusModeLocked</a> ：焦点的位置是固定的。<br>这是很有用的，当你想让用户组成一个场景，然后锁定焦点。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFocusModeAutoFocus" target="_blank" rel="external">AVCaptureFocusModeAutoFocus</a> ：照相机做一次扫描聚焦，然后将焦点锁定。<br>这适合于，你想要选择一个特定的项目，即使它不是现场的中心，但可以专注于该项目的焦点。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFocusModeContinuousAutoFocus" target="_blank" rel="external">AVCaptureFocusModeContinuousAutoFocus</a> 相机需要不断的自动对焦。</li>
</ul>
<p>You use the isFocusModeSupported: method to determine whether a device supports a given focus mode, then set the mode using the focusMode property.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isFocusModeSupported:" target="_blank" rel="external">isFocusModeSupported:</a> 方法来决定设备是否支持给定的聚焦模式，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/focusMode" target="_blank" rel="external">focusMode</a>  属性设置模式。</p>
<p>In addition, a device may support a focus point of interest. You test for support using focusPointOfInterestSupported. If it’s supported, you set the focal point using focusPointOfInterest. You pass a CGPoint where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right in landscape mode with the home button on the right—this applies even if the device is in portrait mode.</p>
<p>此外，设备可能支持一个兴趣焦点。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/focusPointOfInterestSupported" target="_blank" rel="external">focusPointOfInterestSupported</a> 进行支持测试。如果支持，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/focusPointOfInterest" target="_blank" rel="external">focusPointOfInterest</a> 设置焦点。传一个 <code>CGPoing</code>，横向模式下（就是 <code>home</code> 键在右边）图片的左上角是 <code>{0, 0}</code>，右下角是 <code>{1, 1}</code>， – 即使设备是纵向模式也适用。</p>
<p>You can use the adjustingFocus property to determine whether a device is currently focusing. You can observe the property using key-value observing to be notified when a device starts and stops focusing.</p>
<p>你可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isAdjustingFocus" target="_blank" rel="external">adjustingFocus</a> 属性来确定设备是否正在聚焦。当设备开始、停止聚焦时可以使用 <code>key-value observing</code> 观察，接收通知。</p>
<p>If you change the focus mode settings, you can return them to the default configuration as follows:</p>
<p>如果改变聚焦模式设置，可以将其返回到默认配置，如下所示：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ([currentDevice isFocusModeSupported:<span class="built_in">AVCaptureFocusModeContinuousAutoFocus</span>]) &#123;</span><br><span class="line">    <span class="built_in">CGPoint</span> autofocusPoint = <span class="built_in">CGPointMake</span>(<span class="number">0.5</span>f, <span class="number">0.5</span>f);</span><br><span class="line">    [currentDevice setFocusPointOfInterest:autofocusPoint];</span><br><span class="line">    [currentDevice setFocusMode:<span class="built_in">AVCaptureFocusModeContinuousAutoFocus</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Exposure-Modes-曝光模式"><a href="#Exposure-Modes-曝光模式" class="headerlink" title="Exposure Modes - 曝光模式"></a>Exposure Modes - 曝光模式</h4><p>There are two exposure modes:</p>
<ul>
<li>AVCaptureExposureModeContinuousAutoExposure: The device automatically adjusts the exposure level as needed.</li>
<li>AVCaptureExposureModeLocked: The exposure level is fixed at its current level.</li>
</ul>
<p>You use the isExposureModeSupported: method to determine whether a device supports a given exposure mode, then set the mode using the exposureMode property.</p>
<p>有两种曝光模式：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/Reference/Reference.html#//apple_ref/doc/c_ref/AVCaptureExposureModeContinuousAutoExposure" target="_blank" rel="external">AVCaptureExposureModeContinuousAutoExposure</a> ：设备根据需要自动调整曝光等级。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureExposureModeLocked" target="_blank" rel="external">AVCaptureExposureModeLocked</a> ：曝光等级固定在当前等级。</li>
</ul>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isExposureModeSupported:" target="_blank" rel="external">isExposureModeSupported:</a>  方法来确定设备是否支持给定的曝光模式，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/exposureMode" target="_blank" rel="external">exposureMode</a> 属性设置模式。</p>
<p>In addition, a device may support an exposure point of interest. You test for support using exposurePointOfInterestSupported. If it’s supported, you set the exposure point using exposurePointOfInterest. You pass a CGPoint where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right in landscape mode with the home button on the right—this applies even if the device is in portrait mode.</p>
<p>此外，一个设备支持一个曝光点。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/exposurePointOfInterestSupported" target="_blank" rel="external">exposurePointOfInterestSupported</a> 测试支持。如果支持，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/exposurePointOfInterest" target="_blank" rel="external">exposurePointOfInterest</a> 设置曝光点。传一个 <code>CGPoing</code>，横向模式下（就是 <code>home</code> 键在右边）图片的左上角是 <code>{0, 0}</code>，右下角是 <code>{1, 1}</code>， – 即使设备是纵向模式也适用。</p>
<p>You can use the adjustingExposure property to determine whether a device is currently changing its exposure setting. You can observe the property using key-value observing to be notified when a device starts and stops changing its exposure setting.</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isAdjustingExposure" target="_blank" rel="external">adjustingExposure</a> 属性来确定设备当前是否改变它的聚焦设置。当设备开始、停止聚焦时可以使用 <code>key-value observing</code> 观察，接收通知。</p>
<p>If you change the exposure settings, you can return them to the default configuration as follows:</p>
<p>如果改变曝光设置，可以将其返回到默认配置，如下所示：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ([currentDevice isExposureModeSupported:<span class="built_in">AVCaptureExposureModeContinuousAutoExposure</span>]) &#123;</span><br><span class="line">    <span class="built_in">CGPoint</span> exposurePoint = <span class="built_in">CGPointMake</span>(<span class="number">0.5</span>f, <span class="number">0.5</span>f);</span><br><span class="line">    [currentDevice setExposurePointOfInterest:exposurePoint];</span><br><span class="line">    [currentDevice setExposureMode:<span class="built_in">AVCaptureExposureModeContinuousAutoExposure</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Flash-Modes-闪光模式"><a href="#Flash-Modes-闪光模式" class="headerlink" title="Flash Modes - 闪光模式"></a>Flash Modes - 闪光模式</h4><p>There are three flash modes:</p>
<ul>
<li>AVCaptureFlashModeOff: The flash will never fire.</li>
<li>AVCaptureFlashModeOn: The flash will always fire.</li>
<li>AVCaptureFlashModeAuto: The flash will fire dependent on the ambient light conditions.</li>
</ul>
<p>You use hasFlash to determine whether a device has a flash. If that method returns YES, you then use the isFlashModeSupported: method, passing the desired mode to determine whether a device supports a given flash mode, then set the mode using the flashMode property.</p>
<p>有3种闪光模式：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFlashModeOff" target="_blank" rel="external">AVCaptureFlashModeOff</a> ：闪光灯不会闪。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFlashModeOn" target="_blank" rel="external">AVCaptureFlashModeOn</a> ：闪光灯总是会闪。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureFlashModeAuto" target="_blank" rel="external">AVCaptureFlashModeAuto</a> ：闪光灯取决去周围的光感环境。</li>
</ul>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/hasFlash" target="_blank" rel="external">hasFlash</a> 来确定设备是否有闪光灯。如果这个方法返回 <code>YES</code> ，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isFlashModeSupported:" target="_blank" rel="external">isFlashModeSupported:</a> 方法确定设备是否支持给定的闪光模式，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/flashMode" target="_blank" rel="external">flashMode</a>  属性设置模式。</p>
<h4 id="Torch-Mode-手电筒模式"><a href="#Torch-Mode-手电筒模式" class="headerlink" title="Torch Mode - 手电筒模式"></a>Torch Mode - 手电筒模式</h4><p>In torch mode, the flash is continuously enabled at a low power to illuminate a video capture. There are three torch modes:</p>
<ul>
<li>AVCaptureTorchModeOff: The torch is always off.</li>
<li>AVCaptureTorchModeOn: The torch is always on.</li>
<li>AVCaptureTorchModeAuto: The torch is automatically switched on and off as needed.</li>
</ul>
<p>You use hasTorch to determine whether a device has a flash. You use the isTorchModeSupported: method to determine whether a device supports a given flash mode, then set the mode using the torchMode property.</p>
<p>For devices with a torch, the torch only turns on if the device is associated with a running capture session.</p>
<p>在手电筒模式下，闪光灯在一个低功率下一直开启，以照亮对视频捕获。有3个手电筒模式：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureTorchModeOff" target="_blank" rel="external">AVCaptureTorchModeOff</a> ：总是关闭。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureTorchModeOn" target="_blank" rel="external">AVCaptureTorchModeOn</a> ：总是打开。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureTorchModeAuto" target="_blank" rel="external">AVCaptureTorchModeAuto</a> ：闪光灯根据需要自动开关。</li>
</ul>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/hasTorch" target="_blank" rel="external">hasTorch</a> 来确定设备是否有闪光灯。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isTorchModeSupported:" target="_blank" rel="external">isTorchModeSupported:</a> 方法来确定设备是否支持给定的闪光模式，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/torchMode" target="_blank" rel="external">torchMode</a> 属性来设置模式。</p>
<p>对于一个有手电筒的设备，只有当该设备与一个运行时捕捉会话关联时，才能打开手电筒。</p>
<h4 id="Video-Stabilization-视频稳定性"><a href="#Video-Stabilization-视频稳定性" class="headerlink" title="Video Stabilization - 视频稳定性"></a>Video Stabilization - 视频稳定性</h4><p>Cinematic video stabilization is available for connections that operate on video, depending on the specific device hardware. Even so, not all source formats and video resolutions are supported.</p>
<p>Enabling cinematic video stabilization may also introduce additional latency into the video capture pipeline. To detect when video stabilization is in use, use the videoStabilizationEnabled property. The enablesVideoStabilizationWhenAvailable property allows an application to automatically enable video stabilization if it is supported by the camera. By default automatic stabilization is disabled due to the above limitations.</p>
<p>电影视频的稳定化可用于连接视频上的操作，这取决于具体的硬件。尽管如此，不是所有的源格式和视频分辨率都被支持。</p>
<p>使用电影视频稳定化也可能会对视频采集管道引起额外的延迟。正在使用视频稳定化时，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/instp/AVCaptureConnection/videoStabilizationEnabled" target="_blank" rel="external">videoStabilizationEnabled</a> 属性可以检测。<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/instp/AVCaptureConnection/enablesVideoStabilizationWhenAvailable" target="_blank" rel="external">enablesVideoStabilizationWhenAvailable</a> 属性允许应用程序自动使视频稳定化可用，如果它是被摄像头支持的话。由于以上限制，默认自动稳定化是禁用的。</p>
<h4 id="White-Balance-白平衡"><a href="#White-Balance-白平衡" class="headerlink" title="White Balance - 白平衡"></a>White Balance - 白平衡</h4><p>There are two white balance modes:</p>
<ul>
<li>AVCaptureWhiteBalanceModeLocked: The white balance mode is fixed.</li>
<li>AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance: The camera continuously adjusts the white balance as needed.</li>
</ul>
<p>You use the isWhiteBalanceModeSupported: method to determine whether a device supports a given white balance mode, then set the mode using the whiteBalanceMode property.</p>
<p>You can use the adjustingWhiteBalance property to determine whether a device is currently changing its white balance setting. You can observe the property using key-value observing to be notified when a device starts and stops changing its white balance setting.</p>
<p>有两个白平衡模式：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureWhiteBalanceModeLocked" target="_blank" rel="external">AVCaptureWhiteBalanceModeLocked</a> ：白平衡模式是固定的。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/c/econst/AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance" target="_blank" rel="external">AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance</a> ：相机需要不断调整白平衡。</li>
</ul>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/isWhiteBalanceModeSupported:" target="_blank" rel="external">isWhiteBalanceModeSupported:</a> ：方法来确定设备是否支持给定的白平衡模式，然后使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/whiteBalanceMode" target="_blank" rel="external">whiteBalanceMode</a> 属性设置模式。</p>
<p>可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instp/AVCaptureDevice/adjustingWhiteBalance" target="_blank" rel="external">adjustingWhiteBalance</a> 属性来确定设备是否正在改变白平衡设置。当设备开始或者停止改变它的白平衡设置时，可以使用 <code>key-value observing</code> 观察属性，接收通知。</p>
<h4 id="Setting-Device-Orientation-设置设备方向"><a href="#Setting-Device-Orientation-设置设备方向" class="headerlink" title="Setting Device Orientation - 设置设备方向"></a>Setting Device Orientation - 设置设备方向</h4><p>You set the desired orientation on a AVCaptureConnection to specify how you want the images oriented in the AVCaptureOutput (AVCaptureMovieFileOutput, AVCaptureStillImageOutput and AVCaptureVideoDataOutput) for the connection.</p>
<p>Use the AVCaptureConnectionsupportsVideoOrientation property to determine whether the device supports changing the orientation of the video, and the videoOrientation property to specify how you want the images oriented in the output port. Listing 4-1 shows how to set the orientation for a AVCaptureConnection to AVCaptureVideoOrientationLandscapeLeft:</p>
<p>在 <code>AVCaptureConnection</code> 设置期望的方向，来指定你想要的图像在 <code>AVCaptureOutput</code> （<code>AVCaptureMovieFileOutput</code>， <code>AVCaptureStillImageOutput</code>, <code>AVCaptureVideoDataOutput</code>）中的方向，为了连接。</p>
<p>使用 <code>AVCaptureConnectionsupportsVideoOrientation</code> 属性来确定设备是否支持改变视频的方向，<code>videoOrientation</code> 属性指定你想要的图像在输出端口的方向。列表4-1显示了如何设置方向，为 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/cl/AVCaptureConnection" target="_blank" rel="external">AVCaptureConnection</a> 设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/c/econst/AVCaptureVideoOrientationLandscapeLeft" target="_blank" rel="external">AVCaptureVideoOrientationLandscapeLeft</a> 。</p>
<p>Listing 4-1  Setting the orientation of a capture connection</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureConnection</span> *captureConnection = &lt;<span class="meta">#A capture connection#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> ([captureConnection isVideoOrientationSupported])</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">AVCaptureVideoOrientation</span> orientation = <span class="built_in">AVCaptureVideoOrientationLandscapeLeft</span>;</span><br><span class="line">    [captureConnection setVideoOrientation:orientation];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Configuring-a-Device-配置设备"><a href="#Configuring-a-Device-配置设备" class="headerlink" title="Configuring a Device - 配置设备"></a>Configuring a Device - 配置设备</h3><p>To set capture properties on a device, you must first acquire a lock on the device using lockForConfiguration:. This avoids making changes that may be incompatible with settings in other applications. The following code fragment illustrates how to approach changing the focus mode on a device by first determining whether the mode is supported, then attempting to lock the device for reconfiguration. The focus mode is changed only if the lock is obtained, and the lock is released immediately afterward.</p>
<p>在设备上设置捕获属性，必须先使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/instm/AVCaptureDevice/lockForConfiguration:" target="_blank" rel="external">lockForConfiguration:</a> 获得设备锁。这样就避免了在其他应用程序中可能与设置不兼容的更改。下面的代码段演示了首先如何通过确定模式是否被支持的方式改变一个设备上的焦点模式，然后视图锁定设备重新配置。只有当锁被获取到，焦点模式才会被改变，并且锁被释放后立即锁定。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ([device isFocusModeSupported:<span class="built_in">AVCaptureFocusModeLocked</span>]) &#123;</span><br><span class="line">    <span class="built_in">NSError</span> *error = <span class="literal">nil</span>;</span><br><span class="line">    <span class="keyword">if</span> ([device lockForConfiguration:&amp;error]) &#123;</span><br><span class="line">        device.focusMode = <span class="built_in">AVCaptureFocusModeLocked</span>;</span><br><span class="line">        [device unlockForConfiguration];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Respond to the failure as appropriate.</span></span><br></pre></td></tr></table></figure>
<p>You should hold the device lock only if you need the settable device properties to remain unchanged. Holding the device lock unnecessarily may degrade capture quality in other applications sharing the device.</p>
<p>只有在需要设置设备属性保持不变的时候才应该使设备锁保持。没必要的保持设备所，可能会在其他应用程序共享设备时降低捕获质量。</p>
<h3 id="Switching-Between-Devices-切换装置"><a href="#Switching-Between-Devices-切换装置" class="headerlink" title="Switching Between Devices - 切换装置"></a>Switching Between Devices - 切换装置</h3><p>Sometimes you may want to allow users to switch between input devices—for example, switching from using the front-facing to to the back-facing camera. To avoid pauses or stuttering, you can reconfigure a session while it is running, however you should use beginConfiguration and commitConfiguration to bracket your configuration changes:</p>
<p>有时，你可能想允许用户在输入设备之间进行切换，比如使用前置摄像头到后置摄像头的切换。为了避免暂停或者卡顿，可以在运行时配置一个会话，但是你应该使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/beginConfiguration" target="_blank" rel="external">beginConfiguration</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/commitConfiguration" target="_blank" rel="external">commitConfiguration</a> 支持你的配置改变：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *session = &lt;<span class="meta">#A capture session#&gt;;</span></span><br><span class="line">[session beginConfiguration];</span><br><span class="line"> </span><br><span class="line">[session removeInput:frontFacingCameraDeviceInput];</span><br><span class="line">[session addInput:backFacingCameraDeviceInput];</span><br><span class="line"> </span><br><span class="line">[session commitConfiguration];</span><br></pre></td></tr></table></figure>
<p>When the outermost commitConfiguration is invoked, all the changes are made together. This ensures a smooth transition.</p>
<p>当最外面的 <code>commitConfiguration</code> 被调用，所有的改变都是一起做的。这保证了平稳过渡。</p>
<h2 id="Use-Capture-Inputs-to-Add-a-Capture-Device-to-a-Session-使用捕获输入将捕获设备添加到会话中"><a href="#Use-Capture-Inputs-to-Add-a-Capture-Device-to-a-Session-使用捕获输入将捕获设备添加到会话中" class="headerlink" title="Use Capture Inputs to Add a Capture Device to a Session - 使用捕获输入将捕获设备添加到会话中"></a>Use Capture Inputs to Add a Capture Device to a Session - 使用捕获输入将捕获设备添加到会话中</h2><p>To add a capture device to a capture session, you use an instance of AVCaptureDeviceInput (a concrete subclass of the abstract AVCaptureInput class). The capture device input manages the device’s ports.</p>
<p>添加一个捕获装置到捕获会话中，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDeviceInput_Class/index.html#//apple_ref/occ/cl/AVCaptureDeviceInput" target="_blank" rel="external">AVCaptureDeviceInput</a> (<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInput_Class/index.html#//apple_ref/occ/cl/AVCaptureInput" target="_blank" rel="external">AVCaptureInput</a> 抽象类的具体子类)的实例。捕获设备输入管理设备的端口。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSError</span> *error;</span><br><span class="line"><span class="built_in">AVCaptureDeviceInput</span> *input =</span><br><span class="line">        [<span class="built_in">AVCaptureDeviceInput</span> deviceInputWithDevice:device error:&amp;error];</span><br><span class="line"><span class="keyword">if</span> (!input) &#123;</span><br><span class="line">    <span class="comment">// Handle the error appropriately.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You add inputs to a session using addInput:. If appropriate, you can check whether a capture input is compatible with an existing session using canAddInput:.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/addInput:" target="_blank" rel="external">addInput:</a> 给会话添加一个输入。如果合适的话，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/canAddInput:" target="_blank" rel="external">canAddInput:</a> 检查是否有输入捕获与现有会话是兼容的。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *captureSession = &lt;<span class="meta">#Get a capture session#&gt;;</span></span><br><span class="line"><span class="built_in">AVCaptureDeviceInput</span> *captureDeviceInput = &lt;<span class="meta">#Get a capture device input#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> ([captureSession canAddInput:captureDeviceInput]) &#123;</span><br><span class="line">    [captureSession addInput:captureDeviceInput];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Handle the failure.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>See Configuring a Session for more details on how you might reconfigure a running session.</p>
<p>An AVCaptureInput vends one or more streams of media data. For example, input devices can provide both audio and video data. Each media stream provided by an input is represented by an AVCaptureInputPort object. A capture session uses an AVCaptureConnection object to define the mapping between a set of AVCaptureInputPort objects and a single AVCaptureOutput.</p>
<p>有关如果配置一个正在运行的会话，更多细节请查看 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW16" target="_blank" rel="external">Configuring a Session</a> .</p>
<p><code>AVCaptureInput</code> 声明一个或者多个媒体数据流。例如，输入设备可以提供音频和视频数据。输入提供的每个媒体流都被一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInputPort_Class/index.html#//apple_ref/occ/cl/AVCaptureInputPort" target="_blank" rel="external">AVCaptureInputPort</a> 所表示。一个捕获会话使用 <code>AVCaptureConnection</code> 对象来定义一个 一组 <code>AVCaptureInputPort</code> 对象和一个 <code>AVCaptureOutput</code> 之间的映射。</p>
<h2 id="Use-Capture-Outputs-to-Get-Output-from-a-Session-使用捕获输出从会话得到输出"><a href="#Use-Capture-Outputs-to-Get-Output-from-a-Session-使用捕获输出从会话得到输出" class="headerlink" title="Use Capture Outputs to Get Output from a Session - 使用捕获输出从会话得到输出"></a>Use Capture Outputs to Get Output from a Session - 使用捕获输出从会话得到输出</h2><p>To get output from a capture session, you add one or more outputs. An output is an instance of a concrete subclass of AVCaptureOutput. You use:</p>
<ul>
<li>AVCaptureMovieFileOutput to output to a movie file</li>
<li>AVCaptureVideoDataOutput if you want to process frames from the video being captured, for example, - to create your own custom view layer</li>
<li>AVCaptureAudioDataOutput if you want to process the audio data being captured</li>
<li>AVCaptureStillImageOutput if you want to capture still images with accompanying metadata</li>
</ul>
<p>You add outputs to a capture session using addOutput:. You check whether a capture output is compatible with an existing session using canAddOutput:. You can add and remove outputs as required while the session is running.</p>
<p>要从捕获会话得到输出，可以添加一个或多个输出。一个输出是 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureOutput" target="_blank" rel="external">AVCaptureOutput</a> 的具体子类的实例。下面几种使用：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput" target="_blank" rel="external">AVCaptureMovieFileOutput</a> 输出电影文件</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 如果你想处理被捕获视频的帧，例如，创建自己的自定义 <code>view layer</code>。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureAudioDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureAudioDataOutput" target="_blank" rel="external">AVCaptureAudioDataOutput</a> 如果你想处理被捕获的音频数据。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureStillImageOutput" target="_blank" rel="external">AVCaptureStillImageOutput</a> 如果你想捕获有元数据的静态图像。</li>
</ul>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/addOutput:" target="_blank" rel="external">addOutput:</a> 把输出添加到捕获会话中。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/canAddOutput:" target="_blank" rel="external">canAddOutput:</a> 检查是否一个捕获输出与现有的会话是兼容的。可以在会话正在运行的时候添加和删除所需的输出。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *captureSession = &lt;<span class="meta">#Get a capture session#&gt;;</span></span><br><span class="line"><span class="built_in">AVCaptureMovieFileOutput</span> *movieOutput = &lt;<span class="meta">#Create and configure a movie output#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> ([captureSession canAddOutput:movieOutput]) &#123;</span><br><span class="line">    [captureSession addOutput:movieOutput];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Handle the failure.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Saving-to-a-Movie-File-保存电影文件"><a href="#Saving-to-a-Movie-File-保存电影文件" class="headerlink" title="Saving to a Movie File - 保存电影文件"></a>Saving to a Movie File - 保存电影文件</h3><p>You save movie data to a file using an AVCaptureMovieFileOutput object. (AVCaptureMovieFileOutput is a concrete subclass of AVCaptureFileOutput, which defines much of the basic behavior.) You can configure various aspects of the movie file output, such as the maximum duration of a recording, or its maximum file size. You can also prohibit recording if there is less than a given amount of disk space left.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput" target="_blank" rel="external">AVCaptureMovieFileOutput</a> 对象保存电影数据到文件中。（<code>AVCaptureMovieFileOutput</code> 是 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureFileOutput" target="_blank" rel="external">AVCaptureFileOutput</a> 的具体子类，定义了大量的基本行为。）可以电影文件输出的各个方面，如记录的最大时间，或它的最大文件的大小。也可以禁止记录，如果有小于给定磁盘空间的数量。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureMovieFileOutput</span> *aMovieFileOutput = [[<span class="built_in">AVCaptureMovieFileOutput</span> alloc] init];</span><br><span class="line">CMTime maxDuration = &lt;<span class="meta">#Create a CMTime to represent the maximum duration#&gt;;</span></span><br><span class="line">aMovieFileOutput.maxRecordedDuration = maxDuration;</span><br><span class="line">aMovieFileOutput.minFreeDiskSpaceLimit = &lt;<span class="meta">#An appropriate minimum given the quality of the movie format and the duration#&gt;;</span></span><br></pre></td></tr></table></figure>
<p>The resolution and bit rate for the output depend on the capture session’s sessionPreset. The video encoding is typically H.264 and audio encoding is typically AAC. The actual values vary by device.</p>
<p>输出的分辨率和比特率取决于捕获会话的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/sessionPreset" target="_blank" rel="external">sessionPreset</a> 。视频编码通常是 <code>H.264</code> ，音频编码通常是 <code>AAC</code> 。实际值因设备而异。</p>
<h4 id="Starting-a-Recording-开始记录"><a href="#Starting-a-Recording-开始记录" class="headerlink" title="Starting a Recording - 开始记录"></a>Starting a Recording - 开始记录</h4><p>You start recording a QuickTime movie using startRecordingToOutputFileURL:recordingDelegate:. You need to supply a file-based URL and a delegate. The URL must not identify an existing file, because the movie file output does not overwrite existing resources. You must also have permission to write to the specified location. The delegate must conform to the AVCaptureFileOutputRecordingDelegate protocol, and must implement the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: method.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureFileOutput/startRecordingToOutputFileURL:recordingDelegate:" target="_blank" rel="external">startRecordingToOutputFileURL:recordingDelegate:</a> 开始记录一个 <code>QuickTime</code> 电影。需要提供一个基于 <code>URL</code> 和 <code>delegate</code> 的文件。<code>URL</code> 决不能指向一个已经存在的文件，因为电影文件输出不会覆盖存在的资源。你还必须有权限能写入指定的位置。 <code>delegate</code> 必须符合 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/index.html#//apple_ref/occ/intf/AVCaptureFileOutputRecordingDelegate" target="_blank" rel="external">AVCaptureFileOutputRecordingDelegate</a> 协议，并且必须实现 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureFileOutputRecordingDelegate/captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:" target="_blank" rel="external">captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</a> 方法。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureMovieFileOutput</span> *aMovieFileOutput = &lt;<span class="meta">#Get a movie file output#&gt;;</span></span><br><span class="line"><span class="built_in">NSURL</span> *fileURL = &lt;<span class="meta">#A file URL that identifies the output location#&gt;;</span></span><br><span class="line">[aMovieFileOutput startRecordingToOutputFileURL:fileURL recordingDelegate:&lt;<span class="meta">#The delegate#&gt;];</span></span><br></pre></td></tr></table></figure>
<p>In the implementation of captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:, the delegate might write the resulting movie to the Camera Roll album. It should also check for any errors that might have occurred.</p>
<p>在 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureFileOutputRecordingDelegate/captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:" target="_blank" rel="external">captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</a> 的实现中，代理可以将结果电影写入到相机胶卷专辑中。它也应该可能发生的任何错误。</p>
<h4 id="Ensuring-That-the-File-Was-Written-Successfully-确保文件被成功写入"><a href="#Ensuring-That-the-File-Was-Written-Successfully-确保文件被成功写入" class="headerlink" title="Ensuring That the File Was Written Successfully - 确保文件被成功写入"></a>Ensuring That the File Was Written Successfully - 确保文件被成功写入</h4><p>To determine whether the file was saved successfully, in the implementation of captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: you check not only the error but also the value of the AVErrorRecordingSuccessfullyFinishedKey in the error’s user info dictionary:</p>
<p>为了确定文件是否成功被写入，在 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureFileOutputRecordingDelegate/captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:" target="_blank" rel="external">captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</a> 的实现中，不仅要检查错误，还要在错误的用户信息字典中，检查 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/data/AVErrorRecordingSuccessfullyFinishedKey" target="_blank" rel="external">AVErrorRecordingSuccessfullyFinishedKey</a> 的值。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)captureOutput:(<span class="built_in">AVCaptureFileOutput</span> *)captureOutput</span><br><span class="line">        didFinishRecordingToOutputFileAtURL:(<span class="built_in">NSURL</span> *)outputFileURL</span><br><span class="line">        fromConnections:(<span class="built_in">NSArray</span> *)connections</span><br><span class="line">        error:(<span class="built_in">NSError</span> *)error &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">BOOL</span> recordedSuccessfully = <span class="literal">YES</span>;</span><br><span class="line">    <span class="keyword">if</span> ([error code] != noErr) &#123;</span><br><span class="line">        <span class="comment">// A problem occurred: Find out if the recording was successful.</span></span><br><span class="line">        <span class="keyword">id</span> value = [[error userInfo] objectForKey:<span class="built_in">AVErrorRecordingSuccessfullyFinishedKey</span>];</span><br><span class="line">        <span class="keyword">if</span> (value) &#123;</span><br><span class="line">            recordedSuccessfully = [value boolValue];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Continue as appropriate...</span></span><br></pre></td></tr></table></figure>
<p>You should check the value of the AVErrorRecordingSuccessfullyFinishedKeykey in the user info dictionary of the error, because the file might have been saved successfully, even though you got an error. The error might indicate that one of your recording constraints was reached—for example, AVErrorMaximumDurationReached or AVErrorMaximumFileSizeReached. Other reasons the recording might stop are:</p>
<p>The disk is full—AVErrorDiskFull<br>The recording device was disconnected—AVErrorDeviceWasDisconnected<br>The session was interrupted (for example, a phone call was received)—AVErrorSessionWasInterrupted</p>
<p>应该在用户的错误信息字典中检查 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/data/AVErrorRecordingSuccessfullyFinishedKey" target="_blank" rel="external">AVErrorRecordingSuccessfullyFinishedKeykey</a> 的值，因为即使得到了一个错误信息，文件可能已经被成功保存了。这种错误可能表明你的一个记录约束被延迟了，例如 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/econst/AVErrorMaximumDurationReached" target="_blank" rel="external">AVErrorMaximumDurationReached</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/econst/AVErrorMaximumFileSizeReached" target="_blank" rel="external">AVErrorMaximumFileSizeReached</a> 。记录可能停止的其他原因是：</p>
<ul>
<li>磁盘已满 – <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/econst/AVErrorDiskFull" target="_blank" rel="external">AVErrorDiskFull</a></li>
<li>记录设备被断开连接 – <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/econst/AVErrorDeviceWasDisconnected" target="_blank" rel="external">AVErrorDeviceWasDisconnected</a></li>
<li>会话被中断（例如，接收到一个电话） – <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_ErrorConstants/index.html#//apple_ref/c/econst/AVErrorSessionWasInterrupted" target="_blank" rel="external">AVErrorSessionWasInterrupted</a></li>
</ul>
<h4 id="Adding-Metadata-to-a-File-将元数据添加到文件中"><a href="#Adding-Metadata-to-a-File-将元数据添加到文件中" class="headerlink" title="Adding Metadata to a File - 将元数据添加到文件中"></a>Adding Metadata to a File - 将元数据添加到文件中</h4><p>You can set metadata for the movie file at any time, even while recording. This is useful for situations where the information is not available when the recording starts, as may be the case with location information. Metadata for a file output is represented by an array of AVMetadataItem objects; you use an instance of its mutable subclass, AVMutableMetadataItem, to create metadata of your own.</p>
<p>可以在任何时间设置电影文件的元数据，即使在记录的时候。这是有用的，当记录开始，信息室不可用的，因为可能是位置信息的情况下。一个输出文件的元数据是由 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMetadataItem_Class/index.html#//apple_ref/occ/cl/AVMetadataItem" target="_blank" rel="external">AVMetadataItem</a> 对象的数组表示；使用其可变子类(<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableMetadataItem_Class/index.html#//apple_ref/occ/cl/AVMutableMetadataItem" target="_blank" rel="external">AVMutableMetadataItem</a>)的实例，去创建属于你自己的元数据。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureMovieFileOutput</span> *aMovieFileOutput = &lt;<span class="meta">#Get a movie file output#&gt;;</span></span><br><span class="line"><span class="built_in">NSArray</span> *existingMetadataArray = aMovieFileOutput.metadata;</span><br><span class="line"><span class="built_in">NSMutableArray</span> *newMetadataArray = <span class="literal">nil</span>;</span><br><span class="line"><span class="keyword">if</span> (existingMetadataArray) &#123;</span><br><span class="line">    newMetadataArray = [existingMetadataArray mutableCopy];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    newMetadataArray = [[<span class="built_in">NSMutableArray</span> alloc] init];</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="built_in">AVMutableMetadataItem</span> *item = [[<span class="built_in">AVMutableMetadataItem</span> alloc] init];</span><br><span class="line">item.keySpace = <span class="built_in">AVMetadataKeySpaceCommon</span>;</span><br><span class="line">item.key = <span class="built_in">AVMetadataCommonKeyLocation</span>;</span><br><span class="line"> </span><br><span class="line">CLLocation *location - &lt;<span class="meta">#The location to set#&gt;;</span></span><br><span class="line">item.value = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@"%+08.4lf%+09.4lf/"</span></span><br><span class="line">    location.coordinate.latitude, location.coordinate.longitude];</span><br><span class="line"> </span><br><span class="line">[newMetadataArray addObject:item];</span><br><span class="line"> </span><br><span class="line">aMovieFileOutput.metadata = newMetadataArray;</span><br></pre></td></tr></table></figure>
<h4 id="Processing-Frames-of-Video-处理视频的帧"><a href="#Processing-Frames-of-Video-处理视频的帧" class="headerlink" title="Processing Frames of Video - 处理视频的帧"></a>Processing Frames of Video - 处理视频的帧</h4><p>An AVCaptureVideoDataOutput object uses delegation to vend video frames. You set the delegate using setSampleBufferDelegate:queue:. In addition to setting the delegate, you specify a serial queue on which they delegate methods are invoked. You must use a serial queue to ensure that frames are delivered to the delegate in the proper order. You can use the queue to modify the priority given to delivering and processing the video frames. See SquareCam for a sample implementation.</p>
<p>一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 对象使用委托来声明视频帧。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/setSampleBufferDelegate:queue:" target="_blank" rel="external">setSampleBufferDelegate:queue:</a> 设置代理。除了设置代理，还要制定一个调用它们代理方法的串行队列。必须使用一个串行队列以确保帧以适当的顺序传递给代理。可以使用队列来修改给定传输的优先级和处理视频帧的优先级。查看 <a href="https://developer.apple.com/library/ios/samplecode/SquareCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40011190" target="_blank" rel="external">SquareCam</a> 有一个简单的实现。</p>
<p>The frames are presented in the delegate method, captureOutput:didOutputSampleBuffer:fromConnection:, as instances of the CMSampleBufferRef opaque type (see Representations of Media). By default, the buffers are emitted in the camera’s most efficient format. You can use the videoSettings property to specify a custom output format. The video settings property is a dictionary; currently, the only supported key is kCVPixelBufferPixelFormatTypeKey. The recommended pixel formats are returned by the availableVideoCVPixelFormatTypes property , and the availableVideoCodecTypes property returns the supported values. Both Core Graphics and OpenGL work well with the BGRA format:</p>
<p>在代理方法中（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:" target="_blank" rel="external">captureOutput:didOutputSampleBuffer:fromConnection:</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/tdef/CMSampleBufferRef" target="_blank" rel="external">CMSampleBufferRef</a> 不透明类型的实例，详情见 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW16" target="_blank" rel="external">Representations of Media</a>），帧是被露出来的。默认情况下，被放出的缓冲区是相机最有效的格式。可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/videoSettings" target="_blank" rel="external">videoSettings</a> 属性指定自定义输出格式。视频设置属性是一个字典；目前，唯一支持的 <code>key</code> 是 <a href="https://developer.apple.com/library/ios/documentation/QuartzCore/Reference/CVPixelBufferRef/index.html#//apple_ref/c/data/kCVPixelBufferPixelFormatTypeKey" target="_blank" rel="external">kCVPixelBufferPixelFormatTypeKey</a>。推荐的像素格式是由 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoDataOutput/availableVideoCVPixelFormatTypes" target="_blank" rel="external">availableVideoCVPixelFormatTypes</a> 属性返回的，并且 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoDataOutput/availableVideoCodecTypes" target="_blank" rel="external">availableVideoCodecTypes</a> 属性返回支持的值。<code>Core Graphics</code> 和 <code>OpenGL</code> 都很好的使用 <code>BGRA</code> 格式：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureVideoDataOutput</span> *videoDataOutput = [<span class="built_in">AVCaptureVideoDataOutput</span> new];</span><br><span class="line"><span class="built_in">NSDictionary</span> *newSettings =</span><br><span class="line">                @&#123; (<span class="built_in">NSString</span> *)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) &#125;;</span><br><span class="line">videoDataOutput.videoSettings = newSettings;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">// discard if the data output queue is blocked (as we process the still image</span></span><br><span class="line">[videoDataOutput setAlwaysDiscardsLateVideoFrames:<span class="literal">YES</span>];)</span><br><span class="line"> </span><br><span class="line"><span class="comment">// create a serial dispatch queue used for the sample buffer delegate as well as when a still image is captured</span></span><br><span class="line"><span class="comment">// a serial dispatch queue must be used to guarantee that video frames will be delivered in order</span></span><br><span class="line"><span class="comment">// see the header doc for setSampleBufferDelegate:queue: for more information</span></span><br><span class="line">videoDataOutputQueue = dispatch_queue_create(<span class="string">"VideoDataOutputQueue"</span>, DISPATCH_QUEUE_SERIAL);</span><br><span class="line">[videoDataOutput setSampleBufferDelegate:<span class="keyword">self</span> queue:videoDataOutputQueue];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">AVCaptureSession</span> *captureSession = &lt;<span class="meta">#The Capture Session#&gt;;</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> ( [captureSession canAddOutput:videoDataOutput] )</span><br><span class="line">     [captureSession addOutput:videoDataOutput];</span><br></pre></td></tr></table></figure>
<h4 id="Performance-Considerations-for-Processing-Video-处理视频的性能考虑"><a href="#Performance-Considerations-for-Processing-Video-处理视频的性能考虑" class="headerlink" title="Performance Considerations for Processing Video - 处理视频的性能考虑"></a>Performance Considerations for Processing Video - 处理视频的性能考虑</h4><p>You should set the session output to the lowest practical resolution for your application. Setting the output to a higher resolution than necessary wastes processing cycles and needlessly consumes power.</p>
<p>应该将会话输出设置为应用程序的最低分辨率。设置输出超过必要废物处理周期，达到更高的分辨率，从而不必要消耗功率。</p>
<p>You must ensure that your implementation of captureOutput:didOutputSampleBuffer:fromConnection: is able to process a sample buffer within the amount of time allotted to a frame. If it takes too long and you hold onto the video frames, AV Foundation stops delivering frames, not only to your delegate but also to other outputs such as a preview layer.</p>
<p>必须确保 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:" target="_blank" rel="external">captureOutput:didOutputSampleBuffer:fromConnection:</a> 的实现，能够处理大量时间内的样品缓冲，分配到一个帧中。如果它需要很久，你要一直抓住视频帧，<code>AV Foundation</code> 会停止给，你的代理，还有其他输出例如 <code>preview layer</code> ，提供帧。</p>
<p>You can use the capture video data output’s minFrameDuration property to be sure you have enough time to process a frame — at the cost of having a lower frame rate than would otherwise be the case. You might also make sure that the alwaysDiscardsLateVideoFrames property is set to YES (the default). This ensures that any late video frames are dropped rather than handed to you for processing. Alternatively, if you are recording and it doesn’t matter if the output fames are a little late and you would prefer to get all of them, you can set the property value to NO. This does not mean that frames will not be dropped (that is, frames may still be dropped), but that they may not be dropped as early, or as efficiently.</p>
<p>可以使用捕获视频数据输出的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoDataOutput/minFrameDuration" target="_blank" rel="external">minFrameDuration</a> 属性来确保你有足够时间来处理帧 – 在具有较低的帧速率比其他情况下的成本。也可以确保 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/alwaysDiscardsLateVideoFrameshttps://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/alwaysDiscardsLateVideoFrames" target="_blank" rel="external">alwaysDiscardsLateVideoFrames</a> 属性被设为 <code>YES</code> （默认）。这确保任何后期视频的帧都被丢弃，而不是交给你处理。或者，如果你是记录，更想得到它们全部，不介意输出帧稍微晚一点的话，可以设置该属性的值为 <code>NO</code> 。这并不意味着不会丢失帧（即，帧仍有可能丢失），但它们不可能像之前那样减少，或者说是有点效果的。</p>
<h3 id="Capturing-Still-Images-捕获静止图像"><a href="#Capturing-Still-Images-捕获静止图像" class="headerlink" title="Capturing Still Images - 捕获静止图像"></a>Capturing Still Images - 捕获静止图像</h3><p>You use an AVCaptureStillImageOutput output if you want to capture still images with accompanying metadata. The resolution of the image depends on the preset for the session, as well as the device.</p>
<p>如果你想捕获带着元数据的静止图像，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureStillImageOutput" target="_blank" rel="external">AVCaptureStillImageOutput</a> 输出。图像的分辨率取决于会话的预设，以及设备的设置。</p>
<h4 id="Pixel-and-Encoding-Formats-像素和编码格式"><a href="#Pixel-and-Encoding-Formats-像素和编码格式" class="headerlink" title="Pixel and Encoding Formats - 像素和编码格式"></a>Pixel and Encoding Formats - 像素和编码格式</h4><p>Different devices support different image formats. You can find out what pixel and codec types are supported by a device using availableImageDataCVPixelFormatTypes and availableImageDataCodecTypes respectively. Each method returns an array of the supported values for the specific device. You set the outputSettings dictionary to specify the image format you want, for example:</p>
<p>不同的设备支持不同的图像格式。使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureStillImageOutput/availableImageDataCVPixelFormatTypes" target="_blank" rel="external">availableImageDataCVPixelFormatTypes</a> 可以找到什么样的像素被支持，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureStillImageOutput/availableImageDataCodecTypes" target="_blank" rel="external">availableImageDataCodecTypes</a> 可以找到什么样的编解码器类型被支持。每一种方法都返回一个特定设备的支持的值的数组。设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureStillImageOutput/outputSettings" target="_blank" rel="external">outputSettings</a> 字典来指定你想要的图像格式，例如：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureStillImageOutput</span> *stillImageOutput = [[<span class="built_in">AVCaptureStillImageOutput</span> alloc] init];</span><br><span class="line"><span class="built_in">NSDictionary</span> *outputSettings = @&#123; <span class="built_in">AVVideoCodecKey</span> : <span class="built_in">AVVideoCodecJPEG</span>&#125;;</span><br><span class="line">[stillImageOutput setOutputSettings:outputSettings];</span><br></pre></td></tr></table></figure>
<p>If you want to capture a JPEG image, you should typically not specify your own compression format. Instead, you should let the still image output do the compression for you, since its compression is hardware-accelerated. If you need a data representation of the image, you can use jpegStillImageNSDataRepresentation: to get an NSData object without recompressing the data, even if you modify the image’s metadata.</p>
<p>如果你想捕获一个 <code>JPEG</code> 图像，通常应该不要指定自己的压缩格式。相反，应该让静态图像输出为你做压缩，因为它的压缩是硬件加速的。如果你需要图像的表示数据，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/clm/AVCaptureStillImageOutput/jpegStillImageNSDataRepresentation:" target="_blank" rel="external">jpegStillImageNSDataRepresentation:</a> 得到未压缩数据的<code>NSDate</code> 对象，即使你修改修改图像的元数据。</p>
<h4 id="Capturing-an-Image-捕获图像"><a href="#Capturing-an-Image-捕获图像" class="headerlink" title="Capturing an Image - 捕获图像"></a>Capturing an Image - 捕获图像</h4><p>When you want to capture an image, you send the output a captureStillImageAsynchronouslyFromConnection:completionHandler: message. The first argument is the connection you want to use for the capture. You need to look for the connection whose input port is collecting video:</p>
<p>当你想捕获图像，给输出发送一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureStillImageOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureStillImageOutput/captureStillImageAsynchronouslyFromConnection:completionHandler:" target="_blank" rel="external">captureStillImageAsynchronouslyFromConnection:completionHandler:</a> 消息。第一个参数是用于想要捕获使用的连接。你需要寻找输入端口是收集视频的连接。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureConnection</span> *videoConnection = <span class="literal">nil</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">AVCaptureConnection</span> *connection <span class="keyword">in</span> stillImageOutput.connections) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">AVCaptureInputPort</span> *port <span class="keyword">in</span> [connection inputPorts]) &#123;</span><br><span class="line">        <span class="keyword">if</span> ([[port mediaType] isEqual:<span class="built_in">AVMediaTypeVideo</span>] ) &#123;</span><br><span class="line">            videoConnection = connection;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (videoConnection) &#123; <span class="keyword">break</span>; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The second argument to captureStillImageAsynchronouslyFromConnection:completionHandler: is a block that takes two arguments: a CMSampleBuffer opaque type containing the image data, and an error. The sample buffer itself may contain metadata, such as an EXIF dictionary, as an attachment. You can modify the attachments if you want, but note the optimization for JPEG images discussed in Pixel and Encoding Formats.</p>
<p><code>captureStillImageAsynchronouslyFromConnection:completionHandler:</code> 的第二个参数是一个 <code>block</code> ，<code>block</code> 有两个参数：一个包含图像数据的 <code>CMSampleBuffer</code> 不透明类型，一个 <code>error</code>。样品缓冲自身可能包含元数据，例如 <code>EXIF</code> 字典作为附件。如果你想的话，可以修改附件，但是注意 <code>JPEG</code> 图像进行像素和编码格式的优化。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[stillImageOutput captureStillImageAsynchronouslyFromConnection:videoConnection completionHandler:</span><br><span class="line">    ^(CMSampleBufferRef imageSampleBuffer, <span class="built_in">NSError</span> *error) &#123;</span><br><span class="line">        <span class="built_in">CFDictionaryRef</span> exifAttachments =</span><br><span class="line">            CMGetAttachment(imageSampleBuffer, k<span class="built_in">CGImagePropertyExifDictionary</span>, <span class="literal">NULL</span>);</span><br><span class="line">        <span class="keyword">if</span> (exifAttachments) &#123;</span><br><span class="line">            <span class="comment">// Do something with the attachments.</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Continue as appropriate.</span></span><br><span class="line">    &#125;];</span><br></pre></td></tr></table></figure>
<h2 id="Showing-the-User-What’s-Being-Recorded-显示用户正在被记录什么"><a href="#Showing-the-User-What’s-Being-Recorded-显示用户正在被记录什么" class="headerlink" title="Showing the User What’s Being Recorded - 显示用户正在被记录什么"></a>Showing the User What’s Being Recorded - 显示用户正在被记录什么</h2><p>You can provide the user with a preview of what’s being recorded by the camera (using a preview layer) or by the microphone (by monitoring the audio channel).</p>
<p>可以为用户提供一个预览，关于正在被相机(使用 <code>perview layer</code>)记录什么，或者被麦克风(通过监控音频信道)记录什么。</p>
<h3 id="Video-Preview-视频预览"><a href="#Video-Preview-视频预览" class="headerlink" title="Video Preview - 视频预览"></a>Video Preview - 视频预览</h3><p>You can provide the user with a preview of what’s being recorded using an AVCaptureVideoPreviewLayer object. AVCaptureVideoPreviewLayer is a subclass ofCALayer (see Core Animation Programming Guide. You don’t need any outputs to show the preview.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoPreviewLayer_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoPreviewLayer" target="_blank" rel="external"></a> 对象可以给用户提供一个正在被记录的预览。 <code>AVCaptureVideoPreviewLayer</code> 是 <a href="https://developer.apple.com/library/ios/documentation/GraphicsImaging/Reference/CALayer_class/index.html#//apple_ref/occ/cl/CALayer" target="_blank" rel="external">CALayer</a> 的子类。（详情见 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514" target="_blank" rel="external">Core Animation Programming Guide</a>），不需要任何输出去显示预览。</p>
<p>Using the AVCaptureVideoDataOutput class provides the client application with the ability to access the video pixels before they are presented to the user.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 类提供的访问视频像素才呈现给用户的客户端应用程序的能力。</p>
<p>Unlike a capture output, a video preview layer maintains a strong reference to the session with which it is associated. This is to ensure that the session is not deallocated while the layer is attempting to display video. This is reflected in the way you initialize a preview layer:</p>
<p>与捕获输出不同的是，视频预览层与它关联的会话有一个强引用。这是为了确保会话还没有被释放，<code>layer</code> 就尝试去显示视频。这反映在，你初始化一个预览层的方式上：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *captureSession = &lt;<span class="meta">#Get a capture session#&gt;;</span></span><br><span class="line"><span class="built_in">CALayer</span> *viewLayer = &lt;<span class="meta">#Get a layer from the view in which you want to present the preview#&gt;;</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">AVCaptureVideoPreviewLayer</span> *captureVideoPreviewLayer = [[<span class="built_in">AVCaptureVideoPreviewLayer</span> alloc] initWithSession:captureSession];</span><br><span class="line">[viewLayer addSublayer:captureVideoPreviewLayer];</span><br></pre></td></tr></table></figure>
<p>In general, the preview layer behaves like any other CALayer object in the render tree (see Core Animation Programming Guide). You can scale the image and perform transformations, rotations, and so on just as you would any layer. One difference is that you may need to set the layer’s orientation property to specify how it should rotate images coming from the camera. In addition, you can test for device support for video mirroring by querying the supportsVideoMirroring property. You can set the videoMirrored property as required, although when the automaticallyAdjustsVideoMirroring property is set to YES (the default), the mirroring value is automatically set based on the configuration of the session.</p>
<p>在一般情况下，预览层行为就像渲染树中任何其他 <code>CALayer</code> 对象（见 <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514" target="_blank" rel="external">Core Animation Programming Guide</a>）。可以缩放图像和执行转换、旋转等，就像你可以在任何层。一个不同点是，你可能需要设置层的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoPreviewLayer_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoPreviewLayer/orientation" target="_blank" rel="external">orientation</a> 属性来指定它应该如何从相机中旋转图像。此外，可以通过查询 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/instp/AVCaptureConnection/supportsVideoMirroring" target="_blank" rel="external">supportsVideoMirroring</a> 属性来测试设备对于视频镜像的支持。可以根据需要设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/instp/AVCaptureConnection/videoMirrored" target="_blank" rel="external">videoMirrored</a> 属性，虽然当 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureConnection_Class/index.html#//apple_ref/occ/instp/AVCaptureConnection/automaticallyAdjustsVideoMirroring" target="_blank" rel="external">automaticallyAdjustsVideoMirroring</a> 属性被设置为 <code>YES</code> （默认情况下）， <code>mirroring</code> 值是自动的基于会话配置进行设置。</p>
<h4 id="Video-Gravity-Modes-视屏重力模式"><a href="#Video-Gravity-Modes-视屏重力模式" class="headerlink" title="Video Gravity Modes - 视屏重力模式"></a>Video Gravity Modes - 视屏重力模式</h4><p>The preview layer supports three gravity modes that you set using videoGravity:</p>
<ul>
<li>AVLayerVideoGravityResizeAspect: This preserves the aspect ratio, leaving black bars where the - video does not fill the available screen area.</li>
<li>AVLayerVideoGravityResizeAspectFill: This preserves the aspect ratio, but fills the available - screen area, cropping the video when necessary.</li>
<li>AVLayerVideoGravityResize: This simply stretches the video to fill the available screen area, even if doing so distorts the image.</li>
</ul>
<p>预览层支持3种重力模式，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoPreviewLayer_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoPreviewLayer/videoGravity" target="_blank" rel="external">videoGravity</a> 设置：</p>
<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVLayerVideoGravityResizeAspect" target="_blank" rel="external">AVLayerVideoGravityResizeAspect</a>：保持横纵比，视频不能完全填充的地方留出黑色区域。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVLayerVideoGravityResizeAspectFill" target="_blank" rel="external">AVLayerVideoGravityResizeAspectFill</a>：保持横纵比，但填充可用的屏幕区域，必要的时候裁剪视频。</li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVLayerVideoGravityResize" target="_blank" rel="external">AVLayerVideoGravityResize:</a>：只是简单的拉伸视频以填充可用的屏幕区域，即使这样左会扭曲图像。</li>
</ul>
<h4 id="Using-“Tap-to-Focus”-with-a-Preview-使用“点击焦点”预览"><a href="#Using-“Tap-to-Focus”-with-a-Preview-使用“点击焦点”预览" class="headerlink" title="Using “Tap to Focus” with a Preview - 使用“点击焦点”预览"></a>Using “Tap to Focus” with a Preview - 使用“点击焦点”预览</h4><p>You need to take care when implementing tap-to-focus in conjunction with a preview layer. You must account for the preview orientation and gravity of the layer, and for the possibility that the preview may be mirrored. See the sample code project AVCam-iOS: Using AVFoundation to Capture Images and Movies for an implementation of this functionality.</p>
<p>需要注意的是，在实现点击时要注意结合预览层。必须考虑到该层的预览方向和重力，并考虑预览变为镜像显示的可能性。请看示例代码项目：<a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a>，有关这个功能的实现。</p>
<h3 id="Showing-Audio-Levels-显示音频等级"><a href="#Showing-Audio-Levels-显示音频等级" class="headerlink" title="Showing Audio Levels - 显示音频等级"></a>Showing Audio Levels - 显示音频等级</h3><p>To monitor the average and peak power levels in an audio channel in a capture connection, you use an AVCaptureAudioChannel object. Audio levels are not key-value observable, so you must poll for updated levels as often as you want to update your user interface (for example, 10 times a second).</p>
<p>在捕获连接中检测音频信道的平均值和峰值功率水平，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureAudioChannel_Class/index.html#//apple_ref/occ/cl/AVCaptureAudioChannel" target="_blank" rel="external">AVCaptureAudioChannel</a> 对象。音频等级不是 <code>key-value</code> 可观察的，所以当你想更新你的用户界面（比如10秒一次），必须调查最新的等级。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureAudioDataOutput</span> *audioDataOutput = &lt;<span class="meta">#Get the audio data output#&gt;;</span></span><br><span class="line"><span class="built_in">NSArray</span> *connections = audioDataOutput.connections;</span><br><span class="line"><span class="keyword">if</span> ([connections count] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// There should be only one connection to an AVCaptureAudioDataOutput.</span></span><br><span class="line">    <span class="built_in">AVCaptureConnection</span> *connection = [connections objectAtIndex:<span class="number">0</span>];</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">NSArray</span> *audioChannels = connection.audioChannels;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">AVCaptureAudioChannel</span> *channel <span class="keyword">in</span> audioChannels) &#123;</span><br><span class="line">        <span class="keyword">float</span> avg = channel.averagePowerLevel;</span><br><span class="line">        <span class="keyword">float</span> peak = channel.peakHoldLevel;</span><br><span class="line">        <span class="comment">// Update the level meter user interface.</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Putting-It-All-Together-Capturing-Video-Frames-as-UIImage-Objects-总而言之：捕获视频帧用作-UIImage-对象"><a href="#Putting-It-All-Together-Capturing-Video-Frames-as-UIImage-Objects-总而言之：捕获视频帧用作-UIImage-对象" class="headerlink" title="Putting It All Together: Capturing Video Frames as UIImage Objects - 总而言之：捕获视频帧用作 UIImage 对象"></a>Putting It All Together: Capturing Video Frames as UIImage Objects - 总而言之：捕获视频帧用作 <code>UIImage</code> 对象</h2><p>This brief code example to illustrates how you can capture video and convert the frames you get to UIImage objects. It shows you how to:</p>
<ul>
<li>Create an AVCaptureSession object to coordinate the flow of data from an AV input device to an - output</li>
<li>Find the AVCaptureDevice object for the input type you want</li>
<li>Create an AVCaptureDeviceInput object for the device</li>
<li>Create an AVCaptureVideoDataOutput object to produce video frames</li>
<li>Implement a delegate for the AVCaptureVideoDataOutput object to process video frames</li>
<li>Implement a function to convert the CMSampleBuffer received by the delegate into a UIImage object</li>
</ul>
<p>这个简短的代码示例演示了如何捕捉视频和将帧转化为 <code>UIImage</code> 对象，下面说明方法：</p>
<ul>
<li>创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/cl/AVCaptureSession" target="_blank" rel="external">AVCaptureSession</a> 对象去协调从 <code>AV</code> 输入设备到输出设备的数据流。</li>
<li>找到你想要输入类型的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/cl/AVCaptureDevice" target="_blank" rel="external">AVCaptureDevice</a> 对象。</li>
<li>为设备创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDeviceInput_Class/index.html#//apple_ref/occ/cl/AVCaptureDeviceInput" target="_blank" rel="external">AVCaptureDeviceInput</a> 对象。</li>
<li>创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 去生成视频帧。</li>
<li>为 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 实现代理去处理视频帧。</li>
<li>实现一个函数，将从代理收到的 <code>CMSampleBuffer</code> 转换为一个 <code>UIImage</code> 对象。</li>
</ul>
<blockquote>
<p>Note: To focus on the most relevant code, this example omits several aspects of a complete application, including memory management. To use AV Foundation, you are expected to have enough experience with Cocoa to be able to infer the missing pieces.</p>
<p>注意：关注最相关的代码，这个例子省略了一个完成程序的几部分，包括内存管理。为了使用 <code>AV Foundation</code>，你应该有足够的 <code>Cocoa</code> 经验，有能力推断出丢失的碎片。</p>
</blockquote>
<h3 id="Create-and-Configure-a-Capture-Session-创建和配置捕获会话"><a href="#Create-and-Configure-a-Capture-Session-创建和配置捕获会话" class="headerlink" title="Create and Configure a Capture Session - 创建和配置捕获会话"></a>Create and Configure a Capture Session - 创建和配置捕获会话</h3><p>You use an AVCaptureSession object to coordinate the flow of data from an AV input device to an output. Create a session, and configure it to produce medium-resolution video frames.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/cl/AVCaptureSession" target="_blank" rel="external">AVCaptureSession</a> 对象去协调从 <code>AV</code> 输入设备到输出的数据流。创建一个会话，并将其配置产生中等分辨率的视频帧。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureSession</span> *session = [[<span class="built_in">AVCaptureSession</span> alloc] init];</span><br><span class="line">session.sessionPreset = <span class="built_in">AVCaptureSessionPresetMedium</span>;</span><br></pre></td></tr></table></figure>
<h3 id="Create-and-Configure-the-Device-and-Device-Input-创建和配置设备记忆设备输入"><a href="#Create-and-Configure-the-Device-and-Device-Input-创建和配置设备记忆设备输入" class="headerlink" title="Create and Configure the Device and Device Input - 创建和配置设备记忆设备输入"></a>Create and Configure the Device and Device Input - 创建和配置设备记忆设备输入</h3><p>Capture devices are represented by AVCaptureDevice objects; the class provides methods to retrieve an object for the input type you want. A device has one or more ports, configured using an AVCaptureInput object. Typically, you use the capture input in its default configuration.</p>
<p>Find a video capture device, then create a device input with the device and add it to the session. If an appropriate device can not be located, then the deviceInputWithDevice:error: method will return an error by reference.</p>
<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDevice_Class/index.html#//apple_ref/occ/cl/AVCaptureDevice" target="_blank" rel="external">AVCaptureDevice</a> 对象表示捕获设备；类提供你想要的输入类型对象的方法。一个设备具有一个或者多个端口，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureInput_Class/index.html#//apple_ref/occ/cl/AVCaptureInput" target="_blank" rel="external">AVCaptureInput</a> 对象配置。通常情况下，在它的默认配置中使用捕获输入。</p>
<p>找到一个视频捕获设备，然后创建一个带着设备的设备输入，并将其添加到会话中，如果合适的设备无法定位，然后 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDeviceInput_Class/index.html#//apple_ref/occ/clm/AVCaptureDeviceInput/deviceInputWithDevice:error:" target="_blank" rel="external">deviceInputWithDevice:error:</a> 方法将会通过引用返回一个错误。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureDevice</span> *device =</span><br><span class="line">        [<span class="built_in">AVCaptureDevice</span> defaultDeviceWithMediaType:<span class="built_in">AVMediaTypeVideo</span>];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">NSError</span> *error = <span class="literal">nil</span>;</span><br><span class="line"><span class="built_in">AVCaptureDeviceInput</span> *input =</span><br><span class="line">        [<span class="built_in">AVCaptureDeviceInput</span> deviceInputWithDevice:device error:&amp;error];</span><br><span class="line"><span class="keyword">if</span> (!input) &#123;</span><br><span class="line">    <span class="comment">// Handle the error appropriately.</span></span><br><span class="line">&#125;</span><br><span class="line">[session addInput:input];</span><br></pre></td></tr></table></figure>
<h3 id="Create-and-Configure-the-Video-Data-Output-创建和配置视频数据输出"><a href="#Create-and-Configure-the-Video-Data-Output-创建和配置视频数据输出" class="headerlink" title="Create and Configure the Video Data Output - 创建和配置视频数据输出"></a>Create and Configure the Video Data Output - 创建和配置视频数据输出</h3><p>You use an AVCaptureVideoDataOutput object to process uncompressed frames from the video being captured. You typically configure several aspects of an output. For video, for example, you can specify the pixel format using the videoSettings property and cap the frame rate by setting the minFrameDuration property.</p>
<p>Create and configure an output for video data and add it to the session; cap the frame rate to 15 fps by setting the minFrameDuration property to 1/15 second:</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureVideoDataOutput" target="_blank" rel="external">AVCaptureVideoDataOutput</a> 对象去处理视频捕获过程中未被压缩的帧。通常配置输出的几个方面。例如视频，可以使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/videoSettings" target="_blank" rel="external">videoSettings</a> 属性指定像素格式，通过设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoDataOutput/minFrameDuration" target="_blank" rel="external">minFrameDuration</a> 属性覆盖帧速率。</p>
<p>为视频数据创建和配置输出，并将其添加到会话中；通过设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instp/AVCaptureVideoDataOutput/minFrameDuration" target="_blank" rel="external">minFrameDuration</a> 属性为每秒 <code>1/15</code>，将帧速率覆盖为 <code>15 fps</code> 。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVCaptureVideoDataOutput</span> *output = [[<span class="built_in">AVCaptureVideoDataOutput</span> alloc] init];</span><br><span class="line">[session addOutput:output];</span><br><span class="line">output.videoSettings =</span><br><span class="line">                @&#123; (<span class="built_in">NSString</span> *)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) &#125;;</span><br><span class="line">output.minFrameDuration = CMTimeMake(<span class="number">1</span>, <span class="number">15</span>);</span><br></pre></td></tr></table></figure>
<p>The data output object uses delegation to vend the video frames. The delegate must adopt the AVCaptureVideoDataOutputSampleBufferDelegate protocol. When you set the data output’s delegate, you must also provide a queue on which callbacks should be invoked.</p>
<p>数据输出对象使用委托来声明一个视频帧。代理必须 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/index.html#//apple_ref/occ/intf/AVCaptureVideoDataOutputSampleBufferDelegate" target="_blank" rel="external">AVCaptureVideoDataOutputSampleBufferDelegate</a> 协议。当你设置了数据输出的代理，还必须提供一个回调时应该被调用的队列。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dispatch_queue_t</span> queue = dispatch_queue_create(<span class="string">"MyQueue"</span>, <span class="literal">NULL</span>);</span><br><span class="line">[output setSampleBufferDelegate:<span class="keyword">self</span> queue:queue];</span><br><span class="line">dispatch_release(queue);</span><br></pre></td></tr></table></figure>
<p>You use the queue to modify the priority given to delivering and processing the video frames.</p>
<p>使用队列去修改给定传输和处理视频帧的优先级。</p>
<h3 id="Implement-the-Sample-Buffer-Delegate-Method-实现示例缓冲代理方法"><a href="#Implement-the-Sample-Buffer-Delegate-Method-实现示例缓冲代理方法" class="headerlink" title="Implement the Sample Buffer Delegate Method - 实现示例缓冲代理方法"></a>Implement the Sample Buffer Delegate Method - 实现示例缓冲代理方法</h3><p>In the delegate class, implement the method (captureOutput:didOutputSampleBuffer:fromConnection:) that is called when a sample buffer is written. The video data output object delivers frames as CMSampleBuffer opaque types, so you need to convert from the CMSampleBuffer opaque type to a UIImage object. The function for this operation is shown in Converting CMSampleBuffer to a UIImage Object.</p>
<p>在代理类，实现方法（<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureAudioDataOutputSampleBufferDelegate_Protocol/index.html#//apple_ref/occ/intfm/AVCaptureAudioDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:" target="_blank" rel="external">captureOutput:didOutputSampleBuffer:fromConnection:</a>），当样本缓冲写入时被调用。视频数据输出对象传递了 <code>CMSampleBuffer</code> 不透明类型的帧，所以你需要从 <code>CMSampleBuffer</code> 不透明类型转化为一个 <code>UIImage</code> 对象。这个操作的功能在 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW4" target="_blank" rel="external">Converting CMSampleBuffer to a UIImage Object</a> 中展示。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)captureOutput:(<span class="built_in">AVCaptureOutput</span> *)captureOutput</span><br><span class="line">         didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer</span><br><span class="line">         fromConnection:(<span class="built_in">AVCaptureConnection</span> *)connection &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">UIImage</span> *image = imageFromSampleBuffer(sampleBuffer);</span><br><span class="line">    <span class="comment">// Add your code here that uses the image.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Remember that the delegate method is invoked on the queue you specified in setSampleBufferDelegate:queue:; if you want to update the user interface, you must invoke any relevant code on the main thread.</p>
<p>记住，代理方法是在 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/index.html#//apple_ref/occ/instm/AVCaptureVideoDataOutput/setSampleBufferDelegate:queue:" target="_blank" rel="external">setSampleBufferDelegate:queue:</a> 中你指定的队列中调用；如果你想要更新用户界面，必须在主线程上调用任何相关代码。</p>
<h3 id="Starting-and-Stopping-Recording-启动和停止录制"><a href="#Starting-and-Stopping-Recording-启动和停止录制" class="headerlink" title="Starting and Stopping Recording - 启动和停止录制"></a>Starting and Stopping Recording - 启动和停止录制</h3><p>After configuring the capture session, you should ensure that the camera has permission to record according to the user’s preferences.</p>
<p>在配置捕获会话后，应该确保相机根据用户的首相选具有录制的权限。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSString</span> *mediaType = <span class="built_in">AVMediaTypeVideo</span>;</span><br><span class="line"> </span><br><span class="line">[<span class="built_in">AVCaptureDevice</span> requestAccessForMediaType:mediaType completionHandler:^(<span class="built_in">BOOL</span> granted) &#123;</span><br><span class="line">    <span class="keyword">if</span> (granted)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//Granted access to mediaType</span></span><br><span class="line">        [<span class="keyword">self</span> setDeviceAuthorized:<span class="literal">YES</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//Not granted access to mediaType</span></span><br><span class="line">        <span class="built_in">dispatch_async</span>(dispatch_get_main_queue(), ^&#123;</span><br><span class="line">        [[[<span class="built_in">UIAlertView</span> alloc] initWithTitle:<span class="string">@"AVCam!"</span></span><br><span class="line">                                    message:<span class="string">@"AVCam doesn't have permission to use Camera, please change privacy settings"</span></span><br><span class="line">                                   delegate:<span class="keyword">self</span></span><br><span class="line">                          cancelButtonTitle:<span class="string">@"OK"</span></span><br><span class="line">                          otherButtonTitles:<span class="literal">nil</span>] show];</span><br><span class="line">                [<span class="keyword">self</span> setDeviceAuthorized:<span class="literal">NO</span>];</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<p>If the camera session is configured and the user has approved access to the camera (and if required, the microphone), send a startRunning message to start the recording.</p>
<p>如果相机会话被配置，用户批准访问摄像头（如果需要，麦克风），发送 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/startRunning" target="_blank" rel="external">startRunning</a> 消息开始录制。</p>
<blockquote>
<p>Important: The startRunning method is a blocking call which can take some time, therefore you should perform session setup on a serial queue so that the main queue isn’t blocked (which keeps the UI responsive). See AVCam-iOS: Using AVFoundation to Capture Images and Movies for the canonical implementation example.</p>
<p>重点：<code>startRunning</code> 方法正在阻塞调用时，可能需要一些时间，因此你应该在串行队列执行会话建立，为了主队列不被堵塞（使<code>UI</code>相应）。见 <a href="https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112" target="_blank" rel="external">AVCam-iOS: Using AVFoundation to Capture Images and Movies</a> ，典型实现的例子。</p>
</blockquote>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[session startRunning];</span><br></pre></td></tr></table></figure>
<p>To stop recording, you send the session a stopRunning message.</p>
<p>要停止录制，给会话发送一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureSession_Class/index.html#//apple_ref/occ/instm/AVCaptureSession/stopRunning" target="_blank" rel="external">stopRunning</a> 消息。</p>
<h2 id="High-Frame-Rate-Video-Capture-高帧速率视频捕获"><a href="#High-Frame-Rate-Video-Capture-高帧速率视频捕获" class="headerlink" title="High Frame Rate Video Capture - 高帧速率视频捕获"></a>High Frame Rate Video Capture - 高帧速率视频捕获</h2><p>iOS 7.0 introduces high frame rate video capture support (also referred to as “SloMo” video) on selected hardware. The full AVFoundation framework supports high frame rate content.</p>
<p>You determine the capture capabilities of a device using the AVCaptureDeviceFormat class. This class has methods that return the supported media types, frame rates, field of view, maximum zoom factor, whether video stabilization is supported, and more.</p>
<ul>
<li>Capture supports full 720p (1280 x 720 pixels) resolution at 60 frames per second (fps) including - video stabilization and droppable P-frames (a feature of H264 encoded movies, which allow the - movies to play back smoothly even on slower and older hardware.)</li>
<li>Playback has enhanced audio support for slow and fast playback, allowing the time pitch of the - audio can be preserved at slower or faster speeds.</li>
<li>Editing has full support for scaled edits in mutable compositions.</li>
<li>Export provides two options when supporting 60 fps movies. The variable frame rate, slow or fast motion, can be preserved, or the movie and be converted to an arbitrary slower frame rate such as 30 frames per second.</li>
</ul>
<p>The SloPoke sample code demonstrates the AVFoundation support for fast video capture, determining whether hardware supports high frame rate video capture, playback using various rates and time pitch algorithms, and editing (including setting time scales for portions of a composition).</p>
<p><code>iOS 7</code> 在特定的硬件中，引入了高帧速率的视频捕获支持（也被称为 <code>“SloMo”</code> 视频）。所有的 <code>AVFoundation</code> 框架都支持高帧速率内容。</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureDeviceFormat_Class/index.html#//apple_ref/occ/cl/AVCaptureDeviceFormat" target="_blank" rel="external">AVCaptureDeviceFormat</a> 类确定设备的捕获能力。该类有一个方法，返回支持媒体类型、帧速率、视图因子、最大缩放因子，是否支持视频稳定性等等。</p>
<ul>
<li>捕获完全支持每秒60帧的 <code>720p</code> （1280 x 720像素）分辨率，包括视频稳定性和可弃用的帧间编码（ <code>H264</code>编码特征的电影，使得电影甚至在更慢更老的硬件也能很顺畅的播放）</li>
<li>播放增强了对于慢速和快速播放的音频支持，允许音频的时间间距可以被保存在较慢或者更快的速度。</li>
<li>编辑已全面支持规模可变的组成编辑。</li>
<li>当支持<code>60fps</code>电影，出口提供了两种选择。可变的帧速率，缓慢或者快速的移动，可以保存，或者电影可以被转换为一个任意的较慢的帧速率，比如每秒30帧。</li>
</ul>
<p><code>SloPoke</code> 示例代码演示了 <code>AVFoundation</code> 支持快速视频捕获，确定硬件是否支持高帧速率视频采集，使用不同速率和时间间距算法播放、编辑（包括设置为一个组件一部分的时间尺度）。</p>
<h3 id="Playback-播放-2"><a href="#Playback-播放-2" class="headerlink" title="Playback - 播放"></a>Playback - 播放</h3><p>An instance of AVPlayer manages most of the playback speed automatically by setting the setRate: method value. The value is used as a multiplier for the playback speed. A value of 1.0 causes normal playback, 0.5 plays back at half speed, 5.0 plays back five times faster than normal, and so on.</p>
<p><code>AVPlayer</code> 的实例通过设置 <code>setRate:</code> 方法值，自动管理了大部分的播放速度。值被当做播放速度的乘法器使用。值为 <code>1.0</code> 是正常播放，<code>0.5</code> 是播放速度的一半，<code>5.0</code> 表示播放速度是正常速度的5倍，等等。</p>
<p>The AVPlayerItem object supports the audioTimePitchAlgorithm property. This property allows you to specify how audio is played when the movie is played at various frame rates using the Time Pitch Algorithm Settings constants.</p>
<p><code>AVPlayerItem</code> 对象支持 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayerItem_Class/index.html#//apple_ref/occ/instp/AVPlayerItem/audioTimePitchAlgorithm" target="_blank" rel="external">audioTimePitchAlgorithm</a> 属性。此属性允许你指定在使用时距算法设置常量播放不同的帧速率的电影时，音频的播放方式。</p>
<p>The following table shows the supported time pitch algorithms, the quality, whether the algorithm causes the audio to snap to specific frame rates, and the frame rate range that each algorithm supports.</p>
<p>下表显示了支持的时距算法、质量，该算法是否会导致音频突然跳到特定的帧速率，以及每个算法支持的帧速率范围。</p>
<p>| Time pitch algorithm | Quality | Snaps to specific frame rate | Rate range |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundationAudioSettings_Constants/index.html#//apple_ref/c/data/AVAudioTimePitchAlgorithmLowQualityZeroLatency" target="_blank" rel="external">AVAudioTimePitchAlgorithmLowQualityZeroLatency</a> | Low quality, suitable for fast-forward, rewind, or low quality voice. | <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/ObjCRuntimeRef/index.html#//apple_ref/doc/c_ref/YES" target="_blank" rel="external">YES</a> | 0.5, 0.666667, 0.8, 1.0, 1.25, 1.5, 2.0 rates. |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundationAudioSettings_Constants/index.html#//apple_ref/c/data/AVAudioTimePitchAlgorithmTimeDomain" target="_blank" rel="external">AVAudioTimePitchAlgorithmTimeDomain</a> | Modest quality, less expensive computationally, suitable for voice. | <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/ObjCRuntimeRef/index.html#//apple_ref/doc/c_ref/NO" target="_blank" rel="external">NO</a> | 0.5–2x rates. |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundationAudioSettings_Constants/index.html#//apple_ref/c/data/AVAudioTimePitchAlgorithmSpectral" target="_blank" rel="external">AVAudioTimePitchAlgorithmSpectral</a> | Highest quality, most expensive computationally, preserves the pitch of the original item. | <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/ObjCRuntimeRef/index.html#//apple_ref/doc/c_ref/NO" target="_blank" rel="external">NO</a> | 1/32–32 rates. |<br>| <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundationAudioSettings_Constants/index.html#//apple_ref/c/data/AVAudioTimePitchAlgorithmVarispeed" target="_blank" rel="external">AVAudioTimePitchAlgorithmVarispeed</a> | High-quality playback with no pitch correction. | <a href="https://developer.apple.com/library/ios/documentation/Cocoa/Reference/ObjCRuntimeRef/index.html#//apple_ref/doc/c_ref/NO" target="_blank" rel="external">NO</a> | 1/32–32 rates. |</p>
<h3 id="Editing-编辑-2"><a href="#Editing-编辑-2" class="headerlink" title="Editing - 编辑"></a>Editing - 编辑</h3><p>When editing, you use the AVMutableComposition class to build temporal edits.</p>
<ul>
<li>Create a new AVMutableComposition instance using the composition class method.</li>
<li>Insert your video asset using the insertTimeRange:ofAsset:atTime:error: method.</li>
<li>Set the time scale of a portion of the composition using scaleTimeRange:toDuration:</li>
</ul>
<p>当编辑时，使用 <code>AVMutableComposition</code> 类去建立时间编辑。</p>
<ul>
<li>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/clm/AVMutableComposition/composition" target="_blank" rel="external">composition</a> 类方法创建一个新的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/cl/AVMutableComposition" target="_blank" rel="external">AVMutableComposition</a> 实例。</li>
<li>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/instm/AVMutableComposition/insertTimeRange:ofAsset:atTime:error:" target="_blank" rel="external">insertTimeRange:ofAsset:atTime:error:</a> 方法给视频插入资产。</li>
<li>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableComposition_Class/index.html#//apple_ref/occ/instm/AVMutableComposition/scaleTimeRange:toDuration:" target="_blank" rel="external">scaleTimeRange:toDuration:</a> 设置组件部分的时间规模。</li>
</ul>
<h3 id="Export-出口"><a href="#Export-出口" class="headerlink" title="Export - 出口"></a>Export - 出口</h3><p>Exporting 60 fps video uses the AVAssetExportSession class to export an asset. The content can be exported using two techniques:</p>
<p>Use the AVAssetExportPresetPassthrough preset to avoid reencoding the movie. It retimes the media with the sections of the media tagged as section 60 fps, section slowed down, or section sped up.</p>
<p>Use a constant frame rate export for maximum playback compatibility. Set the frameDuration property of the video composition to 30 fps. You can also specify the time pitch by using setting the export session’s audioTimePitchAlgorithm property.</p>
<p>使用 <code>AVAssetExportSession</code> 类将 <code>60fps</code> 的视频导出到资产。该内容可以使用两种技术导出：</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/c/data/AVAssetExportPresetPassthrough" target="_blank" rel="external">AVAssetExportPresetPassthrough</a> 预设，避免将电影重新编码。它重新定时媒体，将媒体部分标记为 <code>60fps</code> 的部分，缓慢的部分或者加速的部分。</p>
<p>使用恒定的帧速率导出最大播放兼容性。设置视频组件的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVVideoComposition_Class/index.html#//apple_ref/occ/instm/AVVideoComposition/frameDuration" target="_blank" rel="external">frameDuration</a> 属性为 <code>30fps</code> 。也可以通过设置导出会话的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/instp/AVAssetExportSession/audioTimePitchAlgorithm" target="_blank" rel="external">audioTimePitchAlgorithm</a> 属性指定时间间距。</p>
<h3 id="Recording-录制"><a href="#Recording-录制" class="headerlink" title="Recording - 录制"></a>Recording - 录制</h3><p>You capture high frame rate video using the AVCaptureMovieFileOutput class, which automatically supports high frame rate recording. It will automatically select the correct H264 pitch level and bit rate.</p>
<p>To do custom recording, you must use the AVAssetWriter class, which requires some additional setup.</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput" target="_blank" rel="external">AVCaptureMovieFileOutput</a> 类捕获高帧速率的视频，该类自动支持高帧率录制。它会自动选择正确的 <code>H264</code> 的高音和比特率。</p>
<p>做定制的录制，必须使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/cl/AVAssetWriter" target="_blank" rel="external">AVAssetWriter</a> 类，这需要一些额外的设置。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">assetWriterInput.expectsMediaDataInRealTime=<span class="literal">YES</span>;</span><br></pre></td></tr></table></figure>
<p>This setting ensures that the capture can keep up with the incoming data.</p>
<p>此设置确保捕获可以跟上传入的数据。</p>
<h1 id="Export-输出"><a href="#Export-输出" class="headerlink" title="Export - 输出"></a>Export - 输出</h1><p>To read and write audiovisual assets, you must use the export APIs provided by the AVFoundation framework. The AVAssetExportSession class provides an interface for simple exporting needs, such as modifying the file format or trimming the length of an asset (see Trimming and Transcoding a Movie). For more in-depth exporting needs, use the AVAssetReader and AVAssetWriter classes.</p>
<p>必须使用 <code>AVFoundation</code> 框架提供的导出 <code>APIs</code> 去读写音视频资产。<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetExportSession_Class/index.html#//apple_ref/occ/cl/AVAssetExportSession" target="_blank" rel="external">AVAssetExportSession</a> 类为简单输出需要，提供了一个接口，例如修改文件格式或者削减资产的长度（见 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW8" target="_blank" rel="external">Trimming and Transcoding a Movie</a>）。为了更深入的导出需求，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReader_Class/index.html#//apple_ref/occ/cl/AVAssetReader" target="_blank" rel="external">AVAssetReader</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/cl/AVAssetWriter" target="_blank" rel="external">AVAssetWriter</a> 类。</p>
<p>Use an AVAssetReader when you want to perform an operation on the contents of an asset. For example, you might read the audio track of an asset to produce a visual representation of the waveform. To produce an asset from media such as sample buffers or still images, use an AVAssetWriter object.</p>
<p>当你想对一项资产的内容进行操作时，使用 <code>AVAssetReader</code> 。例如，可以读取一个资产的音频轨道，以产生波形的可视化表示。为了从媒体（比如样品缓冲或者静态图像）生成资产，使用 <code>AVAssetWriter</code> 对象。</p>
<blockquote>
<p>Note: The asset reader and writer classes are not intended to be used for real-time processing. In fact, an asset reader cannot even be used for reading from a real-time source like an HTTP live stream. However, if you are using an asset writer with a real-time data source, such as an AVCaptureOutput object, set the expectsMediaDataInRealTime property of your asset writer’s inputs to YES. Setting this property to YES for a non-real-time data source will result in your files not being interleaved properly.</p>
<p>注意：资产 <code>reader</code> 和 <code>writer</code> 类不打算用到实时处理。实际上，一个资产读取器甚至不能用于从一个类似 <code>HTTP</code> 直播流的实时资源中读取。然而，如果你使用带着实时数据资源的资产写入器，比如 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureOutput_Class/index.html#//apple_ref/occ/cl/AVCaptureOutput" target="_blank" rel="external">AVCaptureOutput</a> 对象，设置资产写入器入口的 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriterInput_Class/index.html#//apple_ref/occ/instp/AVAssetWriterInput/expectsMediaDataInRealTime" target="_blank" rel="external">expectsMediaDataInRealTime</a> 属性为 <code>YES</code>。将此属性设置为 <code>YES</code> 的非实时数据源将导致你的文件不能被正确的扫描。</p>
</blockquote>
<h2 id="Reading-an-Asset-读取资产"><a href="#Reading-an-Asset-读取资产" class="headerlink" title="Reading an Asset - 读取资产"></a>Reading an Asset - 读取资产</h2><p>Each AVAssetReader object can be associated only with a single asset at a time, but this asset may contain multiple tracks. For this reason, you must assign concrete subclasses of the AVAssetReaderOutput class to your asset reader before you begin reading in order to configure how the media data is read. There are three concrete subclasses of the AVAssetReaderOutput base class that you can use for your asset reading needs: AVAssetReaderTrackOutput, AVAssetReaderAudioMixOutput, and AVAssetReaderVideoCompositionOutput.</p>
<p>每个 <code>AVAssetReader</code> 对象只能与单个资产有关，但这个资产可能包含多个轨道。为此，你必须指定 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderOutput_Class/index.html#//apple_ref/occ/cl/AVAssetReaderOutput" target="_blank" rel="external">AVAssetReaderOutput</a> 类的具体子类给你的资产读取器，在你开始按顺序访问你的资产以配置如何读取数据之前。有 <code>AVAssetReaderOutput</code> 基类的3个具体子类，可以使用你的资产访问需求 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderTrackOutput_Class/index.html#//apple_ref/occ/cl/AVAssetReaderTrackOutput" target="_blank" rel="external">AVAssetReaderTrackOutput</a>，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderAudioMixOutput_Class/index.html#//apple_ref/occ/cl/AVAssetReaderAudioMixOutput" target="_blank" rel="external">AVAssetReaderAudioMixOutput</a>，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderVideoCompositionOutput_Class/index.html#//apple_ref/occ/cl/AVAssetReaderVideoCompositionOutput" target="_blank" rel="external">AVAssetReaderVideoCompositionOutput</a>。</p>
<h3 id="Creating-the-Asset-Reader-创建资产读取器"><a href="#Creating-the-Asset-Reader-创建资产读取器" class="headerlink" title="Creating the Asset Reader - 创建资产读取器"></a>Creating the Asset Reader - 创建资产读取器</h3><p>All you need to initialize an AVAssetReader object is the asset that you want to read.</p>
<p>所有你需要去初始化 <code>AVAssetReader</code> 对象是你想要访问的资产。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSError</span> *outError;</span><br><span class="line"><span class="built_in">AVAsset</span> *someAsset = &lt;<span class="meta">#AVAsset that you want to read#&gt;;</span></span><br><span class="line"><span class="built_in">AVAssetReader</span> *assetReader = [<span class="built_in">AVAssetReader</span> assetReaderWithAsset:someAsset error:&amp;outError];</span><br><span class="line"><span class="built_in">BOOL</span> success = (assetReader != <span class="literal">nil</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Always check that the asset reader returned to you is non-nil to ensure that the asset reader was initialized successfully. Otherwise, the error parameter (outError in the previous example) will contain the relevant error information.</p>
<p>注意：总是要资产读取器是否返回给你的时 <code>non-nil</code> ，以确保资产读取器已经成功被初始化。否则，错误参数（之前的例子中 <code>outError</code>）将会包含有关错误的信息。</p>
</blockquote>
<h3 id="Setting-Up-the-Asset-Reader-Outputs-建立资产读取器出口"><a href="#Setting-Up-the-Asset-Reader-Outputs-建立资产读取器出口" class="headerlink" title="Setting Up the Asset Reader Outputs - 建立资产读取器出口"></a>Setting Up the Asset Reader Outputs - 建立资产读取器出口</h3><p>After you have created your asset reader, set up at least one output to receive the media data being read. When setting up your outputs, be sure to set the alwaysCopiesSampleData property to NO. In this way, you reap the benefits of performance improvements. In all of the examples within this chapter, this property could and should be set to NO.</p>
<p>在你创建了资产读取器之后，至少设置一个出口以接收正在读取的媒体数据。当建立你的出口，确保设置 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderOutput_Class/index.html#//apple_ref/occ/cl/AVAssetReaderOutput" target="_blank" rel="external">alwaysCopiesSampleData</a> 属性为 <code>NO</code>。这样，你就收获了性能改进的好处。这一章的所有例子中，这个属性可以并且应该被设置为 <code>NO</code> 。</p>
<p>If you want only to read media data from one or more tracks and potentially convert that data to a different format, use the AVAssetReaderTrackOutput class, using a single track output object for each AVAssetTrack object that you want to read from your asset. To decompress an audio track to Linear PCM with an asset reader, you set up your track output as follows:</p>
<p>如果你只想从一个或多个轨道读取媒体数据，潜在的数据转换为不同的格式，使用 <code>AVAssetReaderTrackOutput</code> 类，每个你想从你的资产中读取 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/cl/AVAssetTrack" target="_blank" rel="external">AVAssetTrack</a> 对象都使用单轨道出口对象。将音频轨道解压缩为有资产读取器的 <code>Linear PCM</code> ，建立轨道出口如下：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *localAsset = assetReader.asset;</span><br><span class="line"><span class="comment">// Get the audio track to read.</span></span><br><span class="line"><span class="built_in">AVAssetTrack</span> *audioTrack = [[localAsset tracksWithMediaType:<span class="built_in">AVMediaTypeAudio</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line"><span class="comment">// Decompression settings for Linear PCM</span></span><br><span class="line"><span class="built_in">NSDictionary</span> *decompressionAudioSettings = @&#123; <span class="built_in">AVFormatIDKey</span> : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kAudioFormatLinearPCM] &#125;;</span><br><span class="line"><span class="comment">// Create the output with the audio track and decompression settings.</span></span><br><span class="line"><span class="built_in">AVAssetReaderOutput</span> *trackOutput = [<span class="built_in">AVAssetReaderTrackOutput</span> assetReaderTrackOutputWithTrack:audioTrack outputSettings:decompressionAudioSettings];</span><br><span class="line"><span class="comment">// Add the output to the reader if possible.</span></span><br><span class="line"><span class="keyword">if</span> ([assetReader canAddOutput:trackOutput])</span><br><span class="line">    [assetReader addOutput:trackOutput];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: To read the media data from a specific asset track in the format in which it was stored, pass nil to the outputSettings parameter.</p>
<p>注意：从一个特定的资产轨道读取媒体数据，以它被存储的格式，传 <code>nil</code> 给 <code>outputSettings</code> 参数。</p>
</blockquote>
<p>You use the AVAssetReaderAudioMixOutput and AVAssetReaderVideoCompositionOutput classes to read media data that has been mixed or composited together using an AVAudioMix object or AVVideoComposition object, respectively. Typically, these outputs are used when your asset reader is reading from an AVComposition object.</p>
<p>使用 <code>AVAssetReaderAudioMixOutput</code> 和 <code>AVAssetReaderVideoCompositionOutput</code> 类来读取媒体数据，这些媒体数据是分别使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioMix_Class/index.html#//apple_ref/occ/cl/AVAudioMix" target="_blank" rel="external">AVAudioMix</a> 对象或者 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVVideoComposition_Class/index.html#//apple_ref/occ/cl/AVVideoComposition" target="_blank" rel="external">AVVideoComposition</a> 对象混合或者组合在一起。通常情况下，当你的资产读取器正在从 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVComposition_Class/index.html#//apple_ref/occ/cl/AVComposition" target="_blank" rel="external">AVComposition</a> 读取时，才使用这些出口。</p>
<p>With a single audio mix output, you can read multiple audio tracks from your asset that have been mixed together using an AVAudioMix object. To specify how the audio tracks are mixed, assign the mix to the AVAssetReaderAudioMixOutput object after initialization. The following code displays how to create an audio mix output with all of the audio tracks from your asset, decompress the audio tracks to Linear PCM, and assign an audio mix object to the output. For details on how to configure an audio mix, see Editing.</p>
<p>一个单一音频混合出口，可以从 已经使用 <code>AVAudioMix</code> 对象混合在一起的资产中读取多个音轨。指定音轨是如何被混合在一起的，将混合后的 <code>AVAssetReaderAudioMixOutput</code> 对象初始化。下面的代码显示了如何从资产中创建一个带着所有音轨的音频混合出口，将音轨解压为 <code>Linear PCM</code>，并指定音频混合对象到出口。有如何配置音频混合的细节，请参见 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1" target="_blank" rel="external">Editing</a> 。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAudioMix</span> *audioMix = &lt;<span class="meta">#An AVAudioMix that specifies how the audio tracks from the AVAsset are mixed#&gt;;</span></span><br><span class="line"><span class="comment">// Assumes that assetReader was initialized with an AVComposition object.</span></span><br><span class="line"><span class="built_in">AVComposition</span> *composition = (<span class="built_in">AVComposition</span> *)assetReader.asset;</span><br><span class="line"><span class="comment">// Get the audio tracks to read.</span></span><br><span class="line"><span class="built_in">NSArray</span> *audioTracks = [composition tracksWithMediaType:<span class="built_in">AVMediaTypeAudio</span>];</span><br><span class="line"><span class="comment">// Get the decompression settings for Linear PCM.</span></span><br><span class="line"><span class="built_in">NSDictionary</span> *decompressionAudioSettings = @&#123; <span class="built_in">AVFormatIDKey</span> : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kAudioFormatLinearPCM] &#125;;</span><br><span class="line"><span class="comment">// Create the audio mix output with the audio tracks and decompression setttings.</span></span><br><span class="line"><span class="built_in">AVAssetReaderOutput</span> *audioMixOutput = [<span class="built_in">AVAssetReaderAudioMixOutput</span> assetReaderAudioMixOutputWithAudioTracks:audioTracks audioSettings:decompressionAudioSettings];</span><br><span class="line"><span class="comment">// Associate the audio mix used to mix the audio tracks being read with the output.</span></span><br><span class="line">audioMixOutput.audioMix = audioMix;</span><br><span class="line"><span class="comment">// Add the output to the reader if possible.</span></span><br><span class="line"><span class="keyword">if</span> ([assetReader canAddOutput:audioMixOutput])</span><br><span class="line">    [assetReader addOutput:audioMixOutput];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Passing nil for the audioSettings parameter tells the asset reader to return samples in a convenient uncompressed format. The same is true for the AVAssetReaderVideoCompositionOutput class.</p>
<p>注意：给 <code>audioSettings</code> 参数传递 <code>nil</code> ，告诉资产读取器返回一个方便的未压缩格式的样本。对于 <code>AVAssetReaderVideoCompositionOutput</code> 类同样是可以的。</p>
</blockquote>
<p>The video composition output behaves in much the same way: You can read multiple video tracks from your asset that have been composited together using an AVVideoComposition object. To read the media data from multiple composited video tracks and decompress it to ARGB, set up your output as follows:</p>
<p>视频合成输出行为有许多同样的方式：可以从资产（已经被使用 <code>AVVideoComposition</code> 对象合并在一起）读取多个视频轨道。从多个复合视频轨道读取媒体数据，解压缩为 <code>ARGB</code> ，建立出口如下：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVVideoComposition</span> *videoComposition = &lt;<span class="meta">#An AVVideoComposition that specifies how the video tracks from the AVAsset are composited#&gt;;</span></span><br><span class="line"><span class="comment">// Assumes assetReader was initialized with an AVComposition.</span></span><br><span class="line"><span class="built_in">AVComposition</span> *composition = (<span class="built_in">AVComposition</span> *)assetReader.asset;</span><br><span class="line"><span class="comment">// Get the video tracks to read.</span></span><br><span class="line"><span class="built_in">NSArray</span> *videoTracks = [composition tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>];</span><br><span class="line"><span class="comment">// Decompression settings for ARGB.</span></span><br><span class="line"><span class="built_in">NSDictionary</span> *decompressionVideoSettings = @&#123; (<span class="keyword">id</span>)kCVPixelBufferPixelFormatTypeKey : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kCVPixelFormatType_32ARGB], (<span class="keyword">id</span>)kCVPixelBufferIOSurfacePropertiesKey : [<span class="built_in">NSDictionary</span> dictionary] &#125;;</span><br><span class="line"><span class="comment">// Create the video composition output with the video tracks and decompression setttings.</span></span><br><span class="line"><span class="built_in">AVAssetReaderOutput</span> *videoCompositionOutput = [<span class="built_in">AVAssetReaderVideoCompositionOutput</span> assetReaderVideoCompositionOutputWithVideoTracks:videoTracks videoSettings:decompressionVideoSettings];</span><br><span class="line"><span class="comment">// Associate the video composition used to composite the video tracks being read with the output.</span></span><br><span class="line">videoCompositionOutput.videoComposition = videoComposition;</span><br><span class="line"><span class="comment">// Add the output to the reader if possible.</span></span><br><span class="line"><span class="keyword">if</span> ([assetReader canAddOutput:videoCompositionOutput])</span><br><span class="line">    [assetReader addOutput:videoCompositionOutput];</span><br></pre></td></tr></table></figure>
<h3 id="Reading-the-Asset’s-Media-Data-读取资产媒体数据"><a href="#Reading-the-Asset’s-Media-Data-读取资产媒体数据" class="headerlink" title="Reading the Asset’s Media Data - 读取资产媒体数据"></a>Reading the Asset’s Media Data - 读取资产媒体数据</h3><p>To start reading after setting up all of the outputs you need, call the startReading method on your asset reader. Next, retrieve the media data individually from each output using the copyNextSampleBuffer method. To start up an asset reader with a single output and read all of its media samples, do the following:</p>
<p>开始读取后建立所有你需要的出口，在你的资产读取器中调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReader_Class/index.html#//apple_ref/occ/instm/AVAssetReader/startReading" target="_blank" rel="external">startReading</a> 方法。下一步，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetReaderOutput_Class/index.html#//apple_ref/occ/instm/AVAssetReaderOutput/copyNextSampleBuffer" target="_blank" rel="external">copyNextSampleBuffer</a> 方法从每个出口分别获取媒体数据。以一个出口启动一个资产读取器，并读取它的所有媒体样本，跟着下面做：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Start the asset reader up.</span></span><br><span class="line">[<span class="keyword">self</span>.assetReader startReading];</span><br><span class="line"><span class="built_in">BOOL</span> done = <span class="literal">NO</span>;</span><br><span class="line"><span class="keyword">while</span> (!done)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// Copy the next sample buffer from the reader output.</span></span><br><span class="line">  CMSampleBufferRef sampleBuffer = [<span class="keyword">self</span>.assetReaderOutput copyNextSampleBuffer];</span><br><span class="line">  <span class="keyword">if</span> (sampleBuffer)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">// Do something with sampleBuffer here.</span></span><br><span class="line">    <span class="built_in">CFRelease</span>(sampleBuffer);</span><br><span class="line">    sampleBuffer = <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">// Find out why the asset reader output couldn't copy another sample buffer.</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">self</span>.assetReader.status == <span class="built_in">AVAssetReaderStatusFailed</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="built_in">NSError</span> *failureError = <span class="keyword">self</span>.assetReader.error;</span><br><span class="line">      <span class="comment">// Handle the error here.</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="comment">// The asset reader output has read all of its samples.</span></span><br><span class="line">      done = <span class="literal">YES</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Writing-an-Asset-写入资产"><a href="#Writing-an-Asset-写入资产" class="headerlink" title="Writing an Asset - 写入资产"></a>Writing an Asset - 写入资产</h2><p>The AVAssetWriter class to write media data from multiple sources to a single file of a specified file format. You don’t need to associate your asset writer object with a specific asset, but you must use a separate asset writer for each output file that you want to create. Because an asset writer can write media data from multiple sources, you must create an AVAssetWriterInput object for each individual track that you want to write to the output file. Each AVAssetWriterInput object expects to receive data in the form of CMSampleBufferRef objects, but if you want to append CVPixelBufferRef objects to your asset writer input, use the AVAssetWriterInputPixelBufferAdaptor class.</p>
<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/cl/AVAssetWriter" target="_blank" rel="external">AVAssetWriter</a> 类从多个源将媒体数据写入到指定文件格式的单个文件中。不需要将你的资产写入器与一个特定的资产联系起来，但你必须为你要创建的每个输出文件 使用一个独立的资产写入器。因为一个资产写入器可以从多个来源写入媒体数据，你必须为你想写入输出文件的每个独立的轨道创建一个 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriterInput_Class/index.html#//apple_ref/occ/cl/AVAssetWriterInput" target="_blank" rel="external">AVAssetWriterInput</a> 对象。每个 <code>AVAssetWriterInput</code> 对象预计以 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/tdef/CMSampleBufferRef" target="_blank" rel="external">CMSampleBufferRef</a> 对象的形成接收数据，但如果你想给你的资产写入器入口 附加 <a href="https://developer.apple.com/library/ios/documentation/QuartzCore/Reference/CVPixelBufferRef/index.html#//apple_ref/c/tdef/CVPixelBufferRef" target="_blank" rel="external">CVPixelBufferRef</a> 对象，使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriterInputPixelBufferAdaptor_Class/index.html#//apple_ref/occ/cl/AVAssetWriterInputPixelBufferAdaptor" target="_blank" rel="external">AVAssetWriterInputPixelBufferAdaptor</a> 类。</p>
<h3 id="Creating-the-Asset-Writer-创建资产写入器"><a href="#Creating-the-Asset-Writer-创建资产写入器" class="headerlink" title="Creating the Asset Writer - 创建资产写入器"></a>Creating the Asset Writer - 创建资产写入器</h3><p>To create an asset writer, specify the URL for the output file and the desired file type. The following code displays how to initialize an asset writer to create a QuickTime movie:</p>
<p>为了创建一个资产写入器，为出口文件指定 <code>URL</code> 和所需的文件类型。下面的代码显示了如何初始化一个资产写入器来创建一个 <code>QuickTime</code> 影片：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSError</span> *outError;</span><br><span class="line"><span class="built_in">NSURL</span> *outputURL = &lt;<span class="meta">#NSURL object representing the URL where you want to save the video#&gt;;</span></span><br><span class="line"><span class="built_in">AVAssetWriter</span> *assetWriter = [<span class="built_in">AVAssetWriter</span> assetWriterWithURL:outputURL</span><br><span class="line">                                                      fileType:<span class="built_in">AVFileTypeQuickTimeMovie</span></span><br><span class="line">                                                         error:&amp;outError];</span><br><span class="line"><span class="built_in">BOOL</span> success = (assetWriter != <span class="literal">nil</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Setting-Up-the-Asset-Writer-Inputs-建立资产写入器入口"><a href="#Setting-Up-the-Asset-Writer-Inputs-建立资产写入器入口" class="headerlink" title="Setting Up the Asset Writer Inputs - 建立资产写入器入口"></a>Setting Up the Asset Writer Inputs - 建立资产写入器入口</h3><p>For your asset writer to be able to write media data, you must set up at least one asset writer input. For example, if your source of media data is already vending media samples as CMSampleBufferRef objects, just use the AVAssetWriterInput class. To set up an asset writer input that compresses audio media data to 128 kbps AAC and connect it to your asset writer, do the following:</p>
<p>为你的资产写入器能够写入媒体数据，必须至少设置一个资产写入器入口。例如，如果你的媒体数据源已经以 <code>CMSampleBufferRef</code> 对象声明了声明了媒体样本，只使用 <code>AVAssetWriterInput</code> 类。建立一个资产写入器入口，将音频媒体数据压缩到 <code>128 kbps AAC</code> 并且将它与你的资产写入器连接，跟着下面做：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Configure the channel layout as stereo.</span></span><br><span class="line">AudioChannelLayout stereoChannelLayout = &#123;</span><br><span class="line">    .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,</span><br><span class="line">    .mChannelBitmap = <span class="number">0</span>,</span><br><span class="line">    .mNumberChannelDescriptions = <span class="number">0</span></span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Convert the channel layout object to an NSData object.</span></span><br><span class="line"><span class="built_in">NSData</span> *channelLayoutAsData = [<span class="built_in">NSData</span> dataWithBytes:&amp;stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Get the compression settings for 128 kbps AAC.</span></span><br><span class="line"><span class="built_in">NSDictionary</span> *compressionAudioSettings = @&#123;</span><br><span class="line">    <span class="built_in">AVFormatIDKey</span>         : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kAudioFormat<span class="built_in">MPEG4AAC</span>],</span><br><span class="line">    <span class="built_in">AVEncoderBitRateKey</span>   : [<span class="built_in">NSNumber</span> numberWithInteger:<span class="number">128000</span>],</span><br><span class="line">    <span class="built_in">AVSampleRateKey</span>       : [<span class="built_in">NSNumber</span> numberWithInteger:<span class="number">44100</span>],</span><br><span class="line">    <span class="built_in">AVChannelLayoutKey</span>    : channelLayoutAsData,</span><br><span class="line">    <span class="built_in">AVNumberOfChannelsKey</span> : [<span class="built_in">NSNumber</span> numberWithUnsignedInteger:<span class="number">2</span>]</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Create the asset writer input with the compression settings and specify the media type as audio.</span></span><br><span class="line"><span class="built_in">AVAssetWriterInput</span> *assetWriterInput = [<span class="built_in">AVAssetWriterInput</span> assetWriterInputWithMediaType:<span class="built_in">AVMediaTypeAudio</span> outputSettings:compressionAudioSettings];</span><br><span class="line"><span class="comment">// Add the input to the writer if possible.</span></span><br><span class="line"><span class="keyword">if</span> ([assetWriter canAddInput:assetWriterInput])</span><br><span class="line">    [assetWriter addInput:assetWriterInput];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: If you want the media data to be written in the format in which it was stored, pass nil in the outputSettings parameter. Pass nil only if the asset writer was initialized with a fileType of AVFileTypeQuickTimeMovie.</p>
<p>注意：如果你想让媒体数据以它被存储的格式写入，给 <code>outputSettings</code> 参数传 <code>nil</code>。只有资产写入器曾用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVFileTypeQuickTimeMovie" target="_blank" rel="external">AVFileTypeQuickTimeMovie</a> 的 <code>fileType</code> 初始化，才传<code>nil</code> 。</p>
</blockquote>
<p>Your asset writer input can optionally include some metadata or specify a different transform for a particular track using the metadata and transform properties respectively. For an asset writer input whose data source is a video track, you can maintain the video’s original transform in the output file by doing the following:</p>
<p>你的资产写入器入口可以选择性的包含一些元数据 或者 分别使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriterInput_Class/index.html#//apple_ref/occ/instm/AVAssetWriterInput/metadata" target="_blank" rel="external">metadata</a> 和 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriterInput_Class/index.html#//apple_ref/occ/instm/AVAssetWriterInput/transform" target="_blank" rel="external">transform</a> 属性为特定的轨道指定不同的变换。对于一个资产写入器的入口，其数据源是一个视频轨道，可以通过下面示例来在输出文件中维持视频的原始变换：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVAsset</span> *videoAsset = &lt;<span class="meta">#AVAsset with at least one video track#&gt;;</span></span><br><span class="line"><span class="built_in">AVAssetTrack</span> *videoAssetTrack = [[videoAsset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>] objectAtIndex:<span class="number">0</span>];</span><br><span class="line">assetWriterInput.transform = videoAssetTrack.preferredTransform;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Set the metadata and transform properties before you begin writing with your asset writer for them to take effect.</p>
<p>注意：在开始用资产写入器写入生效之前，先设置 <code>metadata</code> 和 <code>transform</code> 属性。</p>
</blockquote>
<p>When writing media data to the output file, sometimes you may want to allocate pixel buffers. To do so, use the AVAssetWriterInputPixelBufferAdaptor class. For greatest efficiency, instead of adding pixel buffers that were allocated using a separate pool, use the pixel buffer pool provided by the pixel buffer adaptor. The following code creates a pixel buffer object working in the RGB domain that will use CGImage objects to create its pixel buffers.</p>
<p>当将媒体数据写入输出文件时，有时你可能要分配像素缓冲区。这样做：使用 <code>AVAssetWriterInputPixelBufferAdaptor</code> 类。为了最大的效率，使用由像素缓冲适配器提供的像素缓冲池，代替添加被分配使用一个单独池的像素缓冲区。下面的代码创建一个像素缓冲区对象，在 <code>RGB</code> 色彩下工作，将使用 <a href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIImage_Class/index.html#//apple_ref/occ/instm/UIImage/CGImage" target="_blank" rel="external">CGImage</a> 对象创建它的像素缓冲。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSDictionary</span> *pixelBufferAttributes = @&#123;</span><br><span class="line">     kCVPixelBuffer<span class="built_in">CGImageCompatibilityKey</span> : [<span class="built_in">NSNumber</span> numberWithBool:<span class="literal">YES</span>],</span><br><span class="line">     kCVPixelBuffer<span class="built_in">CGBitmapContextCompatibilityKey</span> : [<span class="built_in">NSNumber</span> numberWithBool:<span class="literal">YES</span>],</span><br><span class="line">     kCVPixelBufferPixelFormatTypeKey : [<span class="built_in">NSNumber</span> numberWithInt:kCVPixelFormatType_32ARGB]</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">AVAssetWriterInputPixelBufferAdaptor</span> *inputPixelBufferAdaptor = [<span class="built_in">AVAssetWriterInputPixelBufferAdaptor</span> assetWriterInputPixelBufferAdaptorWithAssetWriterInput:<span class="keyword">self</span>.assetWriterInput sourcePixelBufferAttributes:pixelBufferAttributes];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: All AVAssetWriterInputPixelBufferAdaptor objects must be connected to a single asset writer input. That asset writer input must accept media data of type AVMediaTypeVideo.</p>
<p>注：所有的 <code>AVAssetWriterInputPixelBufferAdaptor</code> 对象必须连接到一个单独的资产写入器入口。资产写入器入口必须接受 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVFoundation_Constants/index.html#//apple_ref/c/data/AVMediaTypeVideo" target="_blank" rel="external">AVMediaTypeVideo</a> 类型的媒体数据。</p>
</blockquote>
<h3 id="Writing-Media-Data-写入媒体数据"><a href="#Writing-Media-Data-写入媒体数据" class="headerlink" title="Writing Media Data - 写入媒体数据"></a>Writing Media Data - 写入媒体数据</h3><p>When you have configured all of the inputs needed for your asset writer, you are ready to begin writing media data. As you did with the asset reader, initiate the writing process with a call to the startWriting method. You then need to start a sample-writing session with a call to the startSessionAtSourceTime: method. All writing done by an asset writer has to occur within one of these sessions and the time range of each session defines the time range of media data included from within the source. For example, if your source is an asset reader that is supplying media data read from an AVAsset object and you don’t want to include media data from the first half of the asset, you would do the following:</p>
<p>当你已经为资产写入器配置所有需要的入口时，这时已经准备好开始写入媒体数据。正如在资产读取器所做的，调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/instm/AVAssetWriter/startWriting" target="_blank" rel="external">startWriting</a> 方法发起写入过程。然后你需要启动一个样本 – 调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/instm/AVAssetWriter/startSessionAtSourceTime:" target="_blank" rel="external">startSessionAtSourceTime:</a> 方法的写入会话。资产写入器的所有写入都必须在这些会话中发生，并且每个会话的时间范围 定义 包含在来源内媒体数据的时间范围。例如，如果你的来源是一个资产读取器（它从 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/cl/AVAsset" target="_blank" rel="external">AVAsset</a> 对象读取到供应的媒体数据），并且你不想包含来自资产的前半部分的媒体数据，你可以像下面这样做：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMTime halfAssetDuration = CMTimeMultiplyByFloat64(<span class="keyword">self</span>.asset.duration, <span class="number">0.5</span>);</span><br><span class="line">[<span class="keyword">self</span>.assetWriter startSessionAtSourceTime:halfAssetDuration];</span><br><span class="line"><span class="comment">//Implementation continues.</span></span><br></pre></td></tr></table></figure>
<p>Normally, to end a writing session you must call the endSessionAtSourceTime: method. However, if your writing session goes right up to the end of your file, you can end the writing session simply by calling the finishWriting method. To start up an asset writer with a single input and write all of its media data, do the following:</p>
<p>通常，必须调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/instm/AVAssetWriter/endSessionAtSourceTime:" target="_blank" rel="external">endSessionAtSourceTime:</a> 方法结束写入会话。然而，如果你的写入会话正确走到了你的文件末尾，可以简单地通过调用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/index.html#//apple_ref/occ/instm/AVAssetWriter/finishWriting" target="_blank" rel="external">finishWriting</a> 方法来结束写入会话。要启动一个有单一入口的资产写入器并且写入所有媒体数据。下面示例：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Prepare the asset writer for writing.</span></span><br><span class="line">[<span class="keyword">self</span>.assetWriter startWriting];</span><br><span class="line"><span class="comment">// Start a sample-writing session.</span></span><br><span class="line">[<span class="keyword">self</span>.assetWriter startSessionAtSourceTime:kCMTimeZero];</span><br><span class="line"><span class="comment">// Specify the block to execute when the asset writer is ready for media data and the queue to call it on.</span></span><br><span class="line">[<span class="keyword">self</span>.assetWriterInput requestMediaDataWhenReadyOnQueue:myInputSerialQueue usingBlock:^&#123;</span><br><span class="line">     <span class="keyword">while</span> ([<span class="keyword">self</span>.assetWriterInput isReadyForMoreMediaData])</span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// Get the next sample buffer.</span></span><br><span class="line">          CMSampleBufferRef nextSampleBuffer = [<span class="keyword">self</span> copyNextSampleBufferToWrite];</span><br><span class="line">          <span class="keyword">if</span> (nextSampleBuffer)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If it exists, append the next sample buffer to the output file.</span></span><br><span class="line">               [<span class="keyword">self</span>.assetWriterInput appendSampleBuffer:nextSampleBuffer];</span><br><span class="line">               <span class="built_in">CFRelease</span>(nextSampleBuffer);</span><br><span class="line">               nextSampleBuffer = <span class="literal">nil</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// Assume that lack of a next sample buffer means the sample buffer source is out of samples and mark the input as finished.</span></span><br><span class="line">               [<span class="keyword">self</span>.assetWriterInput markAsFinished];</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">          &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<p>The copyNextSampleBufferToWrite method in the code above is simply a stub. The location of this stub is where you would need to insert some logic to return CMSampleBufferRef objects representing the media data that you want to write. One possible source of sample buffers is an asset reader output.</p>
<p>上述代码中的 <code>copyNextSampleBufferToWrite</code> 方法仅仅是一个 <code>stub</code>。这个 <code>stub</code> 的位置就是你需要插入一些逻辑 去返回 <code>CMSampleBufferRef</code> 对象 表示你想要写入的媒体数据。示例缓冲区的可能来源是一个资产读取器出口。</p>
<h2 id="Reencoding-Assets-重新编码资产"><a href="#Reencoding-Assets-重新编码资产" class="headerlink" title="Reencoding Assets - 重新编码资产"></a>Reencoding Assets - 重新编码资产</h2><p>You can use an asset reader and asset writer object in tandem to convert an asset from one representation to another. Using these objects, you have more control over the conversion than you do with an AVAssetExportSession object. For example, you can choose which of the tracks you want to be represented in the output file, specify your own output format, or modify the asset during the conversion process. The first step in this process is just to set up your asset reader outputs and asset writer inputs as desired. After your asset reader and writer are fully configured, you start up both of them with calls to the startReading and startWriting methods, respectively. The following code snippet displays how to use a single asset writer input to write media data supplied by a single asset reader output:</p>
<p>可以使用资产读取器和资产写入器对象，以一个表现转换到另一个表现的资产。使用这些对象，你必须比用 <code>AVAssetExportSession</code> 对象有更多的控制转换。例如，你可以选择输出文件中想要显示的轨道，指定你自己的输出格式，或者在转换过程中修改该资产。这个过程中第一步是按需建立你的资产读取器出口和资产写入器入口。资产读取器和写入器充分配置后，分别调用 <code>startReading</code> 和 <code>startWriting</code> 方法启动它们。下面的代码片段显示了如何使用一个单一的资产写入器入口去写入 由一个单一的资产读取器出口提供的媒体数据：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSString</span> *serializationQueueDescription = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@"%@ serialization queue"</span>, <span class="keyword">self</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Create a serialization queue for reading and writing.</span></span><br><span class="line"><span class="built_in">dispatch_queue_t</span> serializationQueue = dispatch_queue_create([serializationQueueDescription UTF8String], <span class="literal">NULL</span>);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Specify the block to execute when the asset writer is ready for media data and the queue to call it on.</span></span><br><span class="line">[<span class="keyword">self</span>.assetWriterInput requestMediaDataWhenReadyOnQueue:serializationQueue usingBlock:^&#123;</span><br><span class="line">     <span class="keyword">while</span> ([<span class="keyword">self</span>.assetWriterInput isReadyForMoreMediaData])</span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// Get the asset reader output's next sample buffer.</span></span><br><span class="line">          CMSampleBufferRef sampleBuffer = [<span class="keyword">self</span>.assetReaderOutput copyNextSampleBuffer];</span><br><span class="line">          <span class="keyword">if</span> (sampleBuffer != <span class="literal">NULL</span>)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If it exists, append this sample buffer to the output file.</span></span><br><span class="line">               <span class="built_in">BOOL</span> success = [<span class="keyword">self</span>.assetWriterInput appendSampleBuffer:sampleBuffer];</span><br><span class="line">               <span class="built_in">CFRelease</span>(sampleBuffer);</span><br><span class="line">               sampleBuffer = <span class="literal">NULL</span>;</span><br><span class="line">               <span class="comment">// Check for errors that may have occurred when appending the new sample buffer.</span></span><br><span class="line">               <span class="keyword">if</span> (!success &amp;&amp; <span class="keyword">self</span>.assetWriter.status == <span class="built_in">AVAssetWriterStatusFailed</span>)</span><br><span class="line">               &#123;</span><br><span class="line">                    <span class="built_in">NSError</span> *failureError = <span class="keyword">self</span>.assetWriter.error;</span><br><span class="line">                    <span class="comment">//Handle the error.</span></span><br><span class="line">               &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If the next sample buffer doesn't exist, find out why the asset reader output couldn't vend another one.</span></span><br><span class="line">               <span class="keyword">if</span> (<span class="keyword">self</span>.assetReader.status == <span class="built_in">AVAssetReaderStatusFailed</span>)</span><br><span class="line">               &#123;</span><br><span class="line">                    <span class="built_in">NSError</span> *failureError = <span class="keyword">self</span>.assetReader.error;</span><br><span class="line">                    <span class="comment">//Handle the error here.</span></span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">else</span></span><br><span class="line">               &#123;</span><br><span class="line">                    <span class="comment">// The asset reader output must have vended all of its samples. Mark the input as finished.</span></span><br><span class="line">                    [<span class="keyword">self</span>.assetWriterInput markAsFinished];</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">          &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<h2 id="Putting-It-All-Together-Using-an-Asset-Reader-and-Writer-in-Tandem-to-Reencode-an-Asset-总结：使用资产读取器和写入器串联重新编码资产"><a href="#Putting-It-All-Together-Using-an-Asset-Reader-and-Writer-in-Tandem-to-Reencode-an-Asset-总结：使用资产读取器和写入器串联重新编码资产" class="headerlink" title="Putting It All Together: Using an Asset Reader and Writer in Tandem to Reencode an Asset - 总结：使用资产读取器和写入器串联重新编码资产"></a>Putting It All Together: Using an Asset Reader and Writer in Tandem to Reencode an Asset - 总结：使用资产读取器和写入器串联重新编码资产</h2><p>This brief code example illustrates how to use an asset reader and writer to reencode the first video and audio track of an asset into a new file. It shows how to:</p>
<ul>
<li>Use serialization queues to handle the asynchronous nature of reading and writing audiovisual data</li>
<li>Initialize an asset reader and configure two asset reader outputs, one for audio and one for video</li>
<li>Initialize an asset writer and configure two asset writer inputs, one for audio and one for video</li>
<li>Use an asset reader to asynchronously supply media data to an asset writer through two different - output/input combinations</li>
<li>Use a dispatch group to be notified of completion of the reencoding process</li>
<li>Allow a user to cancel the reencoding process once it has begun</li>
</ul>
<p>这个剪短的代码示例说明如何使用资产读取器和写入器将一个资产的第一个视频和音频轨道重新编码 到一个新文件。它展示了：</p>
<ul>
<li>使用序列化队列来处理读写视听数据的异步性</li>
<li>初始化一个资产读取器，并配置两个资产读取器出口，一个用于音频，一个用于视频</li>
<li>初始化一个资产写入器，并配置两个资产写入器入口，一个用于音频，一个用于视频</li>
<li>使用一个资产读取器，通过两个不同的 输出/输入组合来异步向资产写入器提供媒体数据</li>
<li>使用一个调度组接收重新编码过程的完成的通知</li>
<li>一旦开始，允许用户取消重新编码过程</li>
</ul>
<blockquote>
<p>Note: To focus on the most relevant code, this example omits several aspects of a complete application. To use AVFoundation, you are expected to have enough experience with Cocoa to be able to infer the missing pieces.</p>
<p>注：关注最相关的代码，这个例子中省略了一个完成应用程序的几个方面。为了使用 <code>AVFoundation</code> ，希望你有足够的 <code>Cocoa</code> 经验，能够推断缺少的代码。</p>
</blockquote>
<h3 id="Handling-the-Initial-Setup-处理初始设置"><a href="#Handling-the-Initial-Setup-处理初始设置" class="headerlink" title="Handling the Initial Setup - 处理初始设置"></a>Handling the Initial Setup - 处理初始设置</h3><p>Before you create your asset reader and writer and configure their outputs and inputs, you need to handle some initial setup. The first part of this setup involves creating three separate serialization queues to coordinate the reading and writing process.</p>
<p>在创建资产读取器和写入器和配置它们的出口和入口之前，你需要处理一下初始设置。此设置的第一部分包括创建3个独立的序列化队列来协调读写过程。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">NSString</span> *serializationQueueDescription = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@"%@ serialization queue"</span>, <span class="keyword">self</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Create the main serialization queue.</span></span><br><span class="line"><span class="keyword">self</span>.mainSerializationQueue = dispatch_queue_create([serializationQueueDescription UTF8String], <span class="literal">NULL</span>);</span><br><span class="line"><span class="built_in">NSString</span> *rwAudioSerializationQueueDescription = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@"%@ rw audio serialization queue"</span>, <span class="keyword">self</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Create the serialization queue to use for reading and writing the audio data.</span></span><br><span class="line"><span class="keyword">self</span>.rwAudioSerializationQueue = dispatch_queue_create([rwAudioSerializationQueueDescription UTF8String], <span class="literal">NULL</span>);</span><br><span class="line"><span class="built_in">NSString</span> *rwVideoSerializationQueueDescription = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@"%@ rw video serialization queue"</span>, <span class="keyword">self</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Create the serialization queue to use for reading and writing the video data.</span></span><br><span class="line"><span class="keyword">self</span>.rwVideoSerializationQueue = dispatch_queue_create([rwVideoSerializationQueueDescription UTF8String], <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure>
<p>The main serialization queue is used to coordinate the starting and stopping of the asset reader and writer (perhaps due to cancellation) and the other two serialization queues are used to serialize the reading and writing by each output/input combination with a potential cancellation.</p>
<p>主序列队列用于协调资产读取器和写入器（可能是由于注销）的启动和停止，其他两个序列队列用于序列化读取器和写入器，通过每一个有潜在注销的输入/输出组合。</p>
<p>Now that you have some serialization queues, load the tracks of your asset and begin the reencoding process.</p>
<p>现在你有一些序列化队列，加载你的资产轨道，并开始重新编码过程。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">self</span>.asset = &lt;<span class="meta">#AVAsset that you want to reencode#&gt;;</span></span><br><span class="line"><span class="keyword">self</span>.cancelled = <span class="literal">NO</span>;</span><br><span class="line"><span class="keyword">self</span>.outputURL = &lt;<span class="meta">#NSURL representing desired output URL for file generated by asset writer#&gt;;</span></span><br><span class="line"><span class="comment">// Asynchronously load the tracks of the asset you want to read.</span></span><br><span class="line">[<span class="keyword">self</span>.asset loadValuesAsynchronouslyForKeys:@[<span class="string">@"tracks"</span>] completionHandler:^&#123;</span><br><span class="line">     <span class="comment">// Once the tracks have finished loading, dispatch the work to the main serialization queue.</span></span><br><span class="line">     <span class="built_in">dispatch_async</span>(<span class="keyword">self</span>.mainSerializationQueue, ^&#123;</span><br><span class="line">          <span class="comment">// Due to asynchronous nature, check to see if user has already cancelled.</span></span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">self</span>.cancelled)</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          <span class="built_in">BOOL</span> success = <span class="literal">YES</span>;</span><br><span class="line">          <span class="built_in">NSError</span> *localError = <span class="literal">nil</span>;</span><br><span class="line">          <span class="comment">// Check for success of loading the assets tracks.</span></span><br><span class="line">          success = ([<span class="keyword">self</span>.asset statusOfValueForKey:<span class="string">@"tracks"</span> error:&amp;localError] == <span class="built_in">AVKeyValueStatusLoaded</span>);</span><br><span class="line">          <span class="keyword">if</span> (success)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If the tracks loaded successfully, make sure that no file exists at the output path for the asset writer.</span></span><br><span class="line">               <span class="built_in">NSFileManager</span> *fm = [<span class="built_in">NSFileManager</span> defaultManager];</span><br><span class="line">               <span class="built_in">NSString</span> *localOutputPath = [<span class="keyword">self</span>.outputURL path];</span><br><span class="line">               <span class="keyword">if</span> ([fm fileExistsAtPath:localOutputPath])</span><br><span class="line">                    success = [fm removeItemAtPath:localOutputPath error:&amp;localError];</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (success)</span><br><span class="line">               success = [<span class="keyword">self</span> setupAssetReaderAndAssetWriter:&amp;localError];</span><br><span class="line">          <span class="keyword">if</span> (success)</span><br><span class="line">               success = [<span class="keyword">self</span> startAssetReaderAndWriter:&amp;localError];</span><br><span class="line">          <span class="keyword">if</span> (!success)</span><br><span class="line">               [<span class="keyword">self</span> readingAndWritingDidFinishSuccessfully:success withError:localError];</span><br><span class="line">     &#125;);</span><br><span class="line">&#125;];</span><br></pre></td></tr></table></figure>
<p>When the track loading process finishes, whether successfully or not, the rest of the work is dispatched to the main serialization queue to ensure that all of this work is serialized with a potential cancellation. Now all that’s left is to implement the cancellation process and the three custom methods at the end of the previous code listing.</p>
<p>当轨道加载过程结束后，无论成功与否，剩下的工作就是被分配到主序列队列以确保所有的工作都是有潜在注销的序列化。现在，剩下就是实现注销进程和前面的代码清单的结尾处的3个自定义方法。</p>
<h3 id="Initializing-the-Asset-Reader-and-Writer-初始化资产读取器和写入器"><a href="#Initializing-the-Asset-Reader-and-Writer-初始化资产读取器和写入器" class="headerlink" title="Initializing the Asset Reader and Writer - 初始化资产读取器和写入器"></a>Initializing the Asset Reader and Writer - 初始化资产读取器和写入器</h3><p>The custom setupAssetReaderAndAssetWriter: method initializes the reader and writer and configures two output/input combinations, one for an audio track and one for a video track. In this example, the audio is decompressed to Linear PCM using the asset reader and compressed back to 128 kbps AAC using the asset writer. The video is decompressed to YUV using the asset reader and compressed to H.264 using the asset writer.</p>
<p>自定义 <code>setupAssetReaderAndAssetWriter:</code> 方法初始化读取器和写入器，并且配置两个输入/输出组合，一个用于音频轨道，一个用于视频轨道。在这个例子中，使用资产读取器音频被解压缩到 <code>Linear PCM</code> ，使用资产写入器压缩回 <code>128 kbps AAC</code> 。使用资产读取器将视频解压缩到 <code>YUV</code> ，使用资产写入器压缩为 <code>H.264</code> 。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- (<span class="built_in">BOOL</span>)setupAssetReaderAndAssetWriter:(<span class="built_in">NSError</span> **)outError</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Create and initialize the asset reader.</span></span><br><span class="line">    <span class="keyword">self</span>.assetReader = [[<span class="built_in">AVAssetReader</span> alloc] initWithAsset:<span class="keyword">self</span>.asset error:outError];</span><br><span class="line">    <span class="built_in">BOOL</span> success = (<span class="keyword">self</span>.assetReader != <span class="literal">nil</span>);</span><br><span class="line">    <span class="keyword">if</span> (success)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// If the asset reader was successfully initialized, do the same for the asset writer.</span></span><br><span class="line">        <span class="keyword">self</span>.assetWriter = [[<span class="built_in">AVAssetWriter</span> alloc] initWithURL:<span class="keyword">self</span>.outputURL</span><br><span class="line">                                                     fileType:<span class="built_in">AVFileTypeQuickTimeMovie</span></span><br><span class="line">                                                        error:outError];</span><br><span class="line">        success = (<span class="keyword">self</span>.assetWriter != <span class="literal">nil</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (success)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// If the reader and writer were successfully initialized, grab the audio and video asset tracks that will be used.</span></span><br><span class="line">        <span class="built_in">AVAssetTrack</span> *assetAudioTrack = <span class="literal">nil</span>, *assetVideoTrack = <span class="literal">nil</span>;</span><br><span class="line">        <span class="built_in">NSArray</span> *audioTracks = [<span class="keyword">self</span>.asset tracksWithMediaType:<span class="built_in">AVMediaTypeAudio</span>];</span><br><span class="line">        <span class="keyword">if</span> ([audioTracks count] &gt; <span class="number">0</span>)</span><br><span class="line">            assetAudioTrack = [audioTracks objectAtIndex:<span class="number">0</span>];</span><br><span class="line">        <span class="built_in">NSArray</span> *videoTracks = [<span class="keyword">self</span>.asset tracksWithMediaType:<span class="built_in">AVMediaTypeVideo</span>];</span><br><span class="line">        <span class="keyword">if</span> ([videoTracks count] &gt; <span class="number">0</span>)</span><br><span class="line">            assetVideoTrack = [videoTracks objectAtIndex:<span class="number">0</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (assetAudioTrack)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// If there is an audio track to read, set the decompression settings to Linear PCM and create the asset reader output.</span></span><br><span class="line">            <span class="built_in">NSDictionary</span> *decompressionAudioSettings = @&#123; <span class="built_in">AVFormatIDKey</span> : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kAudioFormatLinearPCM] &#125;;</span><br><span class="line">            <span class="keyword">self</span>.assetReaderAudioOutput = [<span class="built_in">AVAssetReaderTrackOutput</span> assetReaderTrackOutputWithTrack:assetAudioTrack</span><br><span class="line">                                                                                     outputSettings:decompressionAudioSettings];</span><br><span class="line">            [<span class="keyword">self</span>.assetReader addOutput:<span class="keyword">self</span>.assetReaderAudioOutput];</span><br><span class="line">            <span class="comment">// Then, set the compression settings to 128kbps AAC and create the asset writer input.</span></span><br><span class="line">            AudioChannelLayout stereoChannelLayout = &#123;</span><br><span class="line">                .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,</span><br><span class="line">                .mChannelBitmap = <span class="number">0</span>,</span><br><span class="line">                .mNumberChannelDescriptions = <span class="number">0</span></span><br><span class="line">            &#125;;</span><br><span class="line">            <span class="built_in">NSData</span> *channelLayoutAsData = [<span class="built_in">NSData</span> dataWithBytes:&amp;stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];</span><br><span class="line">            <span class="built_in">NSDictionary</span> *compressionAudioSettings = @&#123;</span><br><span class="line">                                                       <span class="built_in">AVFormatIDKey</span>         : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kAudioFormat<span class="built_in">MPEG4AAC</span>],</span><br><span class="line">                                                       <span class="built_in">AVEncoderBitRateKey</span>   : [<span class="built_in">NSNumber</span> numberWithInteger:<span class="number">128000</span>],</span><br><span class="line">                                                       <span class="built_in">AVSampleRateKey</span>       : [<span class="built_in">NSNumber</span> numberWithInteger:<span class="number">44100</span>],</span><br><span class="line">                                                       <span class="built_in">AVChannelLayoutKey</span>    : channelLayoutAsData,</span><br><span class="line">                                                       <span class="built_in">AVNumberOfChannelsKey</span> : [<span class="built_in">NSNumber</span> numberWithUnsignedInteger:<span class="number">2</span>]</span><br><span class="line">                                                       &#125;;</span><br><span class="line">            <span class="keyword">self</span>.assetWriterAudioInput = [<span class="built_in">AVAssetWriterInput</span> assetWriterInputWithMediaType:[assetAudioTrack mediaType]</span><br><span class="line">                                                                            outputSettings:compressionAudioSettings];</span><br><span class="line">            [<span class="keyword">self</span>.assetWriter addInput:<span class="keyword">self</span>.assetWriterAudioInput];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (assetVideoTrack)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// If there is a video track to read, set the decompression settings for YUV and create the asset reader output.</span></span><br><span class="line">            <span class="built_in">NSDictionary</span> *decompressionVideoSettings = @&#123;</span><br><span class="line">                                                         (<span class="keyword">id</span>)kCVPixelBufferPixelFormatTypeKey     : [<span class="built_in">NSNumber</span> numberWithUnsignedInt:kCVPixelFormatType_422YpCbCr8],</span><br><span class="line">                                                         (<span class="keyword">id</span>)kCVPixelBufferIOSurfacePropertiesKey : [<span class="built_in">NSDictionary</span> dictionary]</span><br><span class="line">                                                         &#125;;</span><br><span class="line">            <span class="keyword">self</span>.assetReaderVideoOutput = [<span class="built_in">AVAssetReaderTrackOutput</span> assetReaderTrackOutputWithTrack:assetVideoTrack</span><br><span class="line">                                                                                     outputSettings:decompressionVideoSettings];</span><br><span class="line">            [<span class="keyword">self</span>.assetReader addOutput:<span class="keyword">self</span>.assetReaderVideoOutput];</span><br><span class="line">            CMFormatDescriptionRef formatDescription = <span class="literal">NULL</span>;</span><br><span class="line">            <span class="comment">// Grab the video format descriptions from the video track and grab the first one if it exists.</span></span><br><span class="line">            <span class="built_in">NSArray</span> *videoFormatDescriptions = [assetVideoTrack formatDescriptions];</span><br><span class="line">            <span class="keyword">if</span> ([videoFormatDescriptions count] &gt; <span class="number">0</span>)</span><br><span class="line">                formatDescription = (__bridge CMFormatDescriptionRef)[formatDescriptions objectAtIndex:<span class="number">0</span>];</span><br><span class="line">            <span class="built_in">CGSize</span> trackDimensions = &#123;</span><br><span class="line">                .width = <span class="number">0.0</span>,</span><br><span class="line">                .height = <span class="number">0.0</span>,</span><br><span class="line">            &#125;;</span><br><span class="line">            <span class="comment">// If the video track had a format description, grab the track dimensions from there. Otherwise, grab them direcly from the track itself.</span></span><br><span class="line">            <span class="keyword">if</span> (formatDescription)</span><br><span class="line">                trackDimensions = CMVideoFormatDescriptionGetPresentationDimensions(formatDescription, <span class="literal">false</span>, <span class="literal">false</span>);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                trackDimensions = [assetVideoTrack naturalSize];</span><br><span class="line">            <span class="built_in">NSDictionary</span> *compressionSettings = <span class="literal">nil</span>;</span><br><span class="line">            <span class="comment">// If the video track had a format description, attempt to grab the clean aperture settings and pixel aspect ratio used by the video.</span></span><br><span class="line">            <span class="keyword">if</span> (formatDescription)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">NSDictionary</span> *cleanAperture = <span class="literal">nil</span>;</span><br><span class="line">                <span class="built_in">NSDictionary</span> *pixelAspectRatio = <span class="literal">nil</span>;</span><br><span class="line">                <span class="built_in">CFDictionaryRef</span> cleanApertureFromCMFormatDescription = CMFormatDescriptionGetExtension(formatDescription, kCMFormatDescriptionExtension_CleanAperture);</span><br><span class="line">                <span class="keyword">if</span> (cleanApertureFromCMFormatDescription)</span><br><span class="line">                &#123;</span><br><span class="line">                    cleanAperture = @&#123;</span><br><span class="line">                                      <span class="built_in">AVVideoCleanApertureWidthKey</span>            : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureWidth),</span><br><span class="line">                                      <span class="built_in">AVVideoCleanApertureHeightKey</span>           : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureHeight),</span><br><span class="line">                                      <span class="built_in">AVVideoCleanApertureHorizontalOffsetKey</span> : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureHorizontalOffset),</span><br><span class="line">                                      <span class="built_in">AVVideoCleanApertureVerticalOffsetKey</span>   : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureVerticalOffset)</span><br><span class="line">                                      &#125;;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">CFDictionaryRef</span> pixelAspectRatioFromCMFormatDescription = CMFormatDescriptionGetExtension(formatDescription, kCMFormatDescriptionExtension_PixelAspectRatio);</span><br><span class="line">                <span class="keyword">if</span> (pixelAspectRatioFromCMFormatDescription)</span><br><span class="line">                &#123;</span><br><span class="line">                    pixelAspectRatio = @&#123;</span><br><span class="line">                                         <span class="built_in">AVVideoPixelAspectRatioHorizontalSpacingKey</span> : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(pixelAspectRatioFromCMFormatDescription, kCMFormatDescriptionKey_PixelAspectRatioHorizontalSpacing),</span><br><span class="line">                                         <span class="built_in">AVVideoPixelAspectRatioVerticalSpacingKey</span>   : (<span class="keyword">id</span>)<span class="built_in">CFDictionaryGetValue</span>(pixelAspectRatioFromCMFormatDescription, kCMFormatDescriptionKey_PixelAspectRatioVerticalSpacing)</span><br><span class="line">                                         &#125;;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// Add whichever settings we could grab from the format description to the compression settings dictionary.</span></span><br><span class="line">                <span class="keyword">if</span> (cleanAperture || pixelAspectRatio)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">NSMutableDictionary</span> *mutableCompressionSettings = [<span class="built_in">NSMutableDictionary</span> dictionary];</span><br><span class="line">                    <span class="keyword">if</span> (cleanAperture)</span><br><span class="line">                        [mutableCompressionSettings setObject:cleanAperture forKey:<span class="built_in">AVVideoCleanApertureKey</span>];</span><br><span class="line">                    <span class="keyword">if</span> (pixelAspectRatio)</span><br><span class="line">                        [mutableCompressionSettings setObject:pixelAspectRatio forKey:<span class="built_in">AVVideoPixelAspectRatioKey</span>];</span><br><span class="line">                    compressionSettings = mutableCompressionSettings;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Create the video settings dictionary for H.264.</span></span><br><span class="line">            <span class="built_in">NSMutableDictionary</span> *videoSettings = (<span class="built_in">NSMutableDictionary</span> *) @&#123;</span><br><span class="line">                                                                           <span class="built_in">AVVideoCodecKey</span>  : <span class="built_in">AVVideoCodecH264</span>,</span><br><span class="line">                                                                           <span class="built_in">AVVideoWidthKey</span>  : [<span class="built_in">NSNumber</span> numberWithDouble:trackDimensions.width],</span><br><span class="line">                                                                           <span class="built_in">AVVideoHeightKey</span> : [<span class="built_in">NSNumber</span> numberWithDouble:trackDimensions.height]</span><br><span class="line">                                                                           &#125;;</span><br><span class="line">            <span class="comment">// Put the compression settings into the video settings dictionary if we were able to grab them.</span></span><br><span class="line">            <span class="keyword">if</span> (compressionSettings)</span><br><span class="line">                [videoSettings setObject:compressionSettings forKey:<span class="built_in">AVVideoCompressionPropertiesKey</span>];</span><br><span class="line">            <span class="comment">// Create the asset writer input and add it to the asset writer.</span></span><br><span class="line">            <span class="keyword">self</span>.assetWriterVideoInput = [<span class="built_in">AVAssetWriterInput</span> assetWriterInputWithMediaType:[videoTrack mediaType]</span><br><span class="line">                                                                            outputSettings:videoSettings];</span><br><span class="line">            [<span class="keyword">self</span>.assetWriter addInput:<span class="keyword">self</span>.assetWriterVideoInput];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> success;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Reencoding-the-Asset-重新编码资产"><a href="#Reencoding-the-Asset-重新编码资产" class="headerlink" title="Reencoding the Asset - 重新编码资产"></a>Reencoding the Asset - 重新编码资产</h3><p>Provided that the asset reader and writer are successfully initialized and configured, the startAssetReaderAndWriter: method described in Handling the Initial Setup is called. This method is where the actual reading and writing of the asset takes place.</p>
<p>如果资产读取器和写入器成功地初始化和配置，在 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/05_Export.html#//apple_ref/doc/uid/TP40010188-CH9-SW1" target="_blank" rel="external">Handling the Initial Setup</a> 中发现调用 <code>startAssetReaderAndWriter:</code> 方法。这个方法实际上是资产读写发生的地方。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="built_in">BOOL</span>)startAssetReaderAndWriter:(<span class="built_in">NSError</span> **)outError</span><br><span class="line">&#123;</span><br><span class="line">     <span class="built_in">BOOL</span> success = <span class="literal">YES</span>;</span><br><span class="line">     <span class="comment">// Attempt to start the asset reader.</span></span><br><span class="line">     success = [<span class="keyword">self</span>.assetReader startReading];</span><br><span class="line">     <span class="keyword">if</span> (!success)</span><br><span class="line">          *outError = [<span class="keyword">self</span>.assetReader error];</span><br><span class="line">     <span class="keyword">if</span> (success)</span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// If the reader started successfully, attempt to start the asset writer.</span></span><br><span class="line">          success = [<span class="keyword">self</span>.assetWriter startWriting];</span><br><span class="line">          <span class="keyword">if</span> (!success)</span><br><span class="line">               *outError = [<span class="keyword">self</span>.assetWriter error];</span><br><span class="line">     &#125;</span><br><span class="line"> </span><br><span class="line">     <span class="keyword">if</span> (success)</span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// If the asset reader and writer both started successfully, create the dispatch group where the reencoding will take place and start a sample-writing session.</span></span><br><span class="line">          <span class="keyword">self</span>.dispatchGroup = dispatch_group_create();</span><br><span class="line">          [<span class="keyword">self</span>.assetWriter startSessionAtSourceTime:kCMTimeZero];</span><br><span class="line">          <span class="keyword">self</span>.audioFinished = <span class="literal">NO</span>;</span><br><span class="line">          <span class="keyword">self</span>.videoFinished = <span class="literal">NO</span>;</span><br><span class="line"> </span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">self</span>.assetWriterAudioInput)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If there is audio to reencode, enter the dispatch group before beginning the work.</span></span><br><span class="line">               dispatch_group_enter(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">               <span class="comment">// Specify the block to execute when the asset writer is ready for audio media data, and specify the queue to call it on.</span></span><br><span class="line">               [<span class="keyword">self</span>.assetWriterAudioInput requestMediaDataWhenReadyOnQueue:<span class="keyword">self</span>.rwAudioSerializationQueue usingBlock:^&#123;</span><br><span class="line">                    <span class="comment">// Because the block is called asynchronously, check to see whether its task is complete.</span></span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">self</span>.audioFinished)</span><br><span class="line">                         <span class="keyword">return</span>;</span><br><span class="line">                    <span class="built_in">BOOL</span> completedOrFailed = <span class="literal">NO</span>;</span><br><span class="line">                    <span class="comment">// If the task isn't complete yet, make sure that the input is actually ready for more media data.</span></span><br><span class="line">                    <span class="keyword">while</span> ([<span class="keyword">self</span>.assetWriterAudioInput isReadyForMoreMediaData] &amp;&amp; !completedOrFailed)</span><br><span class="line">                    &#123;</span><br><span class="line">                         <span class="comment">// Get the next audio sample buffer, and append it to the output file.</span></span><br><span class="line">                         CMSampleBufferRef sampleBuffer = [<span class="keyword">self</span>.assetReaderAudioOutput copyNextSampleBuffer];</span><br><span class="line">                         <span class="keyword">if</span> (sampleBuffer != <span class="literal">NULL</span>)</span><br><span class="line">                         &#123;</span><br><span class="line">                              <span class="built_in">BOOL</span> success = [<span class="keyword">self</span>.assetWriterAudioInput appendSampleBuffer:sampleBuffer];</span><br><span class="line">                              <span class="built_in">CFRelease</span>(sampleBuffer);</span><br><span class="line">                              sampleBuffer = <span class="literal">NULL</span>;</span><br><span class="line">                              completedOrFailed = !success;</span><br><span class="line">                         &#125;</span><br><span class="line">                         <span class="keyword">else</span></span><br><span class="line">                         &#123;</span><br><span class="line">                              completedOrFailed = <span class="literal">YES</span>;</span><br><span class="line">                         &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (completedOrFailed)</span><br><span class="line">                    &#123;</span><br><span class="line">                         <span class="comment">// Mark the input as finished, but only if we haven't already done so, and then leave the dispatch group (since the audio work has finished).</span></span><br><span class="line">                         <span class="built_in">BOOL</span> oldFinished = <span class="keyword">self</span>.audioFinished;</span><br><span class="line">                         <span class="keyword">self</span>.audioFinished = <span class="literal">YES</span>;</span><br><span class="line">                         <span class="keyword">if</span> (oldFinished == <span class="literal">NO</span>)</span><br><span class="line">                         &#123;</span><br><span class="line">                              [<span class="keyword">self</span>.assetWriterAudioInput markAsFinished];</span><br><span class="line">                         &#125;</span><br><span class="line">                         dispatch_group_leave(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">                    &#125;</span><br><span class="line">               &#125;];</span><br><span class="line">          &#125;</span><br><span class="line"> </span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">self</span>.assetWriterVideoInput)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// If we had video to reencode, enter the dispatch group before beginning the work.</span></span><br><span class="line">               dispatch_group_enter(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">               <span class="comment">// Specify the block to execute when the asset writer is ready for video media data, and specify the queue to call it on.</span></span><br><span class="line">               [<span class="keyword">self</span>.assetWriterVideoInput requestMediaDataWhenReadyOnQueue:<span class="keyword">self</span>.rwVideoSerializationQueue usingBlock:^&#123;</span><br><span class="line">                    <span class="comment">// Because the block is called asynchronously, check to see whether its task is complete.</span></span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">self</span>.videoFinished)</span><br><span class="line">                         <span class="keyword">return</span>;</span><br><span class="line">                    <span class="built_in">BOOL</span> completedOrFailed = <span class="literal">NO</span>;</span><br><span class="line">                    <span class="comment">// If the task isn't complete yet, make sure that the input is actually ready for more media data.</span></span><br><span class="line">                    <span class="keyword">while</span> ([<span class="keyword">self</span>.assetWriterVideoInput isReadyForMoreMediaData] &amp;&amp; !completedOrFailed)</span><br><span class="line">                    &#123;</span><br><span class="line">                         <span class="comment">// Get the next video sample buffer, and append it to the output file.</span></span><br><span class="line">                         CMSampleBufferRef sampleBuffer = [<span class="keyword">self</span>.assetReaderVideoOutput copyNextSampleBuffer];</span><br><span class="line">                         <span class="keyword">if</span> (sampleBuffer != <span class="literal">NULL</span>)</span><br><span class="line">                         &#123;</span><br><span class="line">                              <span class="built_in">BOOL</span> success = [<span class="keyword">self</span>.assetWriterVideoInput appendSampleBuffer:sampleBuffer];</span><br><span class="line">                              <span class="built_in">CFRelease</span>(sampleBuffer);</span><br><span class="line">                              sampleBuffer = <span class="literal">NULL</span>;</span><br><span class="line">                              completedOrFailed = !success;</span><br><span class="line">                         &#125;</span><br><span class="line">                         <span class="keyword">else</span></span><br><span class="line">                         &#123;</span><br><span class="line">                              completedOrFailed = <span class="literal">YES</span>;</span><br><span class="line">                         &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (completedOrFailed)</span><br><span class="line">                    &#123;</span><br><span class="line">                         <span class="comment">// Mark the input as finished, but only if we haven't already done so, and then leave the dispatch group (since the video work has finished).</span></span><br><span class="line">                         <span class="built_in">BOOL</span> oldFinished = <span class="keyword">self</span>.videoFinished;</span><br><span class="line">                         <span class="keyword">self</span>.videoFinished = <span class="literal">YES</span>;</span><br><span class="line">                         <span class="keyword">if</span> (oldFinished == <span class="literal">NO</span>)</span><br><span class="line">                         &#123;</span><br><span class="line">                              [<span class="keyword">self</span>.assetWriterVideoInput markAsFinished];</span><br><span class="line">                         &#125;</span><br><span class="line">                         dispatch_group_leave(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">                    &#125;</span><br><span class="line">               &#125;];</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// Set up the notification that the dispatch group will send when the audio and video work have both finished.</span></span><br><span class="line">          dispatch_group_notify(<span class="keyword">self</span>.dispatchGroup, <span class="keyword">self</span>.mainSerializationQueue, ^&#123;</span><br><span class="line">               <span class="built_in">BOOL</span> finalSuccess = <span class="literal">YES</span>;</span><br><span class="line">               <span class="built_in">NSError</span> *finalError = <span class="literal">nil</span>;</span><br><span class="line">               <span class="comment">// Check to see if the work has finished due to cancellation.</span></span><br><span class="line">               <span class="keyword">if</span> (<span class="keyword">self</span>.cancelled)</span><br><span class="line">               &#123;</span><br><span class="line">                    <span class="comment">// If so, cancel the reader and writer.</span></span><br><span class="line">                    [<span class="keyword">self</span>.assetReader cancelReading];</span><br><span class="line">                    [<span class="keyword">self</span>.assetWriter cancelWriting];</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">else</span></span><br><span class="line">               &#123;</span><br><span class="line">                    <span class="comment">// If cancellation didn't occur, first make sure that the asset reader didn't fail.</span></span><br><span class="line">                    <span class="keyword">if</span> ([<span class="keyword">self</span>.assetReader status] == <span class="built_in">AVAssetReaderStatusFailed</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                         finalSuccess = <span class="literal">NO</span>;</span><br><span class="line">                         finalError = [<span class="keyword">self</span>.assetReader error];</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// If the asset reader didn't fail, attempt to stop the asset writer and check for any errors.</span></span><br><span class="line">                    <span class="keyword">if</span> (finalSuccess)</span><br><span class="line">                    &#123;</span><br><span class="line">                         finalSuccess = [<span class="keyword">self</span>.assetWriter finishWriting];</span><br><span class="line">                         <span class="keyword">if</span> (!finalSuccess)</span><br><span class="line">                              finalError = [<span class="keyword">self</span>.assetWriter error];</span><br><span class="line">                    &#125;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="comment">// Call the method to handle completion, and pass in the appropriate parameters to indicate whether reencoding was successful.</span></span><br><span class="line">               [<span class="keyword">self</span> readingAndWritingDidFinishSuccessfully:finalSuccess withError:finalError];</span><br><span class="line">          &#125;);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Return success here to indicate whether the asset reader and writer were started successfully.</span></span><br><span class="line">     <span class="keyword">return</span> success;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>During reencoding, the audio and video tracks are asynchronously handled on individual serialization queues to increase the overall performance of the process, but both queues are contained within the same dispatch group. By placing the work for each track within the same dispatch group, the group can send a notification when all of the work is done and the success of the reencoding process can be determined.</p>
<p>重新编码期间，音频和视频轨道是在各自的串行队形上异步处理，来增加进程的整体性能，但两个队列包含在同一调度组中。为同一调度组内的每个轨道安排工作，当所有的工作完成，并能够确定重新编码过程的成功，该组可以发送一个通知。</p>
<h3 id="Handling-Completion-处理完成"><a href="#Handling-Completion-处理完成" class="headerlink" title="Handling Completion - 处理完成"></a>Handling Completion - 处理完成</h3><p>To handle the completion of the reading and writing process, the readingAndWritingDidFinishSuccessfully: method is called—with parameters indicating whether or not the reencoding completed successfully. If the process didn’t finish successfully, the asset reader and writer are both canceled and any UI related tasks are dispatched to the main queue.</p>
<p>处理读写进程的完成，<code>readingAndWritingDidFinishSuccessfully:</code> 方法被调用，带着参数，指出重新编码是否成功完成。如果进程没有成功完成，该资产读取器和写入器都被取消，任何 <code>UI</code> 相关的任何都被发送到主队列中。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)readingAndWritingDidFinishSuccessfully:(<span class="built_in">BOOL</span>)success withError:(<span class="built_in">NSError</span> *)error</span><br><span class="line">&#123;</span><br><span class="line">     <span class="keyword">if</span> (!success)</span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// If the reencoding process failed, we need to cancel the asset reader and writer.</span></span><br><span class="line">          [<span class="keyword">self</span>.assetReader cancelReading];</span><br><span class="line">          [<span class="keyword">self</span>.assetWriter cancelWriting];</span><br><span class="line">          <span class="built_in">dispatch_async</span>(dispatch_get_main_queue(), ^&#123;</span><br><span class="line">               <span class="comment">// Handle any UI tasks here related to failure.</span></span><br><span class="line">          &#125;);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">     &#123;</span><br><span class="line">          <span class="comment">// Reencoding was successful, reset booleans.</span></span><br><span class="line">          <span class="keyword">self</span>.cancelled = <span class="literal">NO</span>;</span><br><span class="line">          <span class="keyword">self</span>.videoFinished = <span class="literal">NO</span>;</span><br><span class="line">          <span class="keyword">self</span>.audioFinished = <span class="literal">NO</span>;</span><br><span class="line">          <span class="built_in">dispatch_async</span>(dispatch_get_main_queue(), ^&#123;</span><br><span class="line">               <span class="comment">// Handle any UI tasks here related to success.</span></span><br><span class="line">          &#125;);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Handling-Cancellation-处理注销"><a href="#Handling-Cancellation-处理注销" class="headerlink" title="Handling Cancellation - 处理注销"></a>Handling Cancellation - 处理注销</h3><p>Using multiple serialization queues, you can allow the user of your app to cancel the reencoding process with ease. On the main serialization queue, messages are asynchronously sent to each of the asset reencoding serialization queues to cancel their reading and writing. When these two serialization queues complete their cancellation, the dispatch group sends a notification to the main serialization queue where the cancelled property is set to YES. You might associate the cancel method from the following code listing with a button on your UI.</p>
<p>使用多个序列化队列，你可以提供方便，让你的应用程序的用户取消重新编码进程。在主串行队列，消息被异步发送到每个资产重编码序列化队列，来取消它们的读写。当这两个序列化队列完成它们的注销，调度组向主序列化队列（<code>cancelled</code> 属性被设置为 <code>YES</code>）发送一个通知.你可能从下面的代码将 <code>cancel</code> 方法与 <code>UI</code> 上的按钮关联起来。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)cancel</span><br><span class="line">&#123;</span><br><span class="line">     <span class="comment">// Handle cancellation asynchronously, but serialize it with the main queue.</span></span><br><span class="line">     <span class="built_in">dispatch_async</span>(<span class="keyword">self</span>.mainSerializationQueue, ^&#123;</span><br><span class="line">          <span class="comment">// If we had audio data to reencode, we need to cancel the audio work.</span></span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">self</span>.assetWriterAudioInput)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// Handle cancellation asynchronously again, but this time serialize it with the audio queue.</span></span><br><span class="line">               <span class="built_in">dispatch_async</span>(<span class="keyword">self</span>.rwAudioSerializationQueue, ^&#123;</span><br><span class="line">                    <span class="comment">// Update the Boolean property indicating the task is complete and mark the input as finished if it hasn't already been marked as such.</span></span><br><span class="line">                    <span class="built_in">BOOL</span> oldFinished = <span class="keyword">self</span>.audioFinished;</span><br><span class="line">                    <span class="keyword">self</span>.audioFinished = <span class="literal">YES</span>;</span><br><span class="line">                    <span class="keyword">if</span> (oldFinished == <span class="literal">NO</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                         [<span class="keyword">self</span>.assetWriterAudioInput markAsFinished];</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// Leave the dispatch group since the audio work is finished now.</span></span><br><span class="line">                    dispatch_group_leave(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">               &#125;);</span><br><span class="line">          &#125;</span><br><span class="line"> </span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">self</span>.assetWriterVideoInput)</span><br><span class="line">          &#123;</span><br><span class="line">               <span class="comment">// Handle cancellation asynchronously again, but this time serialize it with the video queue.</span></span><br><span class="line">               <span class="built_in">dispatch_async</span>(<span class="keyword">self</span>.rwVideoSerializationQueue, ^&#123;</span><br><span class="line">                    <span class="comment">// Update the Boolean property indicating the task is complete and mark the input as finished if it hasn't already been marked as such.</span></span><br><span class="line">                    <span class="built_in">BOOL</span> oldFinished = <span class="keyword">self</span>.videoFinished;</span><br><span class="line">                    <span class="keyword">self</span>.videoFinished = <span class="literal">YES</span>;</span><br><span class="line">                    <span class="keyword">if</span> (oldFinished == <span class="literal">NO</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                         [<span class="keyword">self</span>.assetWriterVideoInput markAsFinished];</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// Leave the dispatch group, since the video work is finished now.</span></span><br><span class="line">                    dispatch_group_leave(<span class="keyword">self</span>.dispatchGroup);</span><br><span class="line">               &#125;);</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// Set the cancelled Boolean property to YES to cancel any work on the main queue as well.</span></span><br><span class="line">          <span class="keyword">self</span>.cancelled = <span class="literal">YES</span>;</span><br><span class="line">     &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Asset-Output-Settings-Assistant-资产出口设置助手"><a href="#Asset-Output-Settings-Assistant-资产出口设置助手" class="headerlink" title="Asset Output Settings Assistant - 资产出口设置助手"></a>Asset Output Settings Assistant - 资产出口设置助手</h2><p>The AVOutputSettingsAssistant class aids in creating output-settings dictionaries for an asset reader or writer. This makes setup much simpler, especially for high frame rate H264 movies that have a number of specific presets. Listing 5-1 shows an example that uses the output settings assistant to use the settings assistant.</p>
<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVOutputSettingsAssistant_Class/index.html#//apple_ref/occ/cl/AVOutputSettingsAssistant" target="_blank" rel="external">AVOutputSettingsAssistant</a> 类在创建出口时能帮上忙 – 为资产读取器或者写入器设置字典。这使得设置更简单，特别是对于有一些具体的预设的高帧速率 <code>H264</code> 影片。 <code>Listing 5-1</code> 显示了使用输出设置助手去使用设置助手的例子。</p>
<p>Listing 5-1  AVOutputSettingsAssistant sample</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AVOutputSettingsAssistant</span> *outputSettingsAssistant = [<span class="built_in">AVOutputSettingsAssistant</span> outputSettingsAssistantWithPreset:&lt;some preset&gt;];</span><br><span class="line">CMFormatDescriptionRef audioFormat = [<span class="keyword">self</span> getAudioFormat];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (audioFormat != <span class="literal">NULL</span>)</span><br><span class="line">    [outputSettingsAssistant setSourceAudioFormat:(CMAudioFormatDescriptionRef)audioFormat];</span><br><span class="line"> </span><br><span class="line">CMFormatDescriptionRef videoFormat = [<span class="keyword">self</span> getVideoFormat];</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (videoFormat != <span class="literal">NULL</span>)</span><br><span class="line">    [outputSettingsAssistant setSourceVideoFormat:(CMVideoFormatDescriptionRef)videoFormat];</span><br><span class="line"> </span><br><span class="line">CMTime assetMinVideoFrameDuration = [<span class="keyword">self</span> getMinFrameDuration];</span><br><span class="line">CMTime averageFrameDuration = [<span class="keyword">self</span> getAvgFrameDuration]</span><br><span class="line"> </span><br><span class="line">[outputSettingsAssistant setSourceVideoAverageFrameDuration:averageFrameDuration];</span><br><span class="line">[outputSettingsAssistant setSourceVideoMinFrameDuration:assetMinVideoFrameDuration];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">AVAssetWriter</span> *assetWriter = [<span class="built_in">AVAssetWriter</span> assetWriterWithURL:&lt;some URL&gt; fileType:[outputSettingsAssistant outputFileType] error:<span class="literal">NULL</span>];</span><br><span class="line"><span class="built_in">AVAssetWriterInput</span> *audioInput = [<span class="built_in">AVAssetWriterInput</span> assetWriterInputWithMediaType:<span class="built_in">AVMediaTypeAudio</span> outputSettings:[outputSettingsAssistant audioSettings] sourceFormatHint:audioFormat];</span><br><span class="line"><span class="built_in">AVAssetWriterInput</span> *videoInput = [<span class="built_in">AVAssetWriterInput</span> assetWriterInputWithMediaType:<span class="built_in">AVMediaTypeVideo</span> outputSettings:[outputSettingsAssistant videoSettings] sourceFormatHint:videoFormat];</span><br></pre></td></tr></table></figure>
<h1 id="Time-and-Media-Representations-时间和媒体表现"><a href="#Time-and-Media-Representations-时间和媒体表现" class="headerlink" title="Time and Media Representations - 时间和媒体表现"></a>Time and Media Representations - 时间和媒体表现</h1><p>Time-based audiovisual data, such as a movie file or a video stream, is represented in the AV Foundation framework by AVAsset. Its structure dictates much of the framework works. Several low-level data structures that AV Foundation uses to represent time and media such as sample buffers come from the Core Media framework.</p>
<p>基于视听资料的时间，比如一个电影文件或视频流，在<code>AV Foundation</code> 框架中是由 <code>AVAsset</code> 代表的。它的结构决定了大部分的框架工程。一些低层的数据结构（<code>AV Foundation</code> 使用来表示时间和媒体，比如样本缓冲区）来自 <code>Core Media framework</code>。</p>
<h2 id="Representation-of-Assets-资产的表示"><a href="#Representation-of-Assets-资产的表示" class="headerlink" title="Representation of Assets - 资产的表示"></a>Representation of Assets - 资产的表示</h2><p>AVAsset is the core class in the AV Foundation framework. It provides a format-independent abstraction of time-based audiovisual data, such as a movie file or a video stream. The primary relationships are shown in Figure 6-1. In many cases, you work with one of its subclasses: You use the composition subclasses when you create new assets (see Editing), and you use AVURLAsset to create a new asset instance from media at a given URL (including assets from the MPMedia framework or the Asset Library framework—see Using Assets).</p>
<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAsset_Class/index.html#//apple_ref/occ/cl/AVAsset" target="_blank" rel="external">AVAsset</a> 是 <code>AV Foundation</code> 框架的核心类。它提供了一个格式 – 与基于时间的视听数据的抽象无关，比如电影文件或视频流。主要的关系如图 6-1所示。在很多情况下，你都与它的一个子类一起工作：当你创建新的资产（见 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW1" target="_blank" rel="external">Editing</a>）使用组件的子类，并使用 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVURLAsset_Class/index.html#//apple_ref/occ/cl/AVURLAsset" target="_blank" rel="external">AVURLAsset</a> 从给定 <code>URL</code> 的媒体来创建一个新的资产实例。（包括来自 <code>MPMedia</code> 框架或者 <code>Asset Library framework</code> 的资产，见<a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1" target="_blank" rel="external">Using Assets</a>）</p>
<center><br>    <img src="http://ww1.sinaimg.cn/large/a9c4d5f6gw1f6kx28dgzuj20lc0cojri.jpg" alt="Figure 6-1  AVAsset provides an abstraction of time-based audiovisual data"><br></center>

<p>An asset contains a collection of tracks that are intended to be presented or processed together, each of a uniform media type, including (but not limited to) audio, video, text, closed captions, and subtitles. The asset object provides information about whole resource, such as its duration or title, as well as hints for presentation, such as its natural size. Assets may also have metadata, represented by instances of AVMetadataItem.</p>
<p>资产包含了一组轨道，旨在被一起呈现或一起处理，每一个统一的媒体类型，包括（但不仅限于）音频、视频、文本、隐藏式字幕，以及字幕。资产对象提供关于整个资源的信息，比如它的持续时间或标题，以及用于呈现提示的信息，例如它的自然尺寸。资产也有可能拥有元数据，通过 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMetadataItem_Class/index.html#//apple_ref/occ/cl/AVMetadataItem" target="_blank" rel="external">AVMetadataItem</a> 的实例表示。</p>
<p>A track is represented by an instance of AVAssetTrack, as shown in Figure 6-2. In a typical simple case, one track represents the audio component and another represents the video component; in a complex composition, there may be multiple overlapping tracks of audio and video.</p>
<p>轨道由 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrack_Class/index.html#//apple_ref/occ/cl/AVAssetTrack" target="_blank" rel="external">AVAssetTrack</a> 的实例表示，如图 6-2所示。在一个典型简单的情况下，一个轨道代表代表音频组件，另一个代表视频组件；在复杂的组成中，可以存在音频和视频的多个重叠的轨道。</p>
<center><br>    <img src="http://ww3.sinaimg.cn/large/a9c4d5f6gw1f6kxhx9jerj20n009f0st.jpg" alt="Figure 6-2  AVAssetTrack"><br></center>

<p>A track has a number of properties, such as its type (video or audio), visual and/or audible characteristics (as appropriate), metadata, and timeline (expressed in terms of its parent asset). A track also has an array of format descriptions. The array contains CMFormatDescription objects (see CMFormatDescriptionRef), each of which describes the format of media samples referenced by the track. A track that contains uniform media (for example, all encoded using to the same settings) will provide an array with a count of 1.</p>
<p>轨道有许多属性，比如它的类型（视频或者音频），视觉和/或听觉特性（根据需要），元数据和时间轴（在其父资产表示）。一个轨道也有格式描述的数组。数组包含 <code>CMFormatDescription</code> 对象（见 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMFormatDescription/index.html#//apple_ref/c/tdef/CMFormatDescriptionRef" target="_blank" rel="external">CMFormatDescriptionRef</a>），其中每一个都描述了轨道引用的媒体样本的格式。包含了统一媒体的轨道（例如，所有使用相同设置的编码）将提供计数为 <code>1</code> 的数组。</p>
<p>A track may itself be divided into segments, represented by instances of AVAssetTrackSegment. A segment is a time mapping from the source to the asset track timeline.</p>
<p>轨道自身可以被分成几段，由 <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetTrackSegment_Class/index.html#//apple_ref/occ/cl/AVAssetTrackSegment" target="_blank" rel="external">AVAssetTrackSegment</a> 的实例表示。一个片段是一个时间映射，从资源到资产轨道时间轴的映射。</p>
<h1 id="Representations-of-Time-时间的表示"><a href="#Representations-of-Time-时间的表示" class="headerlink" title="Representations of Time - 时间的表示"></a>Representations of Time - 时间的表示</h1><p>Time in AV Foundation is represented by primitive structures from the Core Media framework.</p>
<p><code>AV Foundation</code> 中的时间是由来自 <code>Core Media framework</code> 的原始结构体表示的。</p>
<h3 id="CMTime-Represents-a-Length-of-Time-CMTime-表示时间的长度"><a href="#CMTime-Represents-a-Length-of-Time-CMTime-表示时间的长度" class="headerlink" title="CMTime Represents a Length of Time - CMTime 表示时间的长度"></a>CMTime Represents a Length of Time - <code>CMTime</code> 表示时间的长度</h3><p>CMTime is a C structure that represents time as a rational number, with a numerator (an int64_t value), and a denominator (an int32_t timescale). Conceptually, the timescale specifies the fraction of a second each unit in the numerator occupies. Thus if the timescale is 4, each unit represents a quarter of a second; if the timescale is 10, each unit represents a tenth of a second, and so on. You frequently use a timescale of 600, because this is a multiple of several commonly used frame rates: 24 fps for film, 30 fps for NTSC (used for TV in North America and Japan), and 25 fps for PAL (used for TV in Europe). Using a timescale of 600, you can exactly represent any number of frames in these systems.</p>
<p><a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/tdef/CMTime" target="_blank" rel="external">CMTime</a> 是一个C语言的结构体，以一个有理数表示时间，有一个分子（一个 <code>int64_t</code> 值）和一个分母（一个 <code>int32_t</code> 时间刻度）。在概念上讲，时间刻度指定一秒中每个单元占据的分数。因此如果时间刻度为 <code>4</code>，每个单元代表一秒的四分之一；如果时间刻度为 <code>10</code>，每个单元代表一秒的十分之一，等等。经常使用时间刻度为 <code>600</code>，因为这是因为这是几种常用帧速率的倍数：<code>24 fps</code>的电影， <code>30 fps</code> 的<code>NTSC</code>（用在北美洲和日本的电视），<code>25 fps</code>的 <code>PAL</code>（用于欧洲电视）。使用 <code>600</code>的时间刻度，可以在这些系统中精确的表示任意数量的帧。</p>
<p>In addition to a simple time value, a CMTime structure can represent nonnumeric values: +infinity, -infinity, and indefinite. It can also indicate whether the time been rounded at some point, and it maintains an epoch number.</p>
<p>除了简单的时间值，<code>CMTime</code> 结构体可以表示非数字的值：正无穷大、负无穷大，不确定的。它也可以表示时间在哪一位约等于，并且它能保持一个纪元数字。</p>
<h4 id="Using-CMTime-使用-CMTime"><a href="#Using-CMTime-使用-CMTime" class="headerlink" title="Using CMTime - 使用 CMTime"></a>Using CMTime - 使用 <code>CMTime</code></h4><p>You create a time using CMTimeMake or one of the related functions such as CMTimeMakeWithSeconds (which allows you to create a time using a float value and specify a preferred timescale). There are several functions for time-based arithmetic and for comparing times, as illustrated in the following example:</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/func/CMTimeMake" target="_blank" rel="external">CMTimeMake</a> 或一个相关功能的 来创建一个时间，例如 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/func/CMTimeMakeWithSeconds" target="_blank" rel="external">CMTimeMakeWithSeconds</a> （它允许你使用浮点值来创建一个时间，并指定一个首选时间刻度）。有基于时间算术的和比较时间的几个功能，如下面的示例所示：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CMTime time1 = CMTimeMake(<span class="number">200</span>, <span class="number">2</span>); <span class="comment">// 200 half-seconds</span></span><br><span class="line">CMTime time2 = CMTimeMake(<span class="number">400</span>, <span class="number">4</span>); <span class="comment">// 400 quarter-seconds</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">// time1 and time2 both represent 100 seconds, but using different timescales.</span></span><br><span class="line"><span class="keyword">if</span> (CMTimeCompare(time1, time2) == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"time1 and time2 are the same"</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Float64 float64Seconds = <span class="number">200.0</span> / <span class="number">3</span>;</span><br><span class="line">CMTime time3 = CMTimeMakeWithSeconds(float64Seconds , <span class="number">3</span>); <span class="comment">// 66.66... third-seconds</span></span><br><span class="line">time3 = CMTimeMultiply(time3, <span class="number">3</span>);</span><br><span class="line"><span class="comment">// time3 now represents 200 seconds; next subtract time1 (100 seconds).</span></span><br><span class="line">time3 = CMTimeSubtract(time3, time1);</span><br><span class="line">CMTimeShow(time3);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (CMTIME_CO<span class="built_in">MPARE_INLINE</span>(time2, ==, time3)) &#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"time2 and time3 are the same"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>For a list of all the available functions, see CMTime Reference.</p>
<p>有关所有可用的功能列表，请参阅 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/uid/TP40009748" target="_blank" rel="external">CMTime Reference</a></p>
<h4 id="Special-Values-of-CMTime-CMTime-的特殊值"><a href="#Special-Values-of-CMTime-CMTime-的特殊值" class="headerlink" title="Special Values of CMTime - CMTime 的特殊值"></a>Special Values of CMTime - <code>CMTime</code> 的特殊值</h4><p>Core Media provides constants for special values: kCMTimeZero, kCMTimeInvalid, kCMTimePositiveInfinity, and kCMTimeNegativeInfinity. There are many ways in which a CMTime structure can, for example, represent a time that is invalid. To test whether a CMTime is valid, or a nonnumeric value, you should use an appropriate macro, such as CMTIME_IS_INVALID, CMTIME_IS_POSITIVE_INFINITY, or CMTIME_IS_INDEFINITE.</p>
<p><code>Core Media</code> 提供了特殊值的常量：<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/c_ref/kCMTimeZero" target="_blank" rel="external">kCMTimeZero</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/c_ref/kCMTimeInvalid" target="_blank" rel="external">kCMTimeInvalid</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/c_ref/kCMTimePositiveInfinity" target="_blank" rel="external">kCMTimePositiveInfinity</a>，以及 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/doc/c_ref/kCMTimeNegativeInfinity" target="_blank" rel="external">kCMTimeNegativeInfinity</a>。有许多方法，例如，其中 <code>CMTime</code> 结构体可以表示一个无效的时间。为了测试<code>CMTime</code> 是否是无效的，或者是一个非数字值，应该使用一个适当的宏，比如 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/macro/CMTIME_IS_INVALID" target="_blank" rel="external">CMTIME_IS_INVALID</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/macro/CMTIME_IS_POSITIVE_INFINITY" target="_blank" rel="external">CMTIME_IS_POSITIVE_INFINITY</a>，或者 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/macro/CMTIME_IS_INDEFINITE" target="_blank" rel="external">CMTIME_IS_INDEFINITE</a></p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CMTime myTime = &lt;<span class="meta">#Get a CMTime#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> (CMTIME_IS_INVALID(myTime)) &#123;</span><br><span class="line">    <span class="comment">// Perhaps treat this as an error; display a suitable alert to the user.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You should not compare the value of an arbitrary CMTime structure with kCMTimeInvalid.</p>
<p>你不应该将一个任意的 <code>CMTime</code> 结构体的值与 <code>kCMTimeInvalid</code> 比较。</p>
<h4 id="Representing-CMTime-as-an-Object-CMTime表示为一个对象"><a href="#Representing-CMTime-as-an-Object-CMTime表示为一个对象" class="headerlink" title="Representing CMTime as an Object - CMTime表示为一个对象"></a>Representing CMTime as an Object - <code>CMTime</code>表示为一个对象</h4><p>If you need to use CMTime structures in annotations or Core Foundation containers, you can convert a CMTime structure to and from a CFDictionary opaque type (see CFDictionaryRef) using the CMTimeCopyAsDictionary and CMTimeMakeFromDictionary functions, respectively. You can also get a string representation of a CMTime structure using the CMTimeCopyDescription function.</p>
<p>如果你需要在注释或者 <code>Core Foundation</code> 容器中使用 <code>CMTime</code> 结构体，可以使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/func/CMTimeCopyAsDictionary" target="_blank" rel="external">CMTimeCopyAsDictionary</a> 将 <code>CMTime</code> 结构体转换，使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/func/CMTimeMakeFromDictionary" target="_blank" rel="external">CMTimeMakeFromDictionary</a> 从一个 <code>CFDictionary</code> 不透明的类型（见 <a href="https://developer.apple.com/library/ios/documentation/CoreFoundation/Reference/CFDictionaryRef/index.html#//apple_ref/c/tdef/CFDictionaryRef" target="_blank" rel="external">CFDictionaryRef</a>）。使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTime/index.html#//apple_ref/c/func/CMTimeCopyDescription" target="_blank" rel="external">CMTimeCopyDescription</a> 函数可以得到一个 <code>CMTime</code> 结构体的字符串表示。</p>
<h4 id="Epochs-纪元"><a href="#Epochs-纪元" class="headerlink" title="Epochs - 纪元"></a>Epochs - 纪元</h4><p>The epoch number of a CMTime structure is usually set to 0, but you can use it to distinguish unrelated timelines. For example, the epoch could be incremented through each cycle using a presentation loop, to differentiate between time N in loop 0 and time N in loop 1.</p>
<p><code>CMTime</code> 结构体的纪元数量通常设置为 <code>0</code>，但是你可以用它来区分不相关的时间轴。例如，纪元可以通过使用演示循环每个周期递增，区分循环<code>0</code>中的时间 <code>N</code>与循环<code>1</code>中的时间 <code>N</code>。</p>
<h3 id="CMTimeRange-Represents-a-Time-Range-CMTimeRange表示一个时间范围"><a href="#CMTimeRange-Represents-a-Time-Range-CMTimeRange表示一个时间范围" class="headerlink" title="CMTimeRange Represents a Time Range - CMTimeRange表示一个时间范围"></a>CMTimeRange Represents a Time Range - <code>CMTimeRange</code>表示一个时间范围</h3><p>CMTimeRange is a C structure that has a start time and duration, both expressed as CMTime structures. A time range does not include the time that is the start time plus the duration.</p>
<p>You create a time range using CMTimeRangeMake or CMTimeRangeFromTimeToTime. There are constraints on the value of the CMTime epochs:</p>
<ul>
<li>CMTimeRange structures cannot span different epochs.</li>
<li>The epoch in a CMTime structure that represents a timestamp may be nonzero, but you can only - perform range operations (such as CMTimeRangeGetUnion) on ranges whose start fields have the - same epoch.</li>
<li>The epoch in a CMTime structure that represents a duration should always be 0, and the value must be nonnegative.</li>
</ul>
<p><a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/tdef/CMTimeRange" target="_blank" rel="external">CMTimeRange</a> 是一个 C语言结构体，有开始时间和持续时间，即表示为 <code>CMTime</code> 结构体。时间范围不包括开始时间加上持续时间。</p>
<p>使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeMake" target="_blank" rel="external">CMTimeRangeMake</a> 或者 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeFromTimeToTime" target="_blank" rel="external">CMTimeRangeFromTimeToTime</a> 创建一个时间范围。有关 <code>CMTime</code> 纪元的值，有一些约束：</p>
<ul>
<li><code>CMTimeRange</code> 结构体不能跨越不同的纪元。</li>
<li><code>CMTime</code> 结构体中的纪元，表示一个时间戳可能是非零，但你只能在其开始字段具有相同纪元的范围内执行范围操作（比如 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeGetUnion" target="_blank" rel="external">CMTimeRangeGetUnion</a>）。</li>
<li>在 <code>CMTime</code> 结构体中的纪元，表示持续时间应该总是为 <code>0</code>，并且值必须是非负数。</li>
</ul>
<h4 id="Working-with-Time-Ranges-与时间范围工作"><a href="#Working-with-Time-Ranges-与时间范围工作" class="headerlink" title="Working with Time Ranges - 与时间范围工作"></a>Working with Time Ranges - 与时间范围工作</h4><p>Core Media provides functions you can use to determine whether a time range contains a given time or other time range, to determine whether two time ranges are equal, and to calculate unions and intersections of time ranges, such as CMTimeRangeContainsTime, CMTimeRangeEqual, CMTimeRangeContainsTimeRange, and CMTimeRangeGetUnion.</p>
<p><code>Core Media</code> 提供了一些功能，可用于确定一个时间范围是否包含一个特定的时间或其他时间范围，确定两个时间范围是否相等，并计算时间范围的接口和相交范围，比如 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeContainsTime" target="_blank" rel="external">CMTimeRangeContainsTime</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeEqual" target="_blank" rel="external">CMTimeRangeEqual</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeContainsTimeRange" target="_blank" rel="external">CMTimeRangeContainsTimeRange</a>，以及 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeGetUnion" target="_blank" rel="external">CMTimeRangeGetUnion</a>。</p>
<p>Given that a time range does not include the time that is the start time plus the duration, the following expression always evaluates to false:</p>
<p>由于时间范围不包括开始时间加上持续时间，下面的表达式的结果总是为 <code>false</code>：</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CMTimeRangeContainsTime(range, CMTimeRangeGetEnd(range))</span><br></pre></td></tr></table></figure>
<p>For a list of all the available functions, see CMTimeRange Reference.</p>
<p>有关所有可用功能的列表，请参阅 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/doc/uid/TP40009749" target="_blank" rel="external">CMTimeRange Reference</a>。</p>
<h4 id="Special-Values-of-CMTimeRange-CMTimeRange-的特殊值"><a href="#Special-Values-of-CMTimeRange-CMTimeRange-的特殊值" class="headerlink" title="Special Values of CMTimeRange - CMTimeRange 的特殊值"></a>Special Values of CMTimeRange - <code>CMTimeRange</code> 的特殊值</h4><p>Core Media provides constants for a zero-length range and an invalid range, kCMTimeRangeZero and kCMTimeRangeInvalid, respectively. There are many ways, though in which a CMTimeRange structure can be invalid, or zero—or indefinite (if one of the CMTime structures is indefinite. If you need to test whether a CMTimeRange structure is valid, zero, or indefinite, you should use an appropriate macro: CMTIMERANGE_IS_VALID, CMTIMERANGE_IS_INVALID, CMTIMERANGE_IS_EMPTY, or CMTIMERANGE_IS_EMPTY.</p>
<p><code>Core Media</code> 分别提供一个长度为0的范围和一个无效范围，就是<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/data/kCMTimeRangeZero" target="_blank" rel="external">kCMTimeRangeZero</a> 和 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/data/kCMTimeRangeInvalid" target="_blank" rel="external">kCMTimeRangeInvalid</a>。有很多种方法，尽管 <code>CMTimeRange</code> 结构可以是无效的，或为零，或是不确定的（如果<code>CMTime</code> 结构是不确定的）。如果你需要测试 <code>`CMTimeRange</code> 结构体是否是有效的，零，或者不确定，你应该使用适当的宏：<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/macro/CMTIMERANGE_IS_VALID" target="_blank" rel="external">CMTIMERANGE_IS_VALID</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/macro/CMTIMERANGE_IS_INVALID" target="_blank" rel="external">CMTIMERANGE_IS_INVALID</a>，<a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/macro/CMTIMERANGE_IS_EMPTY" target="_blank" rel="external">CMTIMERANGE_IS_EMPTY</a>，或者 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/macro/CMTIMERANGE_IS_INDEFINITE" target="_blank" rel="external">CMTIMERANGE_IS_INDEFINITE</a>。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CMTimeRange myTimeRange = &lt;<span class="meta">#Get a CMTimeRange#&gt;;</span></span><br><span class="line"><span class="keyword">if</span> (CMTIMERANGE_IS_E<span class="built_in">MPTY</span>(myTimeRange)) &#123;</span><br><span class="line">    <span class="comment">// The time range is zero.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You should not compare the value of an arbitrary CMTimeRange structure with kCMTimeRangeInvalid.</p>
<p>你不应该将任意的 <code>CMTimeRange</code> 结构体的值与 <code>kCMTimeRangeInvalid</code>进行比较。</p>
<h4 id="Representing-a-CMTimeRange-Structure-as-an-Object-将-CMTimeRange-结构体表示为对象"><a href="#Representing-a-CMTimeRange-Structure-as-an-Object-将-CMTimeRange-结构体表示为对象" class="headerlink" title="Representing a CMTimeRange Structure as an Object - 将 CMTimeRange 结构体表示为对象"></a>Representing a CMTimeRange Structure as an Object - 将 <code>CMTimeRange</code> 结构体表示为对象</h4><p>If you need to use CMTimeRange structures in annotations or Core Foundation containers, you can convert a CMTimeRange structure to and from a CFDictionary opaque type (see CFDictionaryRef) using CMTimeRangeCopyAsDictionary and CMTimeRangeMakeFromDictionary, respectively. You can also get a string representation of a CMTime structure using the CMTimeRangeCopyDescription function.</p>
<p>如果你需要在注释或 <code>Core Foundation</code> 容器中使用 <code>CMTimeRange</code> 结构，可以使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeCopyAsDictionary" target="_blank" rel="external">CMTimeRangeCopyAsDictionary</a> 转换一个 <code>CMTimeRange</code> ，使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeMakeFromDictionary" target="_blank" rel="external">CMTimeRangeMakeFromDictionary</a> 从一个 <code>CFDictionary</code> 不透明类型 （见 <a href="https://developer.apple.com/library/ios/documentation/CoreFoundation/Reference/CFDictionaryRef/index.html#//apple_ref/c/tdef/CFDictionaryRef" target="_blank" rel="external">CFDictionaryRef</a>）。也可以 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMTimeRange/index.html#//apple_ref/c/func/CMTimeRangeCopyDescription" target="_blank" rel="external">CMTimeRangeCopyDescription</a> 功能得到 <code>CMTime</code> 结构的一个字符串表示。</p>
<h2 id="Representations-of-Media-媒体的表示"><a href="#Representations-of-Media-媒体的表示" class="headerlink" title="Representations of Media - 媒体的表示"></a>Representations of Media - 媒体的表示</h2><p>Video data and its associated metadata are represented in AV Foundation by opaque objects from the Core Media framework. Core Media represents video data using CMSampleBuffer (see CMSampleBufferRef). CMSampleBuffer is a Core Foundation-style opaque type; an instance contains the sample buffer for a frame of video data as a Core Video pixel buffer (see CVPixelBufferRef). You access the pixel buffer from a sample buffer using CMSampleBufferGetImageBuffer:</p>
<p>视频数据和其相关的元数据都是被 <code>AV Foundation</code> 中来自 <code>Core Media framework</code>的不透明对象表示。<code>Core Media</code> 表示视频数据 使用 <code>CMSampleBuffer</code>（见 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/tdef/CMSampleBufferRef" target="_blank" rel="external">CMSampleBufferRef</a>）。<code>CMSampleBuffer</code> 是 <code>Core Foundation</code> 风格的不透明类型；实例包含了用于作为<code>Core Video</code> 像素缓冲（见<a href="https://developer.apple.com/library/ios/documentation/QuartzCore/Reference/CVPixelBufferRef/index.html#//apple_ref/c/tdef/CVPixelBufferRef" target="_blank" rel="external">CVPixelBufferRef</a>）的视频数据的单帧样品缓冲区。使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/func/CMSampleBufferGetImageBuffer" target="_blank" rel="external">CMSampleBufferGetImageBuffer</a> 从一个样本缓冲区访问像素缓冲。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(&lt;<span class="meta">#A CMSampleBuffer#&gt;);</span></span><br></pre></td></tr></table></figure>
<p>From the pixel buffer, you can access the actual video data. For an example, see Converting CMSampleBuffer to a UIImage Object.</p>
<p>In addition to the video data, you can retrieve a number of other aspects of the video frame:</p>
<ul>
<li>Timing information. You get accurate timestamps for both the original presentation time and - the decode time using CMSampleBufferGetPresentationTimeStamp and - CMSampleBufferGetDecodeTimeStamp respectively.</li>
<li>Format information. The format information is encapsulated in a CMFormatDescription object (- see CMFormatDescriptionRef). From the format description, you can get for example the pixel - type and video dimensions using CMVideoFormatDescriptionGetCodecType and - CMVideoFormatDescriptionGetDimensions respectively.</li>
<li>Metadata. Metadata are stored in a dictionary as an attachment. You use CMGetAttachment to retrieve the dictionary:</li>
</ul>
<p>从像素缓冲区，可以访问实际的视频数据。有个例子，请参阅 <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW4" target="_blank" rel="external">Converting CMSampleBuffer to a UIImage Object</a>。</p>
<p>除了视频数据之外，可以从数据帧中检索多个其他方面的信息：</p>
<ul>
<li>定时信息。分别使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/func/CMSampleBufferGetPresentationTimeStamp" target="_blank" rel="external">CMSampleBufferGetPresentationTimeStamp</a> 和 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMSampleBuffer/index.html#//apple_ref/c/func/CMSampleBufferGetDecodeTimeStamp" target="_blank" rel="external">CMSampleBufferGetDecodeTimeStamp</a>为原来的呈现时间和解码时间，获取准确的时间戳。</li>
<li>格式信息。格式信息被封装在 <code>CMFormatDescription</code> 对象(见 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMFormatDescription/index.html#//apple_ref/c/tdef/CMFormatDescriptionRef" target="_blank" rel="external">CMFormatDescriptionRef</a>）。从格式的描述，分别使用 <code>CMVideoFormatDescriptionGetCodecType</code> 和 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMFormatDescription/index.html#//apple_ref/c/func/CMVideoFormatDescriptionGetDimensions" target="_blank" rel="external">CMVideoFormatDescriptionGetDimensions</a> 可以得到例如像素类型和视频尺寸。</li>
<li>元数据。元数据作为附件被存储在字典中。使用 <a href="https://developer.apple.com/library/ios/documentation/CoreMedia/Reference/CMAttachment/index.html#//apple_ref/c/func/CMGetAttachment" target="_blank" rel="external">CMGetAttachment</a> 去检索词典：</li>
</ul>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CMSampleBufferRef sampleBuffer = &lt;<span class="meta">#Get a sample buffer#&gt;;</span></span><br><span class="line"><span class="built_in">CFDictionaryRef</span> metadataDictionary =</span><br><span class="line">    CMGetAttachment(sampleBuffer, <span class="built_in">CFSTR</span>(<span class="string">"MetadataDictionary"</span>, <span class="literal">NULL</span>);</span><br><span class="line"><span class="keyword">if</span> (metadataDictionary) &#123;</span><br><span class="line">    <span class="comment">// Do something with the metadata.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Converting-CMSampleBuffer-to-a-UIImage-Object-将-CMSampleBuffer-转化为-UIImage-对象"><a href="#Converting-CMSampleBuffer-to-a-UIImage-Object-将-CMSampleBuffer-转化为-UIImage-对象" class="headerlink" title="Converting CMSampleBuffer to a UIImage Object - 将 CMSampleBuffer 转化为 UIImage 对象"></a>Converting CMSampleBuffer to a UIImage Object - 将 <code>CMSampleBuffer</code> 转化为 <code>UIImage</code> 对象</h2><p>The following code shows how you can convert a CMSampleBuffer to a UIImage object. You should consider your requirements carefully before using it. Performing the conversion is a comparatively expensive operation. It is appropriate to, for example, create a still image from a frame of video data taken every second or so. You should not use this as a means to manipulate every frame of video coming from a capture device in real time.</p>
<p>下面的代码展示了如何将一个 <code>CMSampleBuffer</code> 转化为一个 <code>UIImage</code> 对象。在使用它之前，应该仔细考虑你的要求。执行转换是一个相对昂贵的操作。例如，比较合适的是 从每一秒左右的视频数据的一帧创建一个静态图像。你不应该使用这个作为一种手段 去操作来自实时捕获设备的视频的每一帧。</p>
<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a UIImage from sample buffer data</span></span><br><span class="line">- (<span class="built_in">UIImage</span> *) imageFromSampleBuffer:(CMSampleBufferRef) sampleBuffer</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Get a CMSampleBuffer's Core Video image buffer for the media data</span></span><br><span class="line">    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);</span><br><span class="line">    <span class="comment">// Lock the base address of the pixel buffer</span></span><br><span class="line">    CVPixelBufferLockBaseAddress(imageBuffer, <span class="number">0</span>);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Get the number of bytes per row for the pixel buffer</span></span><br><span class="line">    <span class="keyword">void</span> *baseAddress = CVPixelBufferGetBaseAddress(imageBuffer);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Get the number of bytes per row for the pixel buffer</span></span><br><span class="line">    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);</span><br><span class="line">    <span class="comment">// Get the pixel buffer width and height</span></span><br><span class="line">    size_t width = CVPixelBufferGetWidth(imageBuffer);</span><br><span class="line">    size_t height = CVPixelBufferGetHeight(imageBuffer);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Create a device-dependent RGB color space</span></span><br><span class="line">    <span class="built_in">CGColorSpaceRef</span> colorSpace = <span class="built_in">CGColorSpaceCreateDeviceRGB</span>();</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Create a bitmap graphics context with the sample buffer data</span></span><br><span class="line">    <span class="built_in">CGContextRef</span> context = <span class="built_in">CGBitmapContextCreate</span>(baseAddress, width, height, <span class="number">8</span>,</span><br><span class="line">      bytesPerRow, colorSpace, k<span class="built_in">CGBitmapByteOrder32Little</span> | k<span class="built_in">CGImageAlphaPremultipliedFirst</span>);</span><br><span class="line">    <span class="comment">// Create a Quartz image from the pixel data in the bitmap graphics context</span></span><br><span class="line">    <span class="built_in">CGImageRef</span> quartzImage = <span class="built_in">CGBitmapContextCreateImage</span>(context);</span><br><span class="line">    <span class="comment">// Unlock the pixel buffer</span></span><br><span class="line">    CVPixelBufferUnlockBaseAddress(imageBuffer,<span class="number">0</span>);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Free up the context and color space</span></span><br><span class="line">    <span class="built_in">CGContextRelease</span>(context);</span><br><span class="line">    <span class="built_in">CGColorSpaceRelease</span>(colorSpace);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Create an image object from the Quartz image</span></span><br><span class="line">    <span class="built_in">UIImage</span> *image = [<span class="built_in">UIImage</span> imageWith<span class="built_in">CGImage</span>:quartzImage];</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Release the Quartz image</span></span><br><span class="line">    <span class="built_in">CGImageRelease</span>(quartzImage);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> (image);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后记：2016年8月7日，16：38，翻译至此结束：本文翻译的版本是官方文档2015-06-30版，也就是现在的最新版，翻译成果中还有许多需要校对的地方，希望查阅的小伙伴遇到问题能反馈给我。我也会在接下来的几天写 demo的同时，再次进行校对。感谢导师和leader，给我机会完成这项工作。</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/post/20160724AVFoundation/">AVFoundation Programming Guide(官方文档翻译)完整版中英对照</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">Yofer Zhang</a></p>
        <p><span>发布时间:</span>2016-07-24, 10:57:30</p>
        <p><span>最后更新:</span>2017-02-09, 13:33:08</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/post/20160724AVFoundation/" title="AVFoundation Programming Guide(官方文档翻译)完整版中英对照">http://yoferzhang.com/post/20160724AVFoundation/</a>
            <span class="copy-path" data-clipboard-text="原文: http://yoferzhang.com/post/20160724AVFoundation/　　作者: Yofer Zhang" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/post/20160803AVFoundation01Introduction/">
                    AVFoundation Programming Guide(官方文档翻译1)About AVFoundation - AVFoundation概述
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/post/20160426Matchismo/">
                    【iOS】Stanford iOS7 Assignment - Matchismo
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#About-AVFoundation-AVFoundation概述"><span class="toc-number">1.</span> <span class="toc-text">About AVFoundation - AVFoundation概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#At-a-Glance-摘要"><span class="toc-number">1.1.</span> <span class="toc-text">At a Glance - 摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Representing-and-Using-Media-with-AVFoundation-用AVFoundation-表示和使用媒体"><span class="toc-number">1.1.1.</span> <span class="toc-text">Representing and Using Media with AVFoundation - 用AVFoundation 表示和使用媒体</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Playback-播放"><span class="toc-number">1.1.2.</span> <span class="toc-text">Playback - 播放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reading-Writing-and-Reencoding-Assets-读取，写入和重新编码Assets"><span class="toc-number">1.1.3.</span> <span class="toc-text">Reading, Writing, and Reencoding Assets - 读取，写入和重新编码Assets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Thumbnails-缩略图"><span class="toc-number">1.1.4.</span> <span class="toc-text">Thumbnails - 缩略图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Editing-编辑"><span class="toc-number">1.1.5.</span> <span class="toc-text">Editing - 编辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Still-and-Video-Media-Capture-静态和视频媒体捕获"><span class="toc-number">1.1.6.</span> <span class="toc-text">Still and Video Media Capture - 静态和视频媒体捕获</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Concurrent-Programming-with-AVFoundation-AVFoundation并发编程"><span class="toc-number">1.1.7.</span> <span class="toc-text">Concurrent Programming with AVFoundation - AVFoundation并发编程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prerequisites-预备知识"><span class="toc-number">1.2.</span> <span class="toc-text">Prerequisites - 预备知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#See-Also-参考"><span class="toc-number">1.3.</span> <span class="toc-text">See Also - 参考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Using-Assets-使用Assets"><span class="toc-number">2.</span> <span class="toc-text">Using Assets - 使用Assets</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Creating-an-Asset-Object-创建一个Asset对象"><span class="toc-number">2.1.</span> <span class="toc-text">Creating an Asset Object - 创建一个Asset对象</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Options-for-Initializing-an-Asset-初始化一个Asset的选择"><span class="toc-number">2.1.1.</span> <span class="toc-text">Options for Initializing an Asset - 初始化一个Asset的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Accessing-the-User’s-Assets-访问用户的Assets"><span class="toc-number">2.1.2.</span> <span class="toc-text">Accessing the User’s Assets - 访问用户的Assets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Preparing-an-Asset-for-Use-将-Asset-准备好使用"><span class="toc-number">2.2.</span> <span class="toc-text">Preparing an Asset for Use - 将 Asset 准备好使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Getting-Still-Images-From-a-Video-从视频中获取静态图像"><span class="toc-number">2.3.</span> <span class="toc-text">Getting Still Images From a Video - 从视频中获取静态图像</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Generating-a-Single-Image-生成一个单独的图像"><span class="toc-number">2.3.1.</span> <span class="toc-text">Generating a Single Image - 生成一个单独的图像</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generating-a-Sequence-of-Images-生成一系列图像"><span class="toc-number">2.4.</span> <span class="toc-text">Generating a Sequence of Images - 生成一系列图像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Trimming-and-Transcoding-a-Movie-微调和转化为一个电影"><span class="toc-number">2.5.</span> <span class="toc-text">Trimming and Transcoding a Movie - 微调和转化为一个电影</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Playback-播放-1"><span class="toc-number">3.</span> <span class="toc-text">Playback - 播放</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Playing-Assets-播放资产"><span class="toc-number">3.1.</span> <span class="toc-text">Playing Assets - 播放资产</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Handling-Different-Types-of-Asset-处理不同类型的资产"><span class="toc-number">3.2.</span> <span class="toc-text">Handling Different Types of Asset - 处理不同类型的资产</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Playing-an-Item-播放一个项目"><span class="toc-number">3.3.</span> <span class="toc-text">Playing an Item - 播放一个项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Changing-the-Playback-Rate-改变播放的速率"><span class="toc-number">3.3.1.</span> <span class="toc-text">Changing the Playback Rate - 改变播放的速率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Seeking—Repositioning-the-Playhead-寻找-重新定位播放头"><span class="toc-number">3.3.2.</span> <span class="toc-text">Seeking—Repositioning the Playhead - 寻找-重新定位播放头</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Playing-Multiple-Items-播放多个项目"><span class="toc-number">3.4.</span> <span class="toc-text">Playing Multiple Items - 播放多个项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Monitoring-Playback-监视播放"><span class="toc-number">3.5.</span> <span class="toc-text">Monitoring Playback - 监视播放</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Responding-to-a-Change-in-Status-响应状态的变化"><span class="toc-number">3.5.1.</span> <span class="toc-text">Responding to a Change in Status - 响应状态的变化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tracking-Readiness-for-Visual-Display-为视觉展示做追踪准备"><span class="toc-number">3.5.2.</span> <span class="toc-text">Tracking Readiness for Visual Display - 为视觉展示做追踪准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tracking-Time-追踪时间"><span class="toc-number">3.5.3.</span> <span class="toc-text">Tracking Time - 追踪时间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reaching-the-End-of-an-Item-到达一个项目的结束"><span class="toc-number">3.5.4.</span> <span class="toc-text">Reaching the End of an Item - 到达一个项目的结束</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Putting-It-All-Together-Playing-a-Video-File-Using-AVPlayerLayer-总而言之，使用-AVPlayerLayer-播放视频文件"><span class="toc-number">3.6.</span> <span class="toc-text">Putting It All Together: Playing a Video File Using AVPlayerLayer - 总而言之，使用 AVPlayerLayer 播放视频文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Player-View-播放器视图"><span class="toc-number">3.6.1.</span> <span class="toc-text">The Player View - 播放器视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Simple-View-Controller-一个简单的-View-Controller"><span class="toc-number">3.6.2.</span> <span class="toc-text">A Simple View Controller - 一个简单的 View Controller</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-the-Asset-创建一个资产"><span class="toc-number">3.6.3.</span> <span class="toc-text">Creating the Asset - 创建一个资产</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Responding-to-the-Player-Item’s-Status-Change-相应播放项目的状态改变"><span class="toc-number">3.6.4.</span> <span class="toc-text">Responding to the Player Item’s Status Change - 相应播放项目的状态改变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Playing-the-Item-播放项目"><span class="toc-number">3.6.5.</span> <span class="toc-text">Playing the Item - 播放项目</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Editing-编辑-1"><span class="toc-number">4.</span> <span class="toc-text">Editing - 编辑</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Creating-a-Composition-创建组件"><span class="toc-number">4.1.</span> <span class="toc-text">Creating a Composition - 创建组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Options-for-Initializing-a-Composition-Track-初始化组件轨道的选项"><span class="toc-number">4.1.1.</span> <span class="toc-text">Options for Initializing a Composition Track - 初始化组件轨道的选项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adding-Audiovisual-Data-to-a-Composition-将视听数据添加到一个组件中"><span class="toc-number">4.2.</span> <span class="toc-text">Adding Audiovisual Data to a Composition - 将视听数据添加到一个组件中</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Retrieving-Compatible-Composition-Tracks-检索兼容的组件轨道"><span class="toc-number">4.2.1.</span> <span class="toc-text">Retrieving Compatible Composition Tracks - 检索兼容的组件轨道</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generating-a-Volume-Ramp-生成一个音量坡度"><span class="toc-number">4.3.</span> <span class="toc-text">Generating a Volume Ramp - 生成一个音量坡度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Performing-Custom-Video-Processing-执行自定义配置"><span class="toc-number">4.4.</span> <span class="toc-text">Performing Custom Video Processing - 执行自定义配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Changing-the-Composition’s-Background-Color-改变组件的背景颜色"><span class="toc-number">4.4.1.</span> <span class="toc-text">Changing the Composition’s Background Color - 改变组件的背景颜色</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Applying-Opacity-Ramps-应用不透明的坡道"><span class="toc-number">4.4.2.</span> <span class="toc-text">Applying Opacity Ramps - 应用不透明的坡道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incorporating-Core-Animation-Effects-结合核心动画效果"><span class="toc-number">4.4.3.</span> <span class="toc-text">Incorporating Core Animation Effects - 结合核心动画效果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Putting-It-All-Together-Combining-Multiple-Assets-and-Saving-the-Result-to-the-Camera-Roll"><span class="toc-number">4.5.</span> <span class="toc-text">Putting It All Together: Combining Multiple Assets and Saving the Result to the Camera Roll -</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-the-Composition-创建组件"><span class="toc-number">4.5.1.</span> <span class="toc-text">Creating the Composition - 创建组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adding-the-Assets-添加资产"><span class="toc-number">4.5.2.</span> <span class="toc-text">Adding the Assets - 添加资产</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Checking-the-Video-Orientations-检查视频的方向"><span class="toc-number">4.5.3.</span> <span class="toc-text">Checking the Video Orientations - 检查视频的方向</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Applying-the-Video-Composition-Layer-Instructions-视频组件层指令的应用"><span class="toc-number">4.5.4.</span> <span class="toc-text">Applying the Video Composition Layer Instructions - 视频组件层指令的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Setting-the-Render-Size-and-Frame-Duration-设置渲染大小和帧周期"><span class="toc-number">4.5.5.</span> <span class="toc-text">Setting the Render Size and Frame Duration - 设置渲染大小和帧周期</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exporting-the-Composition-and-Saving-it-to-the-Camera-Roll-导出组件并存到相机胶卷"><span class="toc-number">4.5.6.</span> <span class="toc-text">Exporting the Composition and Saving it to the Camera Roll - 导出组件并存到相机胶卷</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Still-and-Video-Media-Capture-静态视频媒体捕获。"><span class="toc-number">5.</span> <span class="toc-text">Still and Video Media Capture - 静态视频媒体捕获。</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Use-a-Capture-Session-to-Coordinate-Data-Flow-使用捕捉会话来协调数据流"><span class="toc-number">5.1.</span> <span class="toc-text">Use a Capture Session to Coordinate Data Flow - 使用捕捉会话来协调数据流</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Configuring-a-Session-配置会话"><span class="toc-number">5.1.1.</span> <span class="toc-text">Configuring a Session - 配置会话</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monitoring-Capture-Session-State-监视捕获会话状态"><span class="toc-number">5.1.2.</span> <span class="toc-text">Monitoring Capture Session State - 监视捕获会话状态</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#An-AVCaptureDevice-Object-Represents-an-Input-Device-一个-AVCaptureDevice-对象代表一个输入设备"><span class="toc-number">5.2.</span> <span class="toc-text">An AVCaptureDevice Object Represents an Input Device - 一个 AVCaptureDevice 对象代表一个输入设备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Device-Characteristics-设备特点"><span class="toc-number">5.2.1.</span> <span class="toc-text">Device Characteristics - 设备特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Device-Capture-Settings"><span class="toc-number">5.2.2.</span> <span class="toc-text">Device Capture Settings</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Focus-Modes-聚焦模式"><span class="toc-number">5.2.2.1.</span> <span class="toc-text">Focus Modes - 聚焦模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Exposure-Modes-曝光模式"><span class="toc-number">5.2.2.2.</span> <span class="toc-text">Exposure Modes - 曝光模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Flash-Modes-闪光模式"><span class="toc-number">5.2.2.3.</span> <span class="toc-text">Flash Modes - 闪光模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Torch-Mode-手电筒模式"><span class="toc-number">5.2.2.4.</span> <span class="toc-text">Torch Mode - 手电筒模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Video-Stabilization-视频稳定性"><span class="toc-number">5.2.2.5.</span> <span class="toc-text">Video Stabilization - 视频稳定性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#White-Balance-白平衡"><span class="toc-number">5.2.2.6.</span> <span class="toc-text">White Balance - 白平衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Setting-Device-Orientation-设置设备方向"><span class="toc-number">5.2.2.7.</span> <span class="toc-text">Setting Device Orientation - 设置设备方向</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Configuring-a-Device-配置设备"><span class="toc-number">5.2.3.</span> <span class="toc-text">Configuring a Device - 配置设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Switching-Between-Devices-切换装置"><span class="toc-number">5.2.4.</span> <span class="toc-text">Switching Between Devices - 切换装置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Use-Capture-Inputs-to-Add-a-Capture-Device-to-a-Session-使用捕获输入将捕获设备添加到会话中"><span class="toc-number">5.3.</span> <span class="toc-text">Use Capture Inputs to Add a Capture Device to a Session - 使用捕获输入将捕获设备添加到会话中</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Use-Capture-Outputs-to-Get-Output-from-a-Session-使用捕获输出从会话得到输出"><span class="toc-number">5.4.</span> <span class="toc-text">Use Capture Outputs to Get Output from a Session - 使用捕获输出从会话得到输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Saving-to-a-Movie-File-保存电影文件"><span class="toc-number">5.4.1.</span> <span class="toc-text">Saving to a Movie File - 保存电影文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Starting-a-Recording-开始记录"><span class="toc-number">5.4.1.1.</span> <span class="toc-text">Starting a Recording - 开始记录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Ensuring-That-the-File-Was-Written-Successfully-确保文件被成功写入"><span class="toc-number">5.4.1.2.</span> <span class="toc-text">Ensuring That the File Was Written Successfully - 确保文件被成功写入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adding-Metadata-to-a-File-将元数据添加到文件中"><span class="toc-number">5.4.1.3.</span> <span class="toc-text">Adding Metadata to a File - 将元数据添加到文件中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Processing-Frames-of-Video-处理视频的帧"><span class="toc-number">5.4.1.4.</span> <span class="toc-text">Processing Frames of Video - 处理视频的帧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Performance-Considerations-for-Processing-Video-处理视频的性能考虑"><span class="toc-number">5.4.1.5.</span> <span class="toc-text">Performance Considerations for Processing Video - 处理视频的性能考虑</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Capturing-Still-Images-捕获静止图像"><span class="toc-number">5.4.2.</span> <span class="toc-text">Capturing Still Images - 捕获静止图像</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pixel-and-Encoding-Formats-像素和编码格式"><span class="toc-number">5.4.2.1.</span> <span class="toc-text">Pixel and Encoding Formats - 像素和编码格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Capturing-an-Image-捕获图像"><span class="toc-number">5.4.2.2.</span> <span class="toc-text">Capturing an Image - 捕获图像</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Showing-the-User-What’s-Being-Recorded-显示用户正在被记录什么"><span class="toc-number">5.5.</span> <span class="toc-text">Showing the User What’s Being Recorded - 显示用户正在被记录什么</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Video-Preview-视频预览"><span class="toc-number">5.5.1.</span> <span class="toc-text">Video Preview - 视频预览</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Video-Gravity-Modes-视屏重力模式"><span class="toc-number">5.5.1.1.</span> <span class="toc-text">Video Gravity Modes - 视屏重力模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-“Tap-to-Focus”-with-a-Preview-使用“点击焦点”预览"><span class="toc-number">5.5.1.2.</span> <span class="toc-text">Using “Tap to Focus” with a Preview - 使用“点击焦点”预览</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Showing-Audio-Levels-显示音频等级"><span class="toc-number">5.5.2.</span> <span class="toc-text">Showing Audio Levels - 显示音频等级</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Putting-It-All-Together-Capturing-Video-Frames-as-UIImage-Objects-总而言之：捕获视频帧用作-UIImage-对象"><span class="toc-number">5.6.</span> <span class="toc-text">Putting It All Together: Capturing Video Frames as UIImage Objects - 总而言之：捕获视频帧用作 UIImage 对象</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Create-and-Configure-a-Capture-Session-创建和配置捕获会话"><span class="toc-number">5.6.1.</span> <span class="toc-text">Create and Configure a Capture Session - 创建和配置捕获会话</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Create-and-Configure-the-Device-and-Device-Input-创建和配置设备记忆设备输入"><span class="toc-number">5.6.2.</span> <span class="toc-text">Create and Configure the Device and Device Input - 创建和配置设备记忆设备输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Create-and-Configure-the-Video-Data-Output-创建和配置视频数据输出"><span class="toc-number">5.6.3.</span> <span class="toc-text">Create and Configure the Video Data Output - 创建和配置视频数据输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implement-the-Sample-Buffer-Delegate-Method-实现示例缓冲代理方法"><span class="toc-number">5.6.4.</span> <span class="toc-text">Implement the Sample Buffer Delegate Method - 实现示例缓冲代理方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Starting-and-Stopping-Recording-启动和停止录制"><span class="toc-number">5.6.5.</span> <span class="toc-text">Starting and Stopping Recording - 启动和停止录制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#High-Frame-Rate-Video-Capture-高帧速率视频捕获"><span class="toc-number">5.7.</span> <span class="toc-text">High Frame Rate Video Capture - 高帧速率视频捕获</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Playback-播放-2"><span class="toc-number">5.7.1.</span> <span class="toc-text">Playback - 播放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Editing-编辑-2"><span class="toc-number">5.7.2.</span> <span class="toc-text">Editing - 编辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Export-出口"><span class="toc-number">5.7.3.</span> <span class="toc-text">Export - 出口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Recording-录制"><span class="toc-number">5.7.4.</span> <span class="toc-text">Recording - 录制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Export-输出"><span class="toc-number">6.</span> <span class="toc-text">Export - 输出</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Reading-an-Asset-读取资产"><span class="toc-number">6.1.</span> <span class="toc-text">Reading an Asset - 读取资产</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-the-Asset-Reader-创建资产读取器"><span class="toc-number">6.1.1.</span> <span class="toc-text">Creating the Asset Reader - 创建资产读取器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Setting-Up-the-Asset-Reader-Outputs-建立资产读取器出口"><span class="toc-number">6.1.2.</span> <span class="toc-text">Setting Up the Asset Reader Outputs - 建立资产读取器出口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reading-the-Asset’s-Media-Data-读取资产媒体数据"><span class="toc-number">6.1.3.</span> <span class="toc-text">Reading the Asset’s Media Data - 读取资产媒体数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Writing-an-Asset-写入资产"><span class="toc-number">6.2.</span> <span class="toc-text">Writing an Asset - 写入资产</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Creating-the-Asset-Writer-创建资产写入器"><span class="toc-number">6.2.1.</span> <span class="toc-text">Creating the Asset Writer - 创建资产写入器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Setting-Up-the-Asset-Writer-Inputs-建立资产写入器入口"><span class="toc-number">6.2.2.</span> <span class="toc-text">Setting Up the Asset Writer Inputs - 建立资产写入器入口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Writing-Media-Data-写入媒体数据"><span class="toc-number">6.2.3.</span> <span class="toc-text">Writing Media Data - 写入媒体数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reencoding-Assets-重新编码资产"><span class="toc-number">6.3.</span> <span class="toc-text">Reencoding Assets - 重新编码资产</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Putting-It-All-Together-Using-an-Asset-Reader-and-Writer-in-Tandem-to-Reencode-an-Asset-总结：使用资产读取器和写入器串联重新编码资产"><span class="toc-number">6.4.</span> <span class="toc-text">Putting It All Together: Using an Asset Reader and Writer in Tandem to Reencode an Asset - 总结：使用资产读取器和写入器串联重新编码资产</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Handling-the-Initial-Setup-处理初始设置"><span class="toc-number">6.4.1.</span> <span class="toc-text">Handling the Initial Setup - 处理初始设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Initializing-the-Asset-Reader-and-Writer-初始化资产读取器和写入器"><span class="toc-number">6.4.2.</span> <span class="toc-text">Initializing the Asset Reader and Writer - 初始化资产读取器和写入器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reencoding-the-Asset-重新编码资产"><span class="toc-number">6.4.3.</span> <span class="toc-text">Reencoding the Asset - 重新编码资产</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Handling-Completion-处理完成"><span class="toc-number">6.4.4.</span> <span class="toc-text">Handling Completion - 处理完成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Handling-Cancellation-处理注销"><span class="toc-number">6.4.5.</span> <span class="toc-text">Handling Cancellation - 处理注销</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Asset-Output-Settings-Assistant-资产出口设置助手"><span class="toc-number">6.5.</span> <span class="toc-text">Asset Output Settings Assistant - 资产出口设置助手</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Time-and-Media-Representations-时间和媒体表现"><span class="toc-number">7.</span> <span class="toc-text">Time and Media Representations - 时间和媒体表现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Representation-of-Assets-资产的表示"><span class="toc-number">7.1.</span> <span class="toc-text">Representation of Assets - 资产的表示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Representations-of-Time-时间的表示"><span class="toc-number">8.</span> <span class="toc-text">Representations of Time - 时间的表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CMTime-Represents-a-Length-of-Time-CMTime-表示时间的长度"><span class="toc-number">8.0.1.</span> <span class="toc-text">CMTime Represents a Length of Time - CMTime 表示时间的长度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-CMTime-使用-CMTime"><span class="toc-number">8.0.1.1.</span> <span class="toc-text">Using CMTime - 使用 CMTime</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Special-Values-of-CMTime-CMTime-的特殊值"><span class="toc-number">8.0.1.2.</span> <span class="toc-text">Special Values of CMTime - CMTime 的特殊值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Representing-CMTime-as-an-Object-CMTime表示为一个对象"><span class="toc-number">8.0.1.3.</span> <span class="toc-text">Representing CMTime as an Object - CMTime表示为一个对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Epochs-纪元"><span class="toc-number">8.0.1.4.</span> <span class="toc-text">Epochs - 纪元</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CMTimeRange-Represents-a-Time-Range-CMTimeRange表示一个时间范围"><span class="toc-number">8.0.2.</span> <span class="toc-text">CMTimeRange Represents a Time Range - CMTimeRange表示一个时间范围</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Working-with-Time-Ranges-与时间范围工作"><span class="toc-number">8.0.2.1.</span> <span class="toc-text">Working with Time Ranges - 与时间范围工作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Special-Values-of-CMTimeRange-CMTimeRange-的特殊值"><span class="toc-number">8.0.2.2.</span> <span class="toc-text">Special Values of CMTimeRange - CMTimeRange 的特殊值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Representing-a-CMTimeRange-Structure-as-an-Object-将-CMTimeRange-结构体表示为对象"><span class="toc-number">8.0.2.3.</span> <span class="toc-text">Representing a CMTimeRange Structure as an Object - 将 CMTimeRange 结构体表示为对象</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Representations-of-Media-媒体的表示"><span class="toc-number">8.1.</span> <span class="toc-text">Representations of Media - 媒体的表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Converting-CMSampleBuffer-to-a-UIImage-Object-将-CMSampleBuffer-转化为-UIImage-对象"><span class="toc-number">8.2.</span> <span class="toc-text">Converting CMSampleBuffer to a UIImage Object - 将 CMSampleBuffer 转化为 UIImage 对象</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"AVFoundation Programming Guide(官方文档翻译)完整版中英对照　| YoferZhang 的博客　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    
      <div class="duoshuo" id="comments">
    <div id="comment-box" ></div>
    <div class="ds-thread" id="ds-thread" data-thread-key="post/20160724AVFoundation/" data-title="AVFoundation Programming Guide(官方文档翻译)完整版中英对照" data-url="http://yoferzhang.com/post/20160724AVFoundation/"></div>
    <script>
        var duoshuoQuery = {short_name:"http://yoferzhang.duoshuo.com"};
        var loadComment = function(){
            var d = document, s = d.createElement('script');
            s.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
            s.async = true; s.charset = 'UTF-8';
            (d.head || d.body).appendChild(s);
        }

        
    </script>
    
    <script> loadComment(); </script>

</div>
    




    <div class="scroll" id="post-nav-button">
        
            <a href="/post/20160803AVFoundation01Introduction/" title="上一篇: AVFoundation Programming Guide(官方文档翻译1)About AVFoundation - AVFoundation概述">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/post/20160426Matchismo/" title="下一篇: 【iOS】Stanford iOS7 Assignment - Matchismo">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/post/20170326ML01Introduction/">机器学习入门系列</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20170306DeepLearningRegression/">【测试】只是为了测试数学公式</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20170216FitnessDiary/">【健身】健身记录；健身课表；健身心得</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20170116ViewControllerProgrammingGuide/">为什么要在viewDidLoad 中加载view，在viewWillAppear:中对view进行layout？</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20161130DataManager/">【iOS】iOS数据存储,应用沙盒,XML,Preference,NSKeyedArchiver归档,SQLite3</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20161109TableViewPG/">Table View Programming Guide for iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20161024LLDBQuickStartGuide/">【iOS】LLDB调试指南</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160926Cocos2dxViewAnimation/">【iOS】Cocos2dx封装为view方便做3D动画效果</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160918ReplayKit/">【iOS】ReplayKit库，iOS原生直播神器</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160909ErrorLibpngErrorUnhandledCritical/">【iOS】cocos2dx在xcode8 GM版下的错误`libpng error:CgBI:unhandled critical chunk`</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160905CoreAnimationProgrammingGuide/">【iOS】Core Animation Programming Guide 核心动画编程指南</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160902ViewProgrammingGuide/">【iOS】View Programming Guide for iOS 视图编程指南</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160831KVO/">【iOS】KVO编程指南,Key-Value Observing Programming Guide翻译</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160830ConcurrencyProgrammingGuide/">【iOS】iOS并发编程对比总结,NSThread,NSOperation,GCD</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160825property/">【iOS】property属性的weak,strong,copy,assign</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160824Contacts/">【iOS】私人通讯录Demo</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160823Cocoapods/">Cocoapods 安装</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160821UIKitClasses/">UIKit继承结构</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160821ControllerManager/">【iOS】iOS控制器管理,代码,xib,Storyboard,Segue</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160821ApplicationStart/">【iOS】iOS程序启动过程,原理,UIApplication,代码启动界面</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160817XcodePlug/">【iOS】Xcode插件管理工具Alcatraz,常用插件介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160816NotificationSum/">【iOS】Notification通知,通知中心,发布通知,通知代理对比</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160816ViewControllerProgrammingGuide/">【iOS】视图控制器编程指南View Controller Programming Guide for iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160814UIBase/">UIButton,UIScrollView,UITableView常见属性,使用案例 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160814NSFoundationTran/">集合间相互转换,浅谈相关内存管理,使用NSData处理数据,使用NSDate - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160814NSSetSum/">NSSet 集合创建,获取,遍历,可变集合的删除 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160814NSDictionarySum/">NSDictionary字典创建,获取,遍历,可变字典的删除 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160814NSArraySum/">NSArray/NSMutableArray创建,获取,遍历,排序 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813NSStringSum/">NSString的获取,判断,转换,重组,文件读写 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813FoundationSum/">iOS 结构体</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813PropertySynthesize/">iOS - @property 和 @synthesize 总结</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813DescriptionMethod/">description 方法 和 SEL - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813NatureOfClass/">Class 类的本质 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813CategorySum/">Category 分类、类别 总结 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160813ProtocolSum/">Protocol 协议总结 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160812BlockSum/">Block编程总结 - iOS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160812ARCSum/">Automatic Reference Counting (ARC) 总结</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160811OCMemoryManagementSum/">Objective-C内存管理[iOS]</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160811OCMemoryManagement/">Advanced Memory Management Programming Guide 高级内存管理编程指南(官方文档翻译)[iOS]</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160810AboutHTTPLiveStream/">About HTTP Live Streaming官方文档翻译 [iOS]</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation07TimeAndMediaRepresentations/">AVFoundation Programming Guide(官方文档翻译7)Time and Media Representations 时间和媒体表示</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation06Export/">AVFoundation Programming Guide(官方文档翻译6)Export - 输出</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation05StillAndVideoMediaCapture/">AVFoundation Programming Guide(官方文档翻译5)Still and Video Media Capture - 静态视频媒体捕获。</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation04Editing/">AVFoundation Programming Guide(官方文档翻译4)Editing - 编辑</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation03Playback/">AVFoundation Programming Guide(官方文档翻译3)Playback - 播放</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation02UsingAssets/">AVFoundation Programming Guide(官方文档翻译2)Using Assets - 使用Assets</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160803AVFoundation01Introduction/">AVFoundation Programming Guide(官方文档翻译1)About AVFoundation - AVFoundation概述</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160724AVFoundation/">AVFoundation Programming Guide(官方文档翻译)完整版中英对照</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160426Matchismo/">【iOS】Stanford iOS7 Assignment - Matchismo</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160420ObjectiveCProgrammingNote/">【iOS】Objective C Programming The Big Nerd Ranch Guide 2nd Edition - Note</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160419StructMemory/">【C语言】结构体内存对齐模式</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160417CMemory/">【C语言】C语言内存四区 - 概念与代码示例</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160417MallocCallocRelloc/">【C语言】malloc,calloc,relloc异同示例</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160416DiskFormatForHFS/">【渔】NTFS转HFS</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160415MarkdownSyntax/">【渔】Markdown语法（看这张图就够了）</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160415WebsiteRecommend/">【渔】网站推荐</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160414KobeLeave/">【随笔】2016年4月14日 Kobe Bryant完美谢幕</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160408FirstHackintosh/">【渔】联想Y430P - 成功完美黑苹果上身，同时有win8.1做备用</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160329NumberCodeTable/">【快速记忆】数字编码表</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160328SuperMemoryBackground/">【快速记忆】快速记忆背景</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160325ReadIndex/">【读书笔记】最近读书的目录</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160325ReadNotePomodoro/">【读书笔记】番茄工作法图解</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160323buildBlog01/">【建立博客】使用Github和Hexo搭建属于自己的博客</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160322essay002/">【随笔】原文名：鹅场offer已Get，下周签约，终于能静下心来总结总结</a></li><li class="post-list-item"><a class="post-list-link" href="/post/20160322readNotes001/">【读书笔记】把你的英语用起来</a></li><li class="post-list-item"><a class="post-list-link" href="/post/hexo-post/">模板</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2017 Yofer Zhang
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 4;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>